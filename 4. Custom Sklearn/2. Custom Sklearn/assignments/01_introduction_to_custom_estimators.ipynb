{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/logo.png\" align='right' width=250px>\n",
    "\n",
    "# Custom Estimators\n",
    "\n",
    "A Scikit-Learn [Estimator](https://scikit-learn.org/stable/developers/develop.html) implements a `fit` method to learn from data.\n",
    "\n",
    "<img src=\"../images/oprah.jpeg\" align=\"right\">\n",
    "\n",
    "We shall develop two types of Scikit-Learn Estimators:\n",
    "- A (Predictor) Model (implements a `predict` method to make predictions and a `score` method to evaluate the \"goodness of fit\"),\n",
    "- A Transformer (implements a `transform` method to transform a dataset).\n",
    "\n",
    "This is the live-coded example used in the training. This notebook has been created to allow students to read back through the example.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Build our own Model\n",
    "\n",
    "We are going to build a Model that will look at the smallest euclidean distance between the data we'd like to predict and the initial training dataset.\n",
    "\n",
    "That means when we fit our Model, we want to 'lock in' the X_train data as a fitted variable, so that when we pass in new data, X_test say, we can compare it with the original X_train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.metrics import euclidean_distances\n",
    "\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted, check_array\n",
    "\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break down the predict method with a small example\n",
    "\n",
    "Firstly, let's look at what we're doing - the euclidean distances - with a small sample of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.array([[1, 2, 3], [4, 5, 6], [0, 1, 2], [1, 0, 1]])\n",
    "\n",
    "\n",
    "train_labels = np.array([\"a\", \"b\", \"c\", \"d\"])\n",
    "\n",
    "new_data = np.array([[0, 1, 2], [4, 4, 6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is Euclidean distance?**\n",
    "\n",
    "The Euclidean distance between two points in Euclidean space is the length of a line segment between the two points.\n",
    "\n",
    "Let's demonstate this with the first rows of our new_data and our train_data: `[0, 1, 2]` and `[1, 2, 3]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distances of [0, 1, 2] and [1, 2, 3]\n",
    "\n",
    "((0 - 1) ** 2 + (1 - 2) ** 2 + (2 - 3) ** 2) ** 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now let's work out all the Euclidean distances**\n",
    "\n",
    "We can use the function `euclidean_distances` to calculate the distances between all rows in the train_data and new_data. Note that for each row in the new_data we will get 4 euclidean distances, because we want to calculate the euclidean distance from each row in the train_data (and there are 4 rows in the original train_data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = euclidean_distances(new_data, train_data)\n",
    "print(distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to select the smallest, however we don't want the value, we want the index location. \n",
    "\n",
    "**Question: Why do we want the index location and not the value?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "closest_index = np.argmin(distances, axis=1)\n",
    "closest_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: With the index location, we can use this to filter on our train_labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels[closest_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = train_labels[closest_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Model\n",
    "\n",
    "Now that we understand the mathematics we are aiming to perform, let's put this into a Model.\n",
    "\n",
    "Our code in full is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all distances\n",
    "distances = euclidean_distances(new_data, train_data)\n",
    "\n",
    "# get location of smallest distances\n",
    "closest_index = np.argmin(distances, axis=1)\n",
    "\n",
    "# filter our train_labels to get predictions\n",
    "predictions = train_labels[closest_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To build our Model, we'll inherit from the [`BaseEstimator`](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator.html#examples-using-sklearn-base-baseestimator) class. Amongst other things, this provides a default implementation for the `get_params()` and `set_params()` methods. This is useful to make the model grid search-able with GridSearchCV for automated parameters tuning and behave well with others when combined in a Pipeline.\n",
    "\n",
    "We'll also inherit from [`ClassifierMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html), a Mixin class for all classifiers in scikit-learn. This provides us a default implementation of the `score()` method, which will return the mean accuracy on the given test data and labels.\n",
    "\n",
    "Our custom Model needs:\n",
    "\n",
    "- `__init__` - here we state any hyperparameters*\n",
    "- `fit` - we need to fit our training data and training labels to use in the next method\n",
    "- `predict` - find the distances and locate the predictions from the training labels\n",
    "\n",
    "\n",
    "*we don't need to include any right now since we are finding the smallest distance. Let's include a redundant parameter `k` for example purposes. This won't be used now but could be useful if we wanted to build a k-nearest neighbour algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted, check_array\n",
    "\n",
    "\n",
    "class CustomClassifier(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, k=1):\n",
    "        self.k = k\n",
    "\n",
    "    def fit(self, X, y):\n",
    "\n",
    "        # check that X and y have the correct shape\n",
    "        check_X_y(X, y)\n",
    "\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "\n",
    "        # should always return the object itself\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "\n",
    "        # check that X has the correct shape\n",
    "        check_array(X)\n",
    "\n",
    "        # check has the model been fitted\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        distances = euclidean_distances(X, self.X_)\n",
    "        closest_index = np.argmin(distances, axis=1)\n",
    "        predictions = self.y_[closest_index]\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Model on Large Data\n",
    "\n",
    "Let's use the make_blobs function to create some new, bigger data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_blobs(n_samples=300, centers=2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=X[:, 0], y=X[:, 1], hue=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's follow our classic model build steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can alter the value of k, but it won't change the model at this moment\n",
    "model = CustomClassifier(k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<img src='https://tfwiki.net/mediawiki/images2/thumb/3/37/Optimusg1.jpg/350px-Optimusg1.jpg' align='right' width=200px>\n",
    "\n",
    "# Build our own Transformer\n",
    "\n",
    "We are now going to build a mean imputer Transformer. The steps we will need to take:\n",
    "\n",
    "- calculate the mean for each column\n",
    "- identify the location of the missing values\n",
    "- overwrite the missing values with *their corresponding column mean*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break down the transformation with smaller data\n",
    "\n",
    "Let's work through this with a small sample of data to understand the process. \n",
    "<!-- Since this is a transformer, we don't need some new_data or any labels: -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with missing values in:\n",
    "\n",
    "train_data = np.array([[1, 2, 3], [np.nan, np.nan, 6], [0, 1, 2], [np.nan, 0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get the means of each column (disregarding the missing values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means = np.nanmean(train_data, axis=0)\n",
    "print(f\"Mean value for each column: {means}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the function `np.isnan()` to find the row and column locations of our missing value. Note that row an `m x n` matrix, the output will be as follows:\n",
    "\n",
    "$($ $(array(x_1, x_2, ..., x_n),$  $array(y_1, y_2, ..., y_n)$ $)$\n",
    "\n",
    "where the $x$ values are the rows and corresponding to the $y$ columns in reference to the location in the original matrix.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_locations = np.where(np.isnan(train_data))\n",
    "print(f\"Array of Rows and Columns where we see missings: {missing_locations}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first array states there are missing values in rows 1 and 3, the second states in columns 0 and 1. \n",
    "\n",
    "The exact location of the three missing values are:\n",
    "`(1, 0)`\n",
    "`(1, 1)`\n",
    "`(3, 0)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **only need to know the columns in which missing values occured** since we will replace the missing values with their column means, regardless of in which row they sit.\n",
    "\n",
    "So, let's select only the second array using `missing_locations[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_with_miss = missing_locations[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the missing locations to select the missing values from the column, this is vital so that we can overwrite these values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"These missings: {train_data[missing_locations]} correspond to columns {cols_with_miss}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to use the `cols_with_miss` to select the corresponding mean values for those columns. We can use the `np.take()` function for this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.take(means, cols_with_miss)\n",
    "print(\n",
    "    f\"These missings: {train_data[missing_locations]} will be replaced with {values}\\n\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have all we need! Let's create a copy of our data so we can overwrite those missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "copy = train_data.copy()\n",
    "copy[missing_locations] = np.take(means, cols_with_miss)\n",
    "print(\"Original data:\", end=\"\\n\\n\")\n",
    "print(train_data, end=\"\\n\\n\")\n",
    "print(\"Final output:\", end=\"\\n\\n\")\n",
    "print(copy, end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now build with the Transformer\n",
    "\n",
    "Let's put all of that in a Transformer. Here was our code in full:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "copy = train_data.copy()\n",
    "\n",
    "means = np.nanmean(train_data, axis=0)\n",
    "missing_locations = np.where(np.isnan(train_data))\n",
    "cols_with_miss = missing_locations[1]\n",
    "copy[missing_locations] = np.take(means, cols_with_miss)\n",
    "copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We shall inherit from [`TransformerMixin`](https://scikit-learn.org/stable/modules/generated/sklearn.base.ClassifierMixin.html), a Mixin class for all Transformers in scikit-learn. This provides us a default implementation of the `fit_transform()` method, which can fit to data, then transform it.\n",
    "\n",
    "In a Transformer we need:\n",
    "\n",
    "- `init` - again we don't need any parameters\n",
    "- `fit` - here we want to fit our means values, so these are always learned from the train data\n",
    "- `transform` - this is where we locate our missing values, columns and output a **copy** of the data with missing values overwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomMeanImputer(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.means_ = np.nanmean(X, axis=0)\n",
    "        # ALWAYS return the object itself (self)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "\n",
    "        # check has the model been fitted\n",
    "        check_is_fitted(self)\n",
    "\n",
    "        X = X.copy()\n",
    "        inds = np.where(np.isnan(X))\n",
    "        X[inds] = np.take(self.means_, inds[1])\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mean_imputer = CustomMeanImputer()\n",
    "\n",
    "\n",
    "training_data = np.array([[1, 2, 3], [np.nan, np.nan, 6], [0, 1, 2], [np.nan, 0, 1]])\n",
    "\n",
    "custom_mean_imputer.fit(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mean_imputer.transform(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_mean_imputer.fit_transform(training_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run through this with larger data. This wasn't covered in the live-code session but is good to see on larger data.\n",
    "\n",
    "Note that the Transformer will only work on data that has more than 1 column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_eg_X_y(rows=100, cols=2, percent_miss=0.1, random_state=100):\n",
    "    if cols < 2:\n",
    "        raise ValueError(\"Must create X with more than 1 column\")\n",
    "    np.random.seed(random_state)\n",
    "    np.random.random_sample(size=(100, 2))\n",
    "    np.random.seed(0)\n",
    "    X = np.random.random_sample(size=(rows, cols))\n",
    "    X[X < percent_miss] = np.nan\n",
    "\n",
    "    y = (X.prod(axis=1) > 0.5) * 1\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = create_eg_X_y()\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = CustomMeanImputer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transformed = imputer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_transformed = imputer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[:5], X_train_transformed[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[:10], X_test_transformed[:10]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
