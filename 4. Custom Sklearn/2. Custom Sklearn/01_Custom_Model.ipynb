{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9d6f5bf-5b9e-4fa0-9fc1-47a6d24462fc",
   "metadata": {},
   "source": [
    "<img src=\"images/logo.png\" align='right' width=250px>\n",
    "\n",
    "# Custom Model in Scikit-Learn\n",
    "\n",
    "Scikit-Learn can be extended with functionality that is not already natively included in the library. \n",
    "\n",
    "This notebook covers an example of how to customize an existing Model - the `RandomForestClassifier`.\n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- [Explain the benefits of using a custom Model](#benefits)\n",
    "- [Overwrite the score method on an existing Model](#custom)\n",
    "- [Implement the custom Model in a scikit-learn Pipeline](#pipeline)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac1e38b-ff7d-4e33-9ae6-4b198c56600c",
   "metadata": {},
   "source": [
    "<a id=benefits></a>\n",
    "\n",
    "# Benefits\n",
    "\n",
    "There are a wide range of models that are available in scikit-learn, with integrated methods such as `.predict()` and `.score()`, but the list of models is finite and the methods are set up to work in specific ways that can be limiting.\n",
    "\n",
    "It is possible to extend models by creating custom `Model` classes that inherit from the `BaseEstimator`, or from a specific model. This can provide many benefits:\n",
    "\n",
    "- Design a solution that specifically fits the requirements of your problem\n",
    "- Integrate with scikit-learn Pipelines to streamline routine processes in a machine learning workflow\n",
    "- Gain a deeper understanding of how machine learning models work at a fundamental level\n",
    "- Experiment with novel or niche algorithms to implement and test your ideas\n",
    "- Reuse your custom Model across different projects or share it with others"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b9254f-4d0e-4be3-a8b7-c807e3e023a8",
   "metadata": {},
   "source": [
    "<a id=custom></a>\n",
    "\n",
    "## Overwrite the score method on an existing Model\n",
    "\n",
    "Take this existing model, which takes information about patients and predicts whether they will have a stroke or not. \n",
    "\n",
    "This model can be useful to doctors who can use this information to target specific patients and put in any desired intervention if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98538c07-7fdf-4cb0-94e5-758ecab74cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import (StandardScaler, \n",
    "                                   PolynomialFeatures, \n",
    "                                   OneHotEncoder, \n",
    "                                   OrdinalEncoder, \n",
    "                                   KBinsDiscretizer)\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf345e4-5f08-47ef-b973-9c52cc65172a",
   "metadata": {},
   "source": [
    "First the data is read in, cleaned and split into `X` and `y` and train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ad04a9-8344-44bc-b57a-d299d1a272cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the stroke data\n",
    "stroke = pd.read_csv('data/stroke.csv').rename(columns=str.lower)\n",
    "\n",
    "# Columns to treat\n",
    "drop_cols = ['id', 'address']\n",
    "target = 'stroke'\n",
    "\n",
    "def create_Xy(df, drop_cols, target_col):\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    return (\n",
    "        df.drop(columns=target_col),\n",
    "        df[target_col]\n",
    "    )\n",
    "    \n",
    "# New feature matrix\n",
    "X, y = (\n",
    "    stroke\n",
    "    .pipe(create_Xy, \n",
    "          drop_cols=drop_cols, \n",
    "          target_col=target,\n",
    "          )\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 123,\n",
    "                                                    stratify = y,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b66bd6-0487-4cd8-ba82-6a945382ceec",
   "metadata": {},
   "source": [
    "Now a `Pipeline` with a `RandomForestClassifier` is built and fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fabd3b7-a319-409f-b44d-b3d53ab879c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['work_type', 'smoking_status', 'who', 'gender', 'residence_type']\n",
    "numeric_cols = ['age', 'hypertension', 'heart_disease', 'ever_married', 'avg_glucose_level', 'bmi']\n",
    "missing_cols = ['age','bmi']\n",
    "\n",
    "onehot = Pipeline(steps = [\n",
    "    ('onehot', OneHotEncoder(drop = \"if_binary\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "impute = Pipeline(steps = [\n",
    "    ('impute', SimpleImputer(strategy ='mean')),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    ('onehot', onehot, categorical_cols),\n",
    "    ('impute', impute, numeric_cols)\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "base_model = RandomForestClassifier(class_weight='balanced',\n",
    "                                    max_depth=5,\n",
    "                                    random_state=123,\n",
    "                                    )\n",
    "\n",
    "base_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "base_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff25bb-c401-4a49-ac0a-03587fead474",
   "metadata": {},
   "source": [
    "Every Model in scikit-learn has a score method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f6df79-a279-4250-aec7-35d9c057b1c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7bc8884-affb-4165-a73a-f62dd5934466",
   "metadata": {},
   "source": [
    "<mark>Question</mark>\n",
    "\n",
    "What metric does the score method return (by default)?\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "    \n",
    "This is the accuracy score.\n",
    "This can be seen in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.score) and in the [source code](https://github.com/scikit-learn/scikit-learn/blob/9e38cd00d/sklearn/base.py#L738)\n",
    "\n",
    "In this example the following code would give the same result:\n",
    "```python\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_test, base_pipeline.predict(X_test))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd00f732-e053-4370-a6a0-cdf45e18cef2",
   "metadata": {},
   "source": [
    "However, due to the imbalance of the target variable, it makes sense to not look at accuracy score, but look at the area under the ROC. Which requires the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131ef44a-6654-493a-85ff-9e8f2b85bc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Find the probabilities of stroke for AUC evaluation\n",
    "y_train_probs = base_pipeline.predict_proba(X_train)[:,1]\n",
    "y_test_probs = base_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(f'AUC train: {roc_auc_score(y_train, y_train_probs)}')\n",
    "print(f'AUC test: {roc_auc_score(y_test, y_test_probs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c66ac3d-622e-4088-81e2-4d28e037d72a",
   "metadata": {},
   "source": [
    "## Overwriting the score method\n",
    "\n",
    "Since building a model requires many iterations, that could all improve the performance of the model, it would make sense for the `score` method to return the desired metric for the project. \n",
    "\n",
    "In this example, the concept of parent/child classes in OOP is very important, as the entire functionality of the model should remain the same, and the only method that needs to be changed is `score()`. Therefore the custom class can inherit from the desired model and only contain one method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c65c93e-bb2f-40f4-9aa9-7495ae28d74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomForestClassifierAUC(RandomForestClassifier):\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \n",
    "        from sklearn.metrics import roc_auc_score\n",
    "        \n",
    "        predictions = self.predict_proba(X)[:,1]\n",
    "        return roc_auc_score(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14ccd575-5de8-45c0-b8f0-f04b47429c1a",
   "metadata": {},
   "source": [
    "Now the `RandomForestClassifierAUC` is the same classifier as the original `RandomForestClassifier`.\n",
    "\n",
    "### <mark>Exercise: Use the custom Model</mark>\n",
    "\n",
    "Replace `...` with code to instantiate the new `RandomForestClassifierAUC` and calculate the new score method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9747f1d-4692-4ba1-8e73-cf726f74616b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the custom Model here\n",
    "...\n",
    "\n",
    "# leave this code the same\n",
    "base_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "base_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# calculate the score and compare to the roc_auc_score metric - has it worked?\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc59a3e-df73-44b4-a847-d4e507f41e35",
   "metadata": {},
   "source": [
    "**Answer**: Uncomment and run the code below to reveal a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f3700d-fd8e-47e1-a4e5-428bdecac602",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/overwrite_score_method.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8d70b1-f86a-4274-ae2e-5a88a4e41c0c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "Integrating a custom model with scikit-learn to overwrite the score method with the Area Under the Curve (AUC) metric brings forth a powerful and flexible solution for binary classification tasks. Custom models allow for fine-tuning and customization, enabling the incorporation of specific performance metrics like AUC into the scoring mechanism.\n",
    "\n",
    "One key advantage is the adaptability of custom models within the scikit-learn framework, providing users with the flexibility to tailor their models to specific evaluation criteria. Overwriting the score method with AUC enhances the model's ability to capture the trade-off between true positive and false positive rates, offering a more comprehensive assessment of classification performance beyond traditional accuracy metrics.\n",
    "\n",
    "Employing a custom model in scikit-learn to replace the score method with AUC facilitates personalized model evaluation for binary classification tasks. This customization not only enhances the interpretability of model performance but also aligns the evaluation process with specific objectives, providing a versatile approach for practitioners seeking to optimize models based on AUC and other relevant metrics."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
