{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/logo.png\" align='right' width=250px>\n",
    "\n",
    "# Custom Transformer in Scikit-Learn\n",
    "\n",
    "Scikit-Learn can be extended with functionality that is not included as standard in the library. \n",
    "\n",
    "By the end of this notebook you will be able to:\n",
    "\n",
    "- Explain the benefits of using custom sklearn transformers to determine when you should use them\n",
    "- Develop a custom transformer for a specific use case, adhering to the OOP design in sci-kit learn\n",
    "- Assess the functionality of a customer transformer on data and the impact that it has on model performance\n",
    "- Examine the design principles of OOP in the sci-kit learn framework to discover key patterns including attribute & method naming conventions\n",
    "\n",
    "This notebook covers\n",
    "- [The benefits of custom scikit-learn](#benefits)\n",
    "- [Building a custom Transformer on date columns](#date)\n",
    "- [Building a custom Transformer to bin data](#binning)\n",
    "- [The design of OOP in Sklearn](#design)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Transformers\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation\n",
    "\n",
    "For this notebook, the stroke dataset will be used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke = pd.read_csv('data/stroke.csv').rename(columns=str.lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's prepare the data for machine learning by creating a feature matrix and target vector and performing a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns to treat\n",
    "drop_cols = ['id']\n",
    "target = 'stroke'\n",
    "\n",
    "def create_Xy(df, drop_cols, target_col):\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    return (\n",
    "        df.drop(columns=target_col),\n",
    "        df[target_col]\n",
    "    )\n",
    "    \n",
    "# New feature matrix\n",
    "X, y = (\n",
    "    stroke\n",
    "    .pipe(create_Xy, \n",
    "          drop_cols=drop_cols, \n",
    "          target_col=target,\n",
    "          )\n",
    ")\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 123,\n",
    "                                                    stratify = y,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a state feature\n",
    "\n",
    "There is a column in the data - `address` - that isn't useful as it is as it is too granular for each entry being unique for each person:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"N unique addresses: {stroke['address'].nunique()}\") \n",
    "print(f\"N patients in the data: {stroke['id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this column either needs to be dropped or treated. \n",
    "\n",
    "It is possible to extract a useful bit of information that could be predictive - the location (state) of the patient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "(\n",
    "    stroke\n",
    "    .assign(state = lambda df: df['address'].str.split().str.get(-2))\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be made into a pandas pipeline to make it easier to apply to multiple (`X_train` and `X_test`) dataframes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_from_string(df, column, output_col, splitter=' ', word = -1, drop_column=True):\n",
    "    \"\"\"by default will split on whitespaces and return the last word\n",
    "    \"\"\"\n",
    "    df = df.assign(**{f\"{output_col}\": df[column].str.split(splitter).str.get(word)})\n",
    "    if drop_column:\n",
    "        df = df.drop(columns=column)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_state = X_train.pipe(get_word_from_string, 'address', 'state', word=-2)\n",
    "X_test_state = X_test.pipe(get_word_from_string, 'address', 'state', word=-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing scikit-learn separately\n",
    "\n",
    "Although this treatment of the `address` column is as necessary as using an imputer or one-hot encoding columns, at the moment, it is happening outside of scikit-learn.\n",
    "\n",
    "After the transformation has been performed, we can use a scikit-learn pipeline to perform the remaining preprocessing (treating `state` as a categorical feature) and the modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['work_type', 'smoking_status', 'who', 'gender', 'residence_type', 'state']\n",
    "numeric_cols = ['age', 'hypertension', 'heart_disease', 'ever_married', 'avg_glucose_level', 'bmi']\n",
    "missing_cols = ['age', 'bmi']\n",
    "\n",
    "onehot = Pipeline(steps = [\n",
    "    ('onehot', OneHotEncoder(drop = \"if_binary\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "impute = Pipeline(steps = [\n",
    "    ('impute', SimpleImputer(strategy ='mean')),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    ('onehot', onehot, categorical_cols),\n",
    "    ('impute', impute, numeric_cols)\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "base_model = RandomForestClassifier(class_weight='balanced',\n",
    "                                    max_depth=5,\n",
    "                                    random_state=123,\n",
    "                                    )\n",
    "\n",
    "base_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "base_pipeline.fit(X_train_state, y_train)\n",
    "\n",
    "\n",
    "# Find the probabilities of stroke for AUC evaluation\n",
    "y_train_probs = base_pipeline.predict_proba(X_train_state)[:,1]\n",
    "y_test_probs = base_pipeline.predict_proba(X_test_state)[:,1]\n",
    "\n",
    "print(f'AUC train: {roc_auc_score(y_train, y_train_probs)}')\n",
    "print(f'AUC test: {roc_auc_score(y_test, y_test_probs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A better way\n",
    "\n",
    "Currently, we are using a mix of pandas and scikit-learn to perform feature engineering.\n",
    "\n",
    "However, a more structured and scalable approach is possible: performing the preprocessing entirely within scikit-learn is preferable because you are already working within its broader ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ ðŸŽ‰ \n",
    "\n",
    "## Building a custom sklearn Transformer\n",
    "\n",
    "To build your own custom Transformer, you will be extending the sklearn library. To do this, the concept of parent/child classes in OOP is very important as there are two classes you need to inherit from:\n",
    "- [BaseEstimator](https://scikit-learn.org/stable/modules/generated/sklearn.base.BaseEstimator): All sklearn Transformers (and Models) are built upon this fundamental base class. \n",
    "- [TransformerMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin): All Transformers are built upon this Mixin class. It's what links the `.fit()` and `.transform()` method to make the `.fit_transform()` method!\n",
    "\n",
    "*The design principles of sklearn, including the difference between Transformers, Models and Estimators will be covered later in this notebook!*\n",
    "\n",
    "Let's first see how to build your own Transformer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the required parent clasess\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A custom Transformer requires the following methods:\n",
    "\n",
    "- `__init__`\n",
    "   - To initialise the class when instantiated and can have optional parameters\n",
    "   - Optional return value\n",
    "- `fit`\n",
    "   - To learn what is needed to be able to make a transformation (eg. calculating the imputation value for `SimpleImpute`)\n",
    "   - Requires the object itself to be the return value (`return self`) to align to the sklearn OOP design.\n",
    "- `transform`\n",
    "   - The actual process of transforming the data (eg. filling the missing values with the calculated value from the `.fit()` method)\n",
    "   - Requires the return value to be the transformed data - to adhere to the design pattern and can be used later in the pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_is_fitted\n",
    "class WordExtractor(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, splitter, word, features_out):\n",
    "\n",
    "        if not splitter or not word:\n",
    "            raise ValueError(f\"Two columns need to be passed\")\n",
    "            \n",
    "        self.splitter = splitter\n",
    "        self.word = word\n",
    "        self.features_out = features_out\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self._extract_word = lambda col: col.str.split(self.splitter).str.get(self.word)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \n",
    "        # check if the transformer has been fitted\n",
    "        # check_is_fitted(self)\n",
    "        \n",
    "        X = X.apply(self._extract_word)\n",
    "        \n",
    "        return X\n",
    "\n",
    "    def get_feature_names_out(self, input_features=None):\n",
    "        return self.features_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_extractor = WordExtractor(splitter = ' ', word = -2, features_out = ['state'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the extract can be used as you would use any Transformer from sklearn.\n",
    "\n",
    "Note that the transformer can only be used on string columns, so it is required to select only string columns when using the Transformer in isolation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_extractor.fit(X_train[['address']])\n",
    "word_extractor.transform(X_train[['address']]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is now a Transformer in sklearn, which means you can use the standard practices of the sklearn library, for example using `.fit_transform()` instead of `.fit()` followed by `.transform()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_extractor.fit_transform(X_train[['address']]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is despite the Transformer having no `fit_transform` method included in the class. \n",
    "\n",
    "<mark>**Questions**</mark>\n",
    "\n",
    "1. Why is `fit_transform` available as a method when it wasn't writted as part of the `WordExtractor` class?\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "    \n",
    "`WordExtractor` has inherited from the TransformerMixin, which includes the `fit_transform` method.\n",
    "This can be seen in the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html) and in the [source code](https://github.com/scikit-learn/scikit-learn/blob/9e38cd00d032f777312e639477f1f52f3ea4b3b7/sklearn/base.py#L1006).\n",
    "\n",
    "</details>\n",
    "\n",
    "2. Can you think of any other methods or functionalities that we could use with this Transformer?\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "    \n",
    "One option is to incorporate this Transformer in the `ColumnTransformer` to only apply it to the desired column.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1.\n",
    "2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing a full pipeline with a Custom Transformer\n",
    "\n",
    "Transformers in scikit-learn are designed to be applied to the entire dataset, unless passed through the ColumnTransformer.\n",
    "\n",
    "So... can ColumnTransformer be used with this new custom word extractor Transformer?\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">ðŸš¨ Spoiler alert!</span></summary>\n",
    "    \n",
    "Yes it can. Since the Transformer has been built using the scikit-learn standard, it absolutely can!\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = ['address']\n",
    "\n",
    "word_extractor_ct = ColumnTransformer(transformers = [\n",
    "    ('word_extractor', word_extractor, string_columns),\n",
    "], remainder = 'passthrough').set_output(transform=\"pandas\")\n",
    "\n",
    "word_extractor_ct.fit_transform(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are a little over the top, when all that's been changed is `address` to `state`.\n",
    "\n",
    "Since there is an existing method in the transformer that returns the features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_extractor_ct.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ColumnTransformer can be updated so that this feature name is used by adding the parameter\n",
    "\n",
    "`verbose_feature_names_out=False`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "string_columns = ['address']\n",
    "\n",
    "word_extractor_ct = ColumnTransformer(transformers = [\n",
    "    ('word_extractor', word_extractor, string_columns),\n",
    "], remainder = 'passthrough', verbose_feature_names_out=False).set_output(transform=\"pandas\")\n",
    "\n",
    "word_extractor_ct.fit_transform(X_train).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding this to the full Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['work_type', 'smoking_status', 'who', 'gender', 'residence_type', 'state']\n",
    "numeric_cols = ['age', 'hypertension', 'heart_disease', 'ever_married', 'avg_glucose_level', 'bmi']\n",
    "missing_cols = ['age', 'bmi']\n",
    "\n",
    "word_extractor_ct = ColumnTransformer(transformers = [\n",
    "    ('word_extractor', word_extractor, string_columns),\n",
    "], remainder = 'passthrough', verbose_feature_names_out=False).set_output(transform=\"pandas\")\n",
    "\n",
    "onehot = Pipeline(steps = [\n",
    "    ('onehot', OneHotEncoder(drop = \"if_binary\", sparse_output=False)),\n",
    "])\n",
    "\n",
    "impute = Pipeline(steps = [\n",
    "    ('impute', SimpleImputer(strategy ='mean')),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    ('onehot', onehot, categorical_cols),\n",
    "    ('impute', impute, numeric_cols)\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "forest_model = RandomForestClassifier(class_weight='balanced',\n",
    "                                    max_depth=5,\n",
    "                                    random_state=123,\n",
    "                                    )\n",
    "\n",
    "base_pipeline = Pipeline(steps=[\n",
    "    ('word_extractor', word_extractor_ct),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', forest_model)\n",
    "])\n",
    "\n",
    "base_pipeline.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Find the probabilities of stroke for AUC evaluation\n",
    "y_train_probs = base_pipeline.predict_proba(X_train)[:,1]\n",
    "y_test_probs = base_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(f'AUC train: {roc_auc_score(y_train, y_train_probs)}')\n",
    "print(f'AUC test: {roc_auc_score(y_test, y_test_probs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Incorporating a **custom transformer** to extract substrings from a column and seamlessly integrating it into the powerful `ColumnTransformer` provides the flexibility and extensibility of the Scikit-Learn `Pipeline`.\n",
    "\n",
    "You can now carry out feature engineering and data preprocessing without relying on external tools like pandas. The use of transformers and pipelines not only enhances code modularity but also fosters a more efficient and scalable approach to machine learning workflows."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
