{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "extended-tomato",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "\n",
    "# Parameter tuning\n",
    "\n",
    "So far, you have focused on increasing model performance by making the best out of your data (with categorical feature encodings and imputation of missing values). In this notebook, you will rather focus on the model and its hyperparameters and explore best practices for hyperparameter tuning with Scikit-Learn.\n",
    "\n",
    "**Program**\n",
    "- [Baseline model](#review)\n",
    "    - [<mark>Exercise</mark>](#ex1)\n",
    "- [Addressing Overfitting](#overfitting)\n",
    "- [Reducing the complexity](#reduce)\n",
    "- [Hyperparameter Tuning](#hyper)\n",
    "    - [<mark>Bonus: Build the best model</mark>](#bonus)\n",
    "- [Conclusion and next steps](#conc)\n",
    "\n",
    "Let's first import all the libraries you will need for this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99168b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.metrics import RocCurveDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381e98f8",
   "metadata": {},
   "source": [
    "We will again work with the `stroke` data from before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33cc573",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke = pd.read_csv('data/stroke.csv').rename(columns=str.lower)\n",
    "stroke.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-bearing",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "The Decision Tree Classifier (using categorical features) from the previous notebook will act as the baseline model going forward. \n",
    "\n",
    "Below, you can see the pipeline we created in the first notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924ea0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable definitions\n",
    "categorical_cols = ['work_type', 'smoking_status', 'who', 'gender', 'residence_type']\n",
    "missing_cols = ['age','bmi']\n",
    "drop_cols = ['id','address']\n",
    "\n",
    "target = 'stroke'\n",
    "\n",
    "def create_Xy(df, drop_cols, target_col):\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    return (\n",
    "        df.drop(columns=target_col),\n",
    "        df[target_col]\n",
    "    )\n",
    "\n",
    "# Create X and y\n",
    "X, y = stroke.pipe(create_Xy, \n",
    "                   drop_cols=drop_cols, \n",
    "                   target_col=target,\n",
    "                   )\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 42,\n",
    "                                                    stratify = y,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-yorkshire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 2: Instantiate model and set parameters\n",
    "model = DecisionTreeClassifier(max_depth=3, \n",
    "                               class_weight='balanced', \n",
    "                               random_state=123)\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(drop=\"if_binary\"), categorical_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "preprocessing = Pipeline(steps=[\n",
    "    ('onehot', ct),\n",
    "    ('impute', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', model)\n",
    "])\n",
    "\n",
    "# Step 3: Train model\n",
    "pipeline.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-clarity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate model\n",
    "fig, ax = plt.subplots()\n",
    "RocCurveDisplay.from_estimator(pipeline, X_train, y_train, ax=ax, name='Train')\n",
    "RocCurveDisplay.from_estimator(pipeline, X_test, y_test, ax=ax, name='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c14473",
   "metadata": {},
   "source": [
    "The training and test AUC are quite good already, but we can surely get them higher, right?\n",
    "\n",
    "#### <mark>Exercise:</mark> Changing model parameters\n",
    "\n",
    "Change the hyperparameters of the `DecisionTreeClassifier` to improve its performance.\n",
    "\n",
    "1. What is the default parameter value for `max_depth`? What does it do?\n",
    "2. Change the `max_depth` parameter to improve model performance. What happens to the training and test AUC, if you leave it at the default?\n",
    "3. Find out what other hyperparameters you can tune (see [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)). Which ones would be worth changing as well?\n",
    "4. The model is overfitting with the default parameters. What does ***overfitting*** mean?\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "\n",
    "1. Default is `None`, meaning there is no restriction on the maximum depth.    \n",
    "2. Setting `max_depth` to 20 will result in worse test accuracy and almost 100% training accuracy = the model is overfitting the training data.\n",
    "3. `min_sample_leaf` and `min_sample_split` are also parameters that may be worth tuning. With the defaults `1` and `2`, the model is allowed to create splits that could separate individual samples, which is part of why it is overfitting so drastically.\n",
    "4. Overfitting means the model fitted too well (or even perfectly) on the training data and did not learn a meaningful pattern. It is thus too specific and cannot generalize well to new, unseen data.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84acec18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the parameters!\n",
    "new_model = DecisionTreeClassifier(\n",
    "    max_depth=3,\n",
    "    # add other parameters here\n",
    "    class_weight='balanced', \n",
    "    random_state=123)\n",
    "\n",
    "new_pipeline = Pipeline(steps=[\n",
    "    ('preprocessing', preprocessing),\n",
    "    ('model', new_model)\n",
    "])\n",
    "\n",
    "new_pipeline.fit(X_train,y_train)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "RocCurveDisplay.from_estimator(new_pipeline, X_train, y_train, ax=ax, name='Train')\n",
    "RocCurveDisplay.from_estimator(new_pipeline, X_test, y_test, ax=ax, name='Test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/03-changing-hyperparameters.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38d2456e",
   "metadata": {},
   "source": [
    "<mark>**Bonus:**</mark> \n",
    "\n",
    "**Precision**, **Recall** and their harmonic mean - the **F1-Score** - are other common metrics for classifiers. \n",
    "- Look up what they mean and why they are useful.\n",
    "- Calculate Precision & Recall and the F1-Score for the model (hint: Use the function `classification_report()` from `sklearn.metrics`)\n",
    "- Would you focus on maximizing precision or recall in this use case?\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "\n",
    "The **precision** for a class is the number of correctly classified positives (e.g., stroke classifications that actually had stroke) divided by the total number of all classified positives, i.e., the sum of true positives and false positives (total positive stroke classifications).\n",
    "\n",
    "**Recall** is defined as the number of true positives divided by the total number of true positives in the dataset (e.g., how many actual stroke victims did the model correctly identify).\n",
    "\n",
    "<img src=\"images/precision_recall.png\" width=\"500\" align=\"center\">\n",
    "\n",
    "**F1 score** combines both precision and recall by calculating their harmonic mean:\n",
    "\n",
    "$${\\displaystyle F_{1}={\\frac {2}{\\mathrm {recall} ^{-1}+\\mathrm {precision} ^{-1}}}=2\\cdot {\\frac {\\mathrm {precision} \\cdot \\mathrm {recall} }{\\mathrm {precision} +\\mathrm {recall} }}={\\frac {\\mathrm {tp} }{\\mathrm {tp} +{\\frac {1}{2}}(\\mathrm {fp} +\\mathrm {fn} )}}}$$\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c063b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6eb23a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/03-metrics-classification-report.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cheap-hopkins",
   "metadata": {},
   "source": [
    "<a id=overfitting></a>\n",
    "\n",
    "## Addressing Overfitting\n",
    "\n",
    "The results above show us that the algorithm can severely overfit on the training data.\n",
    "\n",
    "There are many ways to address overfitting, including:\n",
    "\n",
    "- **Train with more data**: It won't work every time, but training with more data can help algorithms detect the signal better. This can involve data augmentation if needed.\n",
    "- **Remove features**: If you have a large number of features, you should only select the most important features for training so that the model doesn’t learn from features that don't generalise well. \n",
    "- ★ **Reduce the complexity of the model**: An over-complex model is more likely to overfit. You can directly reduce the model’s complexity by looking at the type of model, or the specific model parameters.\n",
    "\n",
    "Let's focus on the easiest to begin with - reducing the complexity of the model.\n",
    "\n",
    "<a id=reduce></a>\n",
    "### Reduce the complexity of the model\n",
    "\n",
    "Leaving the `max_depth` parameter unrestricted increases model complexity a lot.\n",
    "\n",
    "We can see this by plotting the default tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distant-remove",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "plot_tree(new_pipeline.named_steps['model'], ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-humor",
   "metadata": {},
   "source": [
    "<mark>**Question:**</mark> What is the maximum number of end leaf nodes a tree could get to with a max depth of 50?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cultural-separation",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "\n",
    "   Each split generates two new leaves. So 50 splits generate 2^50 (1.1 trillion) end nodes.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-funds",
   "metadata": {},
   "source": [
    "<a id=hyper></a>\n",
    "\n",
    "### Hyperparameter Tuning\n",
    "\n",
    "It would of course be a good idea to do some hyperparameter tuning to find the best value for `max_depth` (and/or other parameters).\n",
    "\n",
    "In the exercise before, you have tried to tune these model parameters by hand. Of course, this is not the best idea since it is time-consuming and inefficient.\n",
    " \n",
    "For this problem, sklearn already contains a **Grid Search** algorithm called `GridSearchCV` which will allow you to test different paramter combinations on the dataset using ***Cross Validation***.\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">Refresher Cross-Validation</span></summary>\n",
    "\n",
    "Cross Validation (CV), specifically k-fold cross validation allows you to train and test your algorithm on multiple, mutually exclusive subsets of your data, giving you a better estimate of the true performance. \n",
    "\n",
    "Hereby, the data is split into k equally sized subsets, whereby one is used as a validation set and the remaining k-1 for training (repeating the process for k iterations). After wards, the performance metrics of the k train-validation iterations are averaged.\n",
    "\n",
    "Usually, a small proportion of the data is held out completely as a separate test set for final evaluation.\n",
    "\n",
    "<img src=\"images/crossvalidation.png\" style=\"display: block;margin-left: auto;margin-right: auto;height: 300px\"/>\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjusted-edwards",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-gross",
   "metadata": {},
   "source": [
    "The `GridSearchCV` object requires a parameter grid (as a `{key: value}` dictionary) with the different options you want to explore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-message",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'preprocessing__impute__strategy': ['mean', 'median'],\n",
    "    'model__max_depth': range(1, 21),\n",
    "    'model__criterion': ['gini','entropy',],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-shooting",
   "metadata": {},
   "source": [
    "You can then perform a parameter grid sesarch to optimise your chosen metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-richards",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = GridSearchCV(pipeline, params, scoring='roc_auc', cv = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-exchange",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = pd.DataFrame(grid.cv_results_).sort_values('rank_test_score')\n",
    "cv_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79730c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    cv_results\n",
    "    .groupby('param_model__max_depth')\n",
    "    ['mean_test_score']\n",
    "    .mean()\n",
    "    .plot(title = 'Test AUC by max_depth', \n",
    "          xticks = range(0,21,2)\n",
    "          )\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f02abeb",
   "metadata": {},
   "source": [
    "We can then select the best model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaptive-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = grid.best_estimator_\n",
    "model_best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-wisconsin",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.score(X_train, y_train), model_best.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "through-paintball",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "RocCurveDisplay.from_estimator(pipeline, X_train, y_train, ax=ax[0], name='Train')\n",
    "RocCurveDisplay.from_estimator(pipeline, X_test, y_test, ax=ax[0], name='Test')\n",
    "ax[0].set(title='Base model')\n",
    "\n",
    "RocCurveDisplay.from_estimator(model_best, X_train, y_train, ax=ax[1], name='Train')\n",
    "RocCurveDisplay.from_estimator(model_best, X_test, y_test, ax=ax[1], name='Test')\n",
    "ax[1].set(title='Tuned model');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21265345",
   "metadata": {},
   "source": [
    "<a id=bonus></a>\n",
    "\n",
    "### <mark>Bonus:</mark> Build your best model\n",
    "\n",
    "It is now your turn to put it all together and see if you can build a better model.\n",
    "\n",
    "1. Build a model pipeline that **maximizes test performance (AUC)**\n",
    "2. Select a machine learning model of your choice (see here for [sklearn selection](https://scikit-learn.org/stable/supervised_learning.html))\n",
    "3. Look up what hyperparameters it has and perform a grid search to tune its hyperparameters.\n",
    "\n",
    "*Hint: In real use cases, more complex models such as Random Forests, Support Vector Machines, or Gradient Boosting models are more often used (as they are generally more performant).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c05ad41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186a74ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/03-better-model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ac28fc-257d-4c16-a8c7-1e559032e5c4",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "  <summary><span style=\"color:blue\">Read about how to add different models to your hyperparameter search here. </span></summary>\n",
    "  \n",
    "## More control over the hyperparameter search\n",
    "\n",
    "Let's now consider a scenario where you want to compare two different classifiers, a support vector machine and a random forest one.\n",
    "\n",
    "Naively, you could create a dictionary like the following:\n",
    "\n",
    "```python\n",
    "all_parameters = {'classifier': [SVC(), RandomForestClassifier()],\n",
    "                  'classifier__C': [.5, 1, 1.5], # SVC hyperparam\n",
    "                  'classifier__kernel': [\"linear\", \"poly\", \"rbf\"], # SVC hyperparam\n",
    "                  'classifier__n_estimators': [40, 60, 90], # RFC hyperparam\n",
    "                  'classifier__max_depth': [2, 3, 5, 10], # RFC hyperparam\n",
    "                  'classifier__min_samples_leaf': [1,5,8] # RFC hyperparam\n",
    "                   }\n",
    "```\n",
    "However, if you use this dictionary with `GridSearchCV`, the search will explore all possible combinations of hyperparameters. For example, it will fit pipelines with a `RandomForestClassifier()` for every combination of the `C` and `kernel` hyperparameters. This would be a waste of resources since those hyperparameters are not related to the `RandomForestClassifier()`. \n",
    "\n",
    "To avoid this issue, `GridSearchCV` also accepts a _list_ of dictionaries as an input to give you more control over what combinations of hyperparameters are tested. It will only compare all possible combination of hyperparameters *within* each dictionary.\n",
    "\n",
    "\n",
    "```python\n",
    "svc_parameters = {'classifier': [SVC()],\n",
    "                  'classifier__C': [.5, 1, 1.5],\n",
    "                  'classifier__kernel': [\"linear\", \"poly\", \"rbf\"],\n",
    "                   }\n",
    "```\n",
    "\n",
    "```python\n",
    "rf_parameters = {'classifier': [RandomForestClassifier()],\n",
    "                 'classifier__n_estimators': [40, 60, 90],\n",
    "                 'classifier__max_depth': [2, 3, 5, 10],\n",
    "                 'classifier__min_samples_leaf': [1,5,8]\n",
    "                   }\n",
    "```\n",
    "\n",
    "```python\n",
    "grid_all = GridSearchCV(pipeline, \n",
    "                        [svc_parameters, rf_parameters], \n",
    "                        scoring='accuracy')\n",
    "```\n",
    "\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-pledge",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=conc></a>\n",
    "# Conclusion and Next Steps\n",
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "This notebook has covered an overview of ensemble algorithms, with a particular focus on the random forest. Also covered was overfitting and using hyperparameter tuning to correct issues of overfitting. \n",
    "\n",
    "In the next notebook you will explore another ensemble algorithm - Gradient Boosting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
