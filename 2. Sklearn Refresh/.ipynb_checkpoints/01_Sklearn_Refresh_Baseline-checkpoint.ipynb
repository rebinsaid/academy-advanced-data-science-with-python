{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "lyric-hazard",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "\n",
    "# Scikit Learn Refresh\n",
    "\n",
    "This notebook is a refresher on using Scikit-Learn to build a machine learning model.\n",
    "\n",
    "You will start by building a baseline model and then iterate on it in later notebooks to improve its performance.\n",
    "\n",
    "- [About the data](#about)\n",
    "- [<mark>Exercise: Data Exploration</mark>](#explore)\n",
    "- [Establishing a baseline](#baseline)\n",
    "- [Generalisation (splitting the data)](#splitting)\n",
    "- [Model creation and evaluation](#build)\n",
    "- [Conclusion and next steps](#conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cultural-relay",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prepared-service",
   "metadata": {},
   "source": [
    "<a id=about></a>\n",
    "## About the data\n",
    "\n",
    "<img src='images/who.png' width='500px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "According to the World Health Organization (WHO), strokes are the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n",
    "\n",
    "You will use this dataset to build a model that can **predict whether a patient is likely to have a `stroke`** (based on input parameters like gender, age and whether or not they smoke). \n",
    "\n",
    "Each row in the data provides relavant information about the patient.\n",
    "### Features\n",
    "\n",
    "1. `id`: unique identifier\n",
    "1. `address`: a general address (city/county, state and postal code)\n",
    "1. `gender`: \"Male\", \"Female\" or \"Other\"\n",
    "1. `age`: age of the patient\n",
    "1. `hypertension`: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
    "1. `heart_disease`: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
    "1. `ever_married`: \"No\" or \"Yes\"\n",
    "1. `work_type`: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
    "1. `residence_type`: \"Rural\" or \"Urban\"\n",
    "1. `avg_glucose_level`: average glucose level in blood\n",
    "1. `bmi`: body mass index\n",
    "1. `smoking_status`: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"\n",
    "1. `stroke`: 1 if the patient had a stroke or 0 if not\n",
    "\n",
    "*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-witness",
   "metadata": {},
   "source": [
    "<a id=explore></a>\n",
    "\n",
    "## Data Exploration\n",
    "\n",
    "Let's start by importing the data and investigating its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd27ec7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke = pd.read_csv('data/stroke.csv').rename(columns=str.lower)\n",
    "stroke.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-hungary",
   "metadata": {},
   "source": [
    "### <mark>Exercise</mark>\n",
    "\n",
    "Conduct a preliminary analysis of the data by answering the questions below.\n",
    "\n",
    "1. Take a look at missing values:\n",
    "    - a) Are there any missing values in the data?\n",
    "    - b) Use seaborn's `sns.heatmap` (on `df.isnull()`) to determine if there is a pattern to the missing values.\n",
    "    - c) How will missing values impact training a machine learning model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46f090",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd034853",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"color:blue\">Show text answers</span></summary>\n",
    "\n",
    "If a sample contains missing values for one or more features (or the outcome), this observation cannot be used to train the ML model. \n",
    "\n",
    "Generally there are two options: \n",
    "1) Either you drop the sample entirely, or \n",
    "2) you try to fill/interpolate the missing value by using a constant, the median/mean feature value, or prediction using another (ML) model. We will discuss this in detail later.  \n",
    "  \n",
    "\n",
    "</details>\n",
    "\n",
    "*Uncomment cell below to see the code answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "academic-perfume",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/01-data-exploration-1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-singles",
   "metadata": {},
   "source": [
    "2. Investigate the feature columns:\n",
    "    - a) What are the datatypes of each feature? \n",
    "    - b) Do they make sense?\n",
    "    - c) How will different types of features impact training a machine learning model?\n",
    "    - d) Which features do you think will be informative for predicting stroke?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998fb41f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a398e89e",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"color:blue\">Show text answers</span></summary>\n",
    "\n",
    "b) They mostly make sense. In Pandas, the `object` Dtype usually refers to strings. `age` being float64 is odd, since we usually measure age in full years. It would be good to investigate, if age is indeed only in full years or in fractions. Try: `stroke['age'].unique()`<br>\n",
    "c) Computers cannot interpret strings. In order to use string/text features, you will have to find a way to encode strings into numbers. More about this later.<br>\n",
    "d) Except the `id` column, all of the features could be potentially informative. However, `gender` and `who` could potentially encode the same information. Try: `stroke['who'].unique()`\n",
    "</details>\n",
    "\n",
    "*Uncomment cell below to see the code answer:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-millennium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/01-data-exploration-2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c5454",
   "metadata": {},
   "source": [
    "**Bonus:** Investigate if there are any large outliers in the data. What effect would large outliers have on training a machine learning model? How would you treat them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-leone",
   "metadata": {},
   "source": [
    "<a id=baseline></a>\n",
    "\n",
    "## Establishing a baseline\n",
    "\n",
    "In a first step, you will develop a simple baseline model.\n",
    "\n",
    "\n",
    "<mark>**Question**:</mark> Why is a baseline model useful?\n",
    "\n",
    "<details>\n",
    "<summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "A baseline provides a point of reference from which to compare each model iteration as we build up our model and features. \n",
    "\n",
    "Without a baseline, you have **no point of reference** to consider whether you are continuing to add value with more complex features/ models. It defines the **hurdle** that all other machine learning algorithms must cross to demonstrate “skill” on the problem.\n",
    "</details>\n",
    "\n",
    "\n",
    "The simplest model would be to always predict the most likely outcome:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467cf249",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke['stroke'].value_counts(\n",
    "    # normalize = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-conspiracy",
   "metadata": {},
   "source": [
    "In this case, you would always predict no stroke, which would yield a model accuracy of ~95%! This is good, right?\n",
    "\n",
    "<mark>**Question:**</mark> Why is accuracy a flawed metric in this case?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ignored-scotland",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "  \n",
    "Especially with unbalanced outcomes such as here, high model accuracy does not mean you have a good model, because only ever predicting the majority outcome would already lead to a high accuracy. What you would like the model to do is to make informed predictions. To capture model performance, you will need more refined metrics for this (more about this later).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "private-portsmouth",
   "metadata": {},
   "source": [
    "Alternatively, you could train a simple machine learning model with minimal preprocessing of your data.\n",
    "\n",
    "Let's see what the low hanging fruit is by:\n",
    "- Dropping any categorical (object) columns \n",
    "- Dropping columns with missing values\n",
    "- Dropping the `id` column\n",
    "\n",
    "<!-- - Building a Decision Tree with a max depth of 2\n",
    "- Using `class_weight='balanced'` to address the imbalance of data -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rapid-clinton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping columns \n",
    "\n",
    "categorical_cols = ['work_type', 'smoking_status', 'who', 'gender', 'residence_type']\n",
    "missing_cols = ['bmi', 'age']\n",
    "drop_cols = ['id','address']\n",
    "\n",
    "target = 'stroke'\n",
    "\n",
    "X = stroke.drop(columns = drop_cols \n",
    "                        + categorical_cols\n",
    "                        + missing_cols\n",
    "                        + [target])\n",
    "y = stroke[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convenient-miller",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "muslim-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "continuing-reaction",
   "metadata": {},
   "source": [
    "<a id=splitting></a>\n",
    "\n",
    "## Splitting the dataset\n",
    "\n",
    "In the next step, you will split the dataset into a training set to train the model and a test set assess model performance.\n",
    "\n",
    "<mark>**Question**:</mark> Why is train-test splitting a good idea?\n",
    "\n",
    "<details>\n",
    "<summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "  \n",
    "An important goal of machine learning is to create a model that does not only do well on the data that it has already seen, but will also perform well under new circumstances on data that is **has not seen before**. We call this ***generalisation***. \n",
    "\n",
    "This why it's import to separate our dataset into two parts:\n",
    "* The _training_ set: This is the data (features and targets) that will **guide the learning process**. \n",
    "* The _test_ set: This is the data (features and targets) that will be used to **evaluate** how well our model has learned on *unseen, new* data. \n",
    "\n",
    "</details>\n",
    "\n",
    "Scikit-learn's `train_test_split()` function allows you to split the data in a train- and test set. By default, the test set size is set to 25% and the data is shuffled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.25, \n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f4f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-inside",
   "metadata": {},
   "source": [
    "<mark> **Question:**</mark> What does the parameter `stratify` do and why would you use it?\n",
    "\n",
    "<details>\n",
    "<summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "  \n",
    "The stratify argument makes sure that the same proportion of stroke to no stroke is present in the training and test set. This is especially important with highly imbalanced data (as the likelihood of an unproportional split is increased). \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "supreme-helping",
   "metadata": {},
   "source": [
    "<a id=build></a>\n",
    "\n",
    "## Model creation and evaluation\n",
    "\n",
    "You are now ready to build a machine learning model! \n",
    "\n",
    "Scikit-learn has a rich [collection of algorithms](https://scikit-learn.org/stable/supervised_learning.html) readily available.\n",
    "\n",
    "Normally, one model will be sufficient to establish a baseline. However, for educational purposes, we shall explore a few more simple models. \n",
    "\n",
    "For each model, the process will be as follows:\n",
    "\n",
    "#### Scikit-Learn API usage\n",
    "\n",
    "When training a model, the steps you will need to take in sklearn are always the same:\n",
    "1. First, choose a model class and import that model \n",
    "2. Choose the model hyperparameters by instantiating this class with desired values.\n",
    "3. Train the model to the preprocessed train data by calling the `fit()` method of the model instance.\n",
    "4. Evaluate the model's performance using suitable metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-brooklyn",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "\n",
    "Let's investigate the performance of a simple model such as the Decision Tree classifier.\n",
    "\n",
    "Since the data is very imbalanced, you can add `class_weight='balanced'` as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815af40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 2: Instantiate model and set parameters\n",
    "tree_model = DecisionTreeClassifier(max_depth=3, \n",
    "                                    class_weight='balanced',\n",
    "                                    random_state=42)\n",
    "\n",
    "# Step 3: Train model\n",
    "tree_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfe5c0e-6f6f-4b35-b23a-8ebe07838c24",
   "metadata": {},
   "source": [
    "You already established that accuracy would be a bad metric to use for this particular dataset.\n",
    "\n",
    "A better metric is the ***Receiver Operating Characteristic - Area Under the Curve*** (ROC-AUC). We will discuss the ROC-AUC more later, but for now: **The higher the ROC-AUC, the better the model** is at predicting 0 classes as 0 and 1 classes as 1. \n",
    "\n",
    "Let's see what score you get when using this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "piano-tissue",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "y_pred_test = tree_model.predict_proba(X_test)[:,1]\n",
    "y_pred_train = tree_model.predict_proba(X_train)[:,1]\n",
    "\n",
    "print(f'AUC score: {roc_auc_score(y_test, y_pred_test), roc_auc_score(y_train, y_pred_train)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "european-resistance",
   "metadata": {},
   "source": [
    "Although the Decision Tree is not necessarily the most high performant model, it is very interpretable. This can be helpful if you need to explain your model to non-technical stakeholders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-album",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(16,4))\n",
    "\n",
    "plot_tree(tree_model, \n",
    "          ax=ax, \n",
    "          feature_names=list(X_train.columns), \n",
    "          class_names=['no stroke', 'stroke'],\n",
    "          );"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-nothing",
   "metadata": {},
   "source": [
    "### <mark>Exercise: Logistic regression</mark>\n",
    "\n",
    "Implement a Logistic Regression model to see how it performs. Make sure to add the `class_weight` parameter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a44bbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-uganda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/01-logistic_regression.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed62cc2",
   "metadata": {},
   "source": [
    "## Diving into ROC-AUC\n",
    "\n",
    "You've used ROC-AUC now as a metric, but how exactly is it calculated?\n",
    "\n",
    "Let's unpack these questions by starting to look at a confusion matrix.\n",
    "\n",
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix summarises the predictions of a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quick-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "ConfusionMatrixDisplay.from_estimator(\n",
    "         tree_model, \n",
    "         X_test, y_test,);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4c357a",
   "metadata": {},
   "source": [
    "<mark>**Question:** For this problem, which numbers would you be most interested in increasing/decreasing? "
   ]
  },
  {
   "cell_type": "raw",
   "id": "c7308ad2",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e25dc",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "  \n",
    "We would like to see true 0's predicted as 0, and true 1's predicted as 1. Thus, ideally, all predictions should be on the main diagonal of the confusion matrix.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef50032",
   "metadata": {},
   "source": [
    "### ROC and AUC\n",
    "\n",
    "Typically a binary classifier will use a probability of 0.5 as its threshold for classification.\n",
    "\n",
    "<img src='images/threshold.png' width='400px' style=\"padding: 15px\">\n",
    "\n",
    "However, adjusting this threshold may be beneficial for the purpose of your model.\n",
    "\n",
    "Let's investigate how adjusting the classification threshold affects one of the models you learned previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d301be46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_threshold import CustomThreshold\n",
    "\n",
    "thresholds = [0, 0.25, 0.5]\n",
    "\n",
    "fig, ax = plt.subplots(1,len(thresholds), figsize=(16,8))\n",
    "\n",
    "for idx, threshold in enumerate(thresholds):\n",
    "\n",
    "    tree = CustomThreshold(tree_model, threshold)\n",
    "    \n",
    "    ConfusionMatrixDisplay.from_estimator(\n",
    "         tree, \n",
    "         X_test, y_test,\n",
    "         ax=ax[idx], colorbar=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd2643b",
   "metadata": {},
   "source": [
    "The **Receiver Operating Characteristics (ROC) curve**, is a graphical plot which illustrates the performance of a classifier at various threshold settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438f5369",
   "metadata": {},
   "source": [
    "It is created by plotting the **True Positive Rate (TPR)** (the fraction of true positives out of the positives) against the **False Positive Rate (FPR)** (the fraction of false positives out of the negatives), at various threshold settings.\n",
    "\n",
    "$$TPR = \\frac{TP}{TP + FN} \\ \\ \\  \\text{vs.} \\ \\ \\ FPR = \\frac{FP}{TN + FP}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1868ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "RocCurveDisplay.from_estimator(tree_model, X_train, y_train, ax=ax[0], name='Train')\n",
    "RocCurveDisplay.from_estimator(tree_model, X_test, y_test, ax=ax[0], name='Test')\n",
    "ax[0].set(title='Decision Tree model');\n",
    "\n",
    "# RocCurveDisplay.from_estimator(logistic_model, X_train, y_train, ax=ax[1], name='Train')\n",
    "# RocCurveDisplay.from_estimator(logistic_model, X_test, y_test, ax=ax[1], name='Test')\n",
    "# ax[1].set(title='Logistic model')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f9e67",
   "metadata": {},
   "source": [
    "Typically, the ideal situation is the top left corner of the plot: a FPR of zero and a TPR of one. \n",
    "\n",
    "Whilst this may not be realistic, it does mean that a larger **Area Under the Curve (AUC)** is usually better. The “steepness” of ROC curves is also important, since it is ideal to maximize the TPR while minimizing the FPR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thermal-tiffany",
   "metadata": {},
   "source": [
    "<mark>**Question:** Why is the AUC score a good metric to use in this use case?</mark>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "atomic-republic",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b1135b",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "    \n",
    "   AUC is a better metric here due to the imbalanced data issue: ROC analysis does not have any bias toward models that perform well on the minority class at the expense of the majority class - a property that is quite attractive when dealing with imbalanced data.\n",
    "    \n",
    "[Page 27, Imbalanced Learning: Foundations, Algorithms, and Applications, 2013](https://www.amazon.com/dp/1118074629/ref=as_li_ss_tl?&linkCode=sl1&tag=inspiredalgor-20&linkId=615e87a9105582e292ad2b7e2c7ea339&language=en_US)\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-robin",
   "metadata": {},
   "source": [
    "<mark>**Question:** Why is it important to calculate and visualise the metrics for train and test?</mark>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "amateur-laundry",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7d9ce6",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "    \n",
    "   It allows you to check for overfitting and whether your model is able to generalise to new data.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-confidence",
   "metadata": {},
   "source": [
    "---\n",
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "<a id=conc></a>\n",
    "\n",
    "# Conclusion and next steps\n",
    "\n",
    "This notebook has covered building a baseline classification problem using the sklearn estimator DecisionTreeClassifier.\n",
    "\n",
    "Next steps to improve the model:\n",
    "- Build an ensemble algorithm\n",
    "- Conduct feature selection\n",
    "- Generate useful features\n",
    "\n",
    "Next steps to align with best practice:\n",
    "- Hypertune model parameters\n",
    "- Build custom transformers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
