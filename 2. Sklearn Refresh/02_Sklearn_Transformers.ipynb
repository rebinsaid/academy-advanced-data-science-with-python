{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "quantitative-array",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "# Transformers\n",
    "\n",
    "In the previous notebook, you developed a baseline model. However, we made you ignore any feature that contained text and any row that contained missing data.\n",
    "\n",
    "In this notebook, you shall use sklearn Transformers to perform data preprocessing and investigate how it can aid model performance.\n",
    "\n",
    "- [Baseline model](#baseline)\n",
    "- [Sklearn Objects](#sklearn)\n",
    "- [Sklearn Transformers]()\n",
    "    - [Treating categorical columns](#cat)\n",
    "    - [Using `ColumnTransformer` for a subset of columns](#ct)\n",
    "    - [Using `SimpleImputer` to treat missing values](#impute)\n",
    "- [The sklearn `Pipeline`](#pipeline)\n",
    "- [Conclusion and next steps](#conc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-request",
   "metadata": {},
   "source": [
    "<a id=baseline></a>\n",
    "\n",
    "## Recreating the Baseline Model\n",
    "\n",
    "Let's make sure to have access to the baseline model during this notebook so you can compare performance.\n",
    "\n",
    "Import the relevant packages and modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-clinic",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "super-congress",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke = pd.read_csv('data/stroke.csv').rename(columns=str.lower)\n",
    "stroke.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "associate-model",
   "metadata": {},
   "source": [
    "Create X and y:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "featured-checklist",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = ['work_type', 'smoking_status', 'who', 'gender', 'residence_type']\n",
    "missing_cols = ['bmi', 'age']\n",
    "drop_cols = ['id','address']\n",
    "\n",
    "target = 'stroke'\n",
    "\n",
    "# Function to split data into X and y\n",
    "def create_Xy(df, drop_cols, target_col):\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    return (\n",
    "        df.drop(columns=target_col),\n",
    "        df[target_col]\n",
    "    )\n",
    "\n",
    "X_baseline, y = stroke.pipe(create_Xy, \n",
    "                        drop_cols=drop_cols\n",
    "                        + categorical_cols\n",
    "                        + missing_cols, \n",
    "                        target_col='stroke')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funded-continent",
   "metadata": {},
   "source": [
    "Split the data in training and test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dying-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_params = {\n",
    "    'test_size':0.25, \n",
    "    'random_state':42, \n",
    "    'stratify':y\n",
    "}\n",
    "\n",
    "X_baseline_train, X_baseline_test, y_train, y_test = train_test_split(\n",
    "                                                                      X_baseline, \n",
    "                                                                      y, \n",
    "                                                                      **train_test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "written-hanging",
   "metadata": {},
   "source": [
    "Recreate and train the base model (Decision Tree Classifier):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Import model\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Step 2: Instantiate model and set parameters\n",
    "base_model = DecisionTreeClassifier(max_depth=3, \n",
    "                                    class_weight='balanced',\n",
    "                                    random_state=42)\n",
    "\n",
    "# Step 3: Train model\n",
    "base_model.fit(X_baseline_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "meaning-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Evaluate model\n",
    "y_baseline_train_probs = base_model.predict_proba(X_baseline_train)[:,1]\n",
    "y_baseline_test_probs = base_model.predict_proba(X_baseline_test)[:,1]\n",
    "\n",
    "print(f'AUC: {roc_auc_score(y_train, y_baseline_train_probs), roc_auc_score(y_test, y_baseline_test_probs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74524504",
   "metadata": {},
   "source": [
    "In this notebook, you will try to improve on the ROC-AUC scores achieved with your simple models through the use of sklearn **Transformers**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-cowboy",
   "metadata": {},
   "source": [
    "<a id=sklearn></a>\n",
    "\n",
    "## Sklearn Objects: Estimators, Predictors, Models and Transformers\n",
    "\n",
    "Sklearn is built up of different types of [Objects](https://scikit-learn.org/stable/developers/develop.html). \n",
    "\n",
    "- An **Estimator** which implements a fit method to learn from data. \n",
    "- A **Predictor** makes predictions using the `.predict()` method.\n",
    "- A **Model** can give a goodness of fit measure or a likelihood of unseen data using a `.score()` method.\n",
    "- A **Transformer** can be used for filtering or modifying the data with the `.transform()` and `.fit_transform()` methods.\n",
    "\n",
    "The Decision Tree algorithm is an example of an **Estimator** since it will use the `.fit()` method to apply the decision tree algorithm to some given data. Once fitted it becomes both a **Predictor** and a **Model** since, once fitted, it can make predictions and supply a measure of goodness of fit.\n",
    "\n",
    "The great thing about sklearn is that all model algorithms follow this pattern. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afb24b6-387d-4057-9c4a-0ffb48345068",
   "metadata": {},
   "source": [
    "### Transformers\n",
    "\n",
    "You will now use **Transformers** to *transform* the data (who would have guessed). \n",
    "\n",
    "Transformers help you:\n",
    "- deal with missing values (e.g., by imputation)\n",
    "- deal with categorical and string features.\n",
    "- create new features from existing ones (e.g., by adding polynomial features and interactions)\n",
    "- and much more...\n",
    "\n",
    "\n",
    "Let's recreate X and y without dropping the categorical features or features with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chief-franklin",
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_cols = ['id','address']\n",
    "\n",
    "X, y = stroke.pipe(create_Xy, \n",
    "                   drop_cols=drop_cols, \n",
    "                   target_col='stroke')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, **train_test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "phantom-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "desperate-stroke",
   "metadata": {},
   "source": [
    "<a id=cat></a>\n",
    "\n",
    "### Treating categorical data\n",
    "\n",
    "The data now contains categorical and string features.\n",
    "\n",
    "We will have to use **One-hot Encoding** or **Ordinal Encoding** to transform the categories into numerical features.\n",
    "\n",
    "####  <mark>**Exercise**</mark> \n",
    "\n",
    "1. Which features do you consider categorical?\n",
    "2. Sklearn implements the [`OneHotEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) and [`OrdinalEncoder`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OrdinalEncoder.html) to deal with categorical data. Find out what they do.\n",
    "3. For which ones would you use one-hot encoding and for which ones ordinal encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-spouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find columns of DType \"object\"\n",
    "stroke.select_dtypes('O').columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb76cdb-511d-4b68-9201-9f1b7021df33",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke['who'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-zoning",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "    \n",
    "  You can use `stroke.select_dtypes('O').nunique()` to see how many unique categories each feature contains.\n",
    "  \n",
    "  None of the columns have an ordinal relationship (`smoking_status` may be a candidate but where would `unknown` fit?).\n",
    "  We should therfore use `OneHotEncoder`.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coastal-biodiversity",
   "metadata": {},
   "source": [
    "<a id=ct></a>\n",
    "\n",
    "### Using `ColumnTransformer` to select columns\n",
    "\n",
    "Of course, we only want to transform the categorical columns and leave the numerical columns as they are.\n",
    "\n",
    "What happens when you use `OneHotEncoder` on the entire dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "onehot = OneHotEncoder()\n",
    "onehot.fit_transform(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "packed-edwards",
   "metadata": {},
   "source": [
    "Let's look at the names of the outputted columns. \n",
    "\n",
    "*Note: On sklearn 1.0+ the method is `get_feature_names_out()`. For earlier versions use `get_feature_names()`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sonic-japan",
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot.get_feature_names_out()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-windsor",
   "metadata": {},
   "source": [
    "<mark>**Question:**</mark> What happened?\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "    \n",
    "  You have created a new column for each new unique value of each column, regardless of whether the data was categorical or not. Instead you want to apply the `OneHotEncoder` and `OrdinalEncoder` only on the categorical columns.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    " To select only a subset of features, you can use the `ColumnTransformer` object with the following syntax:\n",
    "```python\n",
    "onehot = ColumnTransformer([\n",
    "    ('name_of_step_1', Transformer_1, list_of_cols_1),\n",
    "    ('name_of_step_2', Transformer_2, list_of_cols_2),\n",
    "    ...\n",
    "    ('name_of_step_n', Transformer_n, list_of_cols_n),\n",
    "], remainder='passthrough')\n",
    "```\n",
    "\n",
    "You can implement a `ColumnTransformer` to only select the categorical features like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(), categorical_cols)],\n",
    "    remainder='passthrough')\n",
    "ct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a1f908",
   "metadata": {},
   "source": [
    "Running the `.fit_transform()` and `.tranform()` methods, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d5e0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_encoded = ct.fit_transform(X_train)\n",
    "X_test_encoded = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boring-placement",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, X_train_encoded.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prescription-duplicate",
   "metadata": {},
   "source": [
    "<mark>**Question**:</mark> There were 11 columns and now there are 22. Does that make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52064155",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f461eec",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "\n",
    "Using `ct.get_feature_names_out()`, you can see that it created a column for each category: `onehot__gender_Male`, `onehot__gender_Female`, `onehot__work_type_Govt_job`, and so on... There were 17 categories and 5 remaining numerical features that were not one-hot encoded.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fixed-sheriff",
   "metadata": {},
   "source": [
    "### <mark>Exercise:</mark> Drop-first One-Hot Encoding\n",
    "\n",
    "It is not necessary to add a column for each categorical value.\n",
    "\n",
    "There are two options to add to the `drop=` parameter in the `OneHotEncoder`:\n",
    "\n",
    "- `'first'`\n",
    "- `'if_binary'`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba9a3fd",
   "metadata": {},
   "source": [
    "**Question 1:** What do each of the parameters do? Why would you drop a column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-session",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71cfd139",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show solution</span></summary>\n",
    "    \n",
    "`drop='first'` will remove one column for each feature\n",
    "\n",
    "`drop='if_binary'` will only remove one column for the binary features\n",
    "\n",
    "**Why drop a column in the first place?** Because you actually need one less column than categories to fully encode all the information. E.g., if `gender__Male = 1` you know that `gender__Female` must be zero. So you can drop one column and still have all the information. ML practice follows the *principle of parsimony*. If a simpler model (e.g., fewer features) works as well as a more complex model (e.g., more features), you will prefer the simpler model.\n",
    "\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f745ee26",
   "metadata": {},
   "source": [
    "**Question 2:** Rebuild the `ColumnTransformer` with the correct parameter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab9fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ct = ColumnTransformer([\n",
    "    # your code here\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae423c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/02-ohe.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parental-wales",
   "metadata": {},
   "source": [
    "<a id='impute'></a>\n",
    "\n",
    "### Imputing missing values\n",
    "\n",
    "Recall that there were two numeric features with missing values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_cols = list(stroke.columns[stroke.isnull().any()])\n",
    "missing_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3451563",
   "metadata": {},
   "source": [
    "You can use the [SimpleImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html) from sklearn to fill in these missing values.\n",
    "\n",
    "It will fill the missing values with some constant, e.g., the *median* value of that feature column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worse-conditioning",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "impute = SimpleImputer(strategy='median')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd10801",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imputed = impute.fit_transform(X_train_encoded)\n",
    "X_test_imputed = impute.transform(X_test_encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a880354",
   "metadata": {},
   "source": [
    "Applying it directly to the (encoded) data means the same imputing strategy will be used for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca4dd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['age'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b541cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train['bmi'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "formal-spectrum",
   "metadata": {},
   "source": [
    "However, if you employed the `ColumnTransformer` you could apply different strategies to the columns.\n",
    "\n",
    "*Note: If the strategy is mean or median, this transformer will only work when all columns are numeric (so you need to impute after the one-hot transformer has been implemented).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transparent-voice",
   "metadata": {},
   "source": [
    "<a id=pipeline></a>\n",
    "\n",
    "## Sklearn Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparative-framing",
   "metadata": {},
   "source": [
    "To do this all in one go, you can use the sklearn `Pipeline` object.\n",
    "\n",
    "\n",
    "<img src=\"images/sklearn-pipe.png\" style=\"display: block;margin-left: auto;margin-right: auto;width: 400px\" align='right'/>\n",
    "\n",
    "**Pipelines** can encapsulate all the preprocessing steps (feature selections, scaling, encoding of variables and so on), as well as the final model, into a single Scikit-Learn estimator, thereby simplifying and automating many steps.\n",
    "\n",
    "Pipelines are defined as a **list of steps**, with each step being a `(name, object)` **tuple**:\n",
    "\n",
    "```Python\n",
    "pipe = Pipeline(steps=[\n",
    "    ('name_of_step_1', Transformer/Estimator/Model/Pipeline_1),\n",
    "    ('name_of_step_2', Transformer/Estimator/Model/Pipeline_2),\n",
    "    ...\n",
    "    ('name_of_step_n', Transformer/Estimator/Model/Pipeline_n),\n",
    "])\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "armed-defensive",
   "metadata": {},
   "outputs": [],
   "source": [
    "ct = ColumnTransformer([\n",
    "    ('onehot', OneHotEncoder(drop=\"if_binary\", sparse_output=False), categorical_cols),\n",
    "    ], \n",
    "    remainder='passthrough', \n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc889b1",
   "metadata": {},
   "source": [
    "#### <mark>Exercise:</mark> Build a `Pipeline`\n",
    "\n",
    "Build a pipeline called `preprocessing`.\n",
    "1. In the first step, you should add the `ct` Columntransformer.\n",
    "2. In the second step, add the `SimpleImputer` with `strategy='mean'`.\n",
    "3. Check if the data was correctly transformed by using the `.fit_transform(X_train)` method on your preprocessing pipeline. Does the output make sense?\n",
    "4. Create a new pipeline that adds the Decision Tree Classifier after the preprocessing steps.\n",
    "\n",
    "*Note: You can output a pandas dataframe by using `preprocessing.set_output(transform='pandas')` before calling the transform method.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-performer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd5e7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/02-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specified-strength",
   "metadata": {},
   "source": [
    "You can access parts of the pipeline by indexing on their names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sweet-salon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline['preprocessing']['ct']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-compiler",
   "metadata": {},
   "source": [
    "This way, you can still access the estimators/transformers/models and their parameters. \n",
    "\n",
    "E.g. the feature names after one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-health",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features = pipeline['preprocessing']['ct'].get_feature_names_out()\n",
    "all_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acting-costa",
   "metadata": {},
   "source": [
    "### Model creation and evaluation\n",
    "\n",
    "Let's fit the new pipeline to the original `X_train` and `y_train` data and compare it to the baseline you achieved earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-afternoon",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae21b4",
   "metadata": {},
   "source": [
    "Now, let's look at the ROC-AUC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increased-leather",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import RocCurveDisplay\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,6))\n",
    "\n",
    "RocCurveDisplay.from_estimator(base_model, X_baseline_train, y_train, ax=ax[0], name='Baseline')\n",
    "RocCurveDisplay.from_estimator(pipeline, X_train, y_train, ax=ax[0], name='Improved Model')\n",
    "ax[0].set(title='Train');\n",
    "\n",
    "RocCurveDisplay.from_estimator(base_model, X_baseline_test, y_test, ax=ax[1], name='Baseline')\n",
    "RocCurveDisplay.from_estimator(pipeline, X_test, y_test, ax=ax[1], name='Improved Model')\n",
    "ax[1].set(title='Test')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec61b89",
   "metadata": {},
   "source": [
    "A good improvement!\n",
    "\n",
    "Let's see what features the Decision Tree Classifier now used for its splitting rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-petite",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import plot_tree\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig,ax = plt.subplots(figsize=(20,20))\n",
    "\n",
    "plot_tree(pipeline.named_steps['model'], \n",
    "          feature_names=list(all_features),\n",
    "          ax=ax);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-hamilton",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<img src='images/gdd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "<a id=conc></a>\n",
    "\n",
    "# Conclusion and next steps\n",
    "\n",
    "This notebook covered the main objects in scikit-learn: Estimator, Predictor, Model, Transformer. Two transformers were used to preprocess the data to treat categorical features and features with missing values.\n",
    "\n",
    "Pipelines were also used as an elegant way to write your code.\n",
    "\n",
    "Next up you will work to improve this model even further by focussing on the model algorithm."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
