{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/gdd-logo.png' width='250px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "How good a machine learning model performs is partly dependent on how you choose to represent your data. Feature engineering is the **practice of creating new features** from your **existing** and **additional** data sources to improve model performance and/or model interpretability. It is often where the biggest improvements in model performance happenâ€”not through fancy algorithms, but by giving the model better representations of the data. \n",
    "\n",
    "**Program:**\n",
    "- [Introduction to Feature Engineering](#discussion)\n",
    "- [About the data](#data)\n",
    "- [Baseline model](#baseline)\n",
    "- [Feature Engineering](#engineering)\n",
    "    - [Indices and ratios](#ratios)\n",
    "    - [Engineering from texts](#texts)\n",
    "    - [Discretization](#binning)\n",
    "    - [Combining outside sources](#outside)\n",
    "- [Types of Feature Engineering](#types)\n",
    "- [Conclusion](#conclusion)\n",
    "- [Next Steps](#next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=data></a>\n",
    "\n",
    "## About the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# data processing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import (OneHotEncoder, KBinsDiscretizer)\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# pipelines\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# models\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='../images/feature_engineering/stroke_man.png' width='250px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "According to the World Health Organization (WHO), strokes are the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\n",
    "\n",
    "In this notebook, you will **predict whether a patient is likely to get a** `stroke`, based on input parameters like gender, age and whether or not they smoke. Each row in the data provides relavant information about the patient.\n",
    "\n",
    "### Features\n",
    "\n",
    "1. `id`: unique identifier\n",
    "1. `address`: A general address (town, state (abbreviation) & postal code)\n",
    "1. `gender`: \"Male\", \"Female\" or \"Other\"\n",
    "1. `age`: age of the patient\n",
    "1. `height`: height of the patient\n",
    "1. `weight`: height of the patient\n",
    "1. `hypertension`: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
    "1. `heart_disease`: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease\n",
    "1. `ever_married`: \"No\" or \"Yes\"\n",
    "1. `work_type`: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"\n",
    "1. `residence_type`: \"Rural\" or \"Urban\"\n",
    "1. `avg_glucose_level`: average glucose level in blood\n",
    "1. `smoking_status`: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"\n",
    "1. `stroke`: 1 if the patient had a stroke or 0 if not\n",
    "\n",
    "*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient.*\n",
    "\n",
    "Let's start by importing the data and looking at the dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke = pd.read_csv('../data/stroke.csv').rename(columns=str.lower)\n",
    "stroke.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Question:**</mark> What kind of features do you think would useful to generate from this data?\n",
    "\n",
    "<details>\n",
    "\n",
    "  <summary><span style=\"color:blue\">Show answers</span></summary>\n",
    "  \n",
    "Feature engineering often requires a good level of domain knowledge. The following areas could be looked into:\n",
    "  - **Indices**: From height and weight, we could create a bmi column that is a bit more interpretable.\n",
    "  - **Grouping continuous features**: There may be features - such as age - that do no show a linear relationship, however when grouping them and treating them as separate categorical groups (e.g., young, middle-aged, elderly) can capture the different risk factors associated with stroke in those different groups.\n",
    "  - **Location Features**: Extracting information from the address column, such as separating town and state, or deriving features like region or proximity to healthcare facilities, could be useful in understanding geographical influences on stroke occurrence.\n",
    "  - **Demographic Features**: Creating binary indicators for specific demographics, such as \"is_urban\" for residence_type or \"is_married\" for ever_married, can help the model capture demographic-specific patterns.\n",
    "  - **External features**: If the state for each patient is extracted, an external feature could be mapped to the state, for example *number of healthcare facilities in the state per person*.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='baseline'></a>\n",
    "\n",
    "## Baseline model\n",
    "\n",
    "Before you should do any feature engineering, you should again create a baseline model on the already existing features to see how the model is doing from the start.\n",
    "\n",
    "Below, you can see the usual setup already implemented:\n",
    "1. Define the features $X$ and target $y$,\n",
    "2. Split the data into training and test data,\n",
    "3. Define a `Pipeline` for onehot-encoding, imputing missing values, and the model, and\n",
    "4. Fit and evaluate a model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Define features X and target y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable definitions\n",
    "categorical_cols = ['work_type', 'smoking_status', 'who', 'gender', 'residence_type']\n",
    "numeric_cols = ['age', 'hypertension', 'heart_disease', 'ever_married', 'avg_glucose_level', 'height', 'weight']\n",
    "missing_cols = ['age','height', 'weight']\n",
    "drop_cols = ['id','address']\n",
    "\n",
    "target = 'stroke'\n",
    "\n",
    "def create_Xy(df, drop_cols, target_col):\n",
    "    df = df.drop(columns=drop_cols)\n",
    "    return (\n",
    "        df.drop(columns=target_col),\n",
    "        df[target_col]\n",
    "    )\n",
    "\n",
    "X, y = stroke.pipe(create_Xy, \n",
    "                   drop_cols=drop_cols, \n",
    "                   target_col=target,\n",
    "                   )\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 123,\n",
    "                                                    stratify = y,\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Set up pipeline, fit and score model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onehot = Pipeline(steps = [\n",
    "    ('onehot', OneHotEncoder(drop = \"if_binary\")),\n",
    "])\n",
    "\n",
    "impute = Pipeline(steps = [\n",
    "    ('impute', SimpleImputer(strategy ='mean')),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [\n",
    "    ('onehot', onehot, categorical_cols),\n",
    "    ('impute', impute, numeric_cols)\n",
    "], remainder = 'passthrough')\n",
    "\n",
    "base_model = RandomForestClassifier(class_weight='balanced',\n",
    "                                    max_depth=3,\n",
    "                                    random_state=123,\n",
    "                                    )\n",
    "\n",
    "base_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', base_model)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_baseline_train_probs = base_pipeline.predict_proba(X_train)[:,1]\n",
    "y_baseline_test_probs = base_pipeline.predict_proba(X_test)[:,1]\n",
    "\n",
    "print(f'Train AUC: {roc_auc_score(y_train, y_baseline_train_probs):.4f}',\n",
    "      f'Test AUC: {roc_auc_score(y_test, y_baseline_test_probs):.4f}',\n",
    "      sep='\\n'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "<a id='engineering'></a>\n",
    "\n",
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start engineering some features!\n",
    "\n",
    "<a id='ratios'></a>\n",
    "### Indices and Ratios\n",
    "\n",
    "***Indices or ratios*** are features that are calculated by combining multiple original features via division, multiplication, addition, or subtraction.\n",
    "\n",
    "For example, `bmi` is widely adopted as an index in the healthcare domain, which can be created as the combination of `height` and `weight` using this formula:\n",
    "\n",
    "\n",
    "$$ BMI = \\dfrac{weight}{height^2} $$\n",
    "\n",
    "In reality, indices often do not improve model performance. However, they increase overall model interpretability. For example, it is easier for us to only look at the effect of BMI on stroke rather than having to the interaction of height and weight ourselves.\n",
    "    \n",
    "</details>\n",
    "\n",
    "\n",
    "**Note:** There is no Transformer in sklearn that creates a BMI index, so you will do this in Pandas for now (ideally, you would want to build a custom transformer later on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <mark>Exercise:</mark> Create a BMI/age index\n",
    "\n",
    "What you should do:\n",
    "1. Create a new feature matrix and call it `X_bmi` (*Hint: We already provided the scaffolding for this below*)\n",
    "2. Using the Pandas `.assign()` method, calculate the BMI and call it `bmi`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bmi, _ = (\n",
    "    stroke\n",
    "    # create a bmi column here\n",
    "    .pipe(create_Xy,\n",
    "          drop_cols=drop_cols,\n",
    "          target_col=target)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answers**: Uncomment and run the cells below for answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/feature_engineering/create-bmi.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the `bmi` column created, you can now re-fit your baseline model to see how much this new feature improved performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_bmi, X_test_bmi, _, _ = train_test_split(X_bmi,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 123,\n",
    "                                                    stratify = y,\n",
    "                                                    )\n",
    "\n",
    "\n",
    "# add bmi to list of cols with NA's\n",
    "missing_cols = ['age','height', 'weight', 'bmi']\n",
    "\n",
    "# Refit model pipeline\n",
    "preprocessor_bmi = ColumnTransformer(transformers = [\n",
    "    ('onehot', onehot, categorical_cols),\n",
    "    ('impute', impute, missing_cols),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline_bmi = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_bmi),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "pipeline_bmi.fit(X_train_bmi, y_train)\n",
    "\n",
    "y_train_probs_bmi = pipeline_bmi.predict_proba(X_train_bmi)[:,1]\n",
    "y_test_probs_bmi = pipeline_bmi.predict_proba(X_test_bmi)[:,1]\n",
    "\n",
    "print(f'Train AUC: {round(roc_auc_score(y_train, y_train_probs_bmi),4)}',\n",
    "      f'Test AUC: {round(roc_auc_score(y_test, y_test_probs_bmi),4)}',\n",
    "      sep='\\n'\n",
    "      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='texts'></a>\n",
    "### Engineering features from texts or strings\n",
    "\n",
    "<img src='../images/feature_engineering/confused-robot.png' width='150px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "In real-life settings a lot of the information available to us is in form of unstructured or unprocessed data, such as texts, longer strings, or pictures. For traditional ML models to understand this data, you will have to engineer meaningful features yourself before feeding it into the model.\n",
    "\n",
    "<mark>**Question:**</mark> Which string-based feature is currently being ignored and what information would be useful to get from it?\n",
    "\n",
    "<details>\n",
    "\n",
    "  <summary><span style=\"color:blue\">Show answers</span></summary>\n",
    "\n",
    "The `address` column is currently being dropped, due to the fact that it is unique for each patient. However, it contains categories (such as states and zip codes) that may be useful. \n",
    "\n",
    "</details>\n",
    "\n",
    "First look at how many states and zip codes are in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke['address'].str.split().str[-1].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not a good use case in terms of grouping the patients since there are a vast number of zip codes. However for state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke['address'].str.split().str[-2].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only 17 unique states. Let's look at how many are in each group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke['address'].str.split().str[-2].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Question:**</mark> Will zip codes be a useful feature here? Why/Why not?\n",
    "\n",
    "<details>\n",
    "\n",
    "  <summary><span style=\"color:blue\">Show answers</span></summary>\n",
    "\n",
    "It will not be useful because there are almost as many zip codes as patients. This will make for very sparse and uninformative features when one-hot encoded.\n",
    "\n",
    "</details>\n",
    "\n",
    "The smallest states only have 51 patients. Considering how little actual stroke victims are in the dataset (~5%), you may have too few samples per state. Ideally, you may want to further group states into larger groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <mark>Exercise:</mark> Extract the state from the address\n",
    "\n",
    "What you should do:\n",
    "1. Create a new feature matrix and call it `X_address`\n",
    "2. Extract the *state* of each address and add a `state` column to the dataframe.<br>\n",
    "*Hint: To extract the states from the address, take a look at the code above where we calculated the number of unique states.* \n",
    "3. Why could the state be a useful feature?\n",
    "\n",
    "<details>\n",
    "  <summary><span style=\"color:blue\">Show answers</span></summary>\n",
    "\n",
    "3. There may be systematic differences between states in terms of life styles, obesity, genetics, demographics etc., which will make the state an indirect predictor of these differences.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_state, _ = (\n",
    "    stroke\n",
    "    .assign(bmi = lambda df: df['weight'] / df['height']**2)\n",
    "    # create a state column here\n",
    "    .pipe(create_Xy, \n",
    "          drop_cols=drop_cols, \n",
    "          target_col=target,\n",
    "          )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/feature_engineering/create-states.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now re-fit the model to see if the state data increases model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_state, X_test_state, _, _ = train_test_split(X_state,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 123,\n",
    "                                                    stratify = y,\n",
    "                                                    )\n",
    "\n",
    "# include state in list of categorical cols\n",
    "categorical_cols = ['work_type', 'smoking_status', 'who', 'gender', 'residence_type', 'state']\n",
    "\n",
    "# pipeline using both bmi and state\n",
    "preprocessor_state = ColumnTransformer(transformers = [\n",
    "    ('onehot', onehot, categorical_cols),\n",
    "    ('impute', impute, missing_cols),\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline_state = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_state),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "pipeline_state.fit(X_train_state, y_train)\n",
    "\n",
    "y_state_train_probs = pipeline_state.predict_proba(X_train_state)[:,1]\n",
    "y_state_test_probs = pipeline_state.predict_proba(X_test_state)[:,1]\n",
    "\n",
    "print(f'Train AUC: {round(roc_auc_score(y_train, y_state_train_probs),4)}',\n",
    "      f'Test AUC: {round(roc_auc_score(y_test, y_state_test_probs),4)}',\n",
    "      sep='\\n'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='binning'></a>\n",
    "## Discretization\n",
    "\n",
    "*Discretization* refers to the idea of categorizing continous data.\n",
    "\n",
    "For example, you may want to discretize the average glucose level into \"too low\", \"normal\", and \"too high\". \n",
    "\n",
    "Discretization helps simplifying variables, making the model more interpretable. In addition, discretization can help to reduce noise and normalize outliers in the data.\n",
    "\n",
    "In `sklearn`, you can use the [`KBinsDiscretizer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html) object, which can (you guessed it) be implemented into our existing `preprocessor` pipeline. \n",
    "\n",
    "For demonstration, let's discretize `age`, `bmi`, and `avg_glucose_level` into 6 quantiles and see how the model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binning_cols = ['age','bmi','avg_glucose_level']\n",
    "\n",
    "binner = Pipeline(steps = [\n",
    "    ('impute', SimpleImputer(strategy ='mean')),\n",
    "    ('bin', KBinsDiscretizer(n_bins = 6, strategy = 'quantile'))\n",
    "])\n",
    "\n",
    "preprocessor_bin = ColumnTransformer(transformers = [\n",
    "    ('onehot', onehot, categorical_cols),\n",
    "    ('impute', impute, missing_cols),\n",
    "    ('bin', binner, binning_cols)\n",
    "], remainder='passthrough')\n",
    "\n",
    "pipeline_bin = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_bin),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "pipeline_bin.fit(X_train_state, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>**Question:**</mark> Why does it not make sense to discretize `hypertension` or `heart_disease`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "  <summary><span style=\"color:blue\">Show answers</span></summary>\n",
    "\n",
    "1. Because these are binary (1 vs. 0) measures.\n",
    "    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_bin_train_probs = pipeline_bin.predict_proba(X_train_state)[:,1]\n",
    "y_bin_test_probs = pipeline_bin.predict_proba(X_test_state)[:,1]\n",
    "\n",
    "print(f'Train AUC: {round(roc_auc_score(y_train, y_bin_train_probs),4)}',\n",
    "      f'Test AUC: {round(roc_auc_score(y_test, y_bin_test_probs),4)}',\n",
    "      sep='\\n'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='outside'></a>\n",
    "## Combining outside sources\n",
    "Combining outside sources for feature engineering involves integrating external information with the original dataset to create new, more informative features.\n",
    "\n",
    "Incorporating outside information provides additional contextual information that may not be available in the original dataset, potentially improving model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <mark>Exercise:</mark> Using public stroke statistics to augment your data\n",
    "<img src='../images/feature_engineering/cdc-stroke-mortality.png' width='400px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "\n",
    "The ***US Center for Disease Control and Prevention (CDC)*** publishes [yearly stroke mortality rates](https://www.cdc.gov/nchs/pressroom/sosmap/stroke_mortality/stroke.htm) by each US state. You can use this information to link each state's mortality rate with the patients.\n",
    "\n",
    "1. Incorporate the `rate` data from `2021` into your dataset and call the new feature matrix `X_rate`.\n",
    "2. Why is the `rate` column more informative than the total `deaths` column?\n",
    "\n",
    "*Hint: You will need to [merge](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.merge.html) the `state_rate2021` dataframe with your X_state dataframe.*\n",
    "\n",
    "<details>\n",
    "\n",
    "  <summary><span style=\"color:blue\">Show answers</span></summary>\n",
    "\n",
    "2. Because the `rate` column already normalizes the population of each state. If we went with total `deaths`, Texas (much more populous) would seem much more deadly compared to e.g., New Hampshire.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_mortality = pd.read_csv('../data/cdc_mortality.csv').rename(columns=str.lower)\n",
    "\n",
    "state_mortality.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune CDC data to 2021 and only state and rate columns\n",
    "state_rate2021 = state_mortality.loc[lambda df: df['year'] == 2021, ['state','rate']]\n",
    "\n",
    "# New feature matrix\n",
    "X_rate, _ = (\n",
    "    stroke\n",
    "    .assign(bmi = lambda df: df['weight'] / df['height']**2)\n",
    "    .assign(state = lambda df: df['address'].str.split().str[-2])\n",
    "\n",
    "    ### your code here - merge the state_rate2021\n",
    "\n",
    "    \n",
    "    .pipe(create_Xy, \n",
    "          drop_cols=drop_cols, \n",
    "          target_col=target,\n",
    "          )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ../answers/feature_engineering/create-rate.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-fit and evaluate the model pipeline with the new feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train_rate, X_test_rate, _, _ = train_test_split(X_rate,\n",
    "                                                    y,\n",
    "                                                    test_size = 0.25,\n",
    "                                                    random_state = 123,\n",
    "                                                    stratify = y,\n",
    "                                                    )\n",
    "\n",
    "# Refit model pipeline\n",
    "preprocessor_state = ColumnTransformer(transformers = [\n",
    "    ('onehot', onehot, categorical_cols),\n",
    "    ('bin', binner, binning_cols)\n",
    "], remainder='passthrough', verbose_feature_names_out=False)\n",
    "\n",
    "pipeline_state = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor_state),\n",
    "    ('impute', impute),\n",
    "    ('model', base_model)\n",
    "])\n",
    "\n",
    "pipeline_state.fit(X_train_rate, y_train)\n",
    "\n",
    "y_rate_train_probs = pipeline_state.predict_proba(X_train_rate)[:,1]\n",
    "y_rate_test_probs = pipeline_state.predict_proba(X_test_rate)[:,1]\n",
    "\n",
    "print(f'Train AUC: {round(roc_auc_score(y_train, y_rate_train_probs),4)}',\n",
    "      f'Test AUC: {round(roc_auc_score(y_test, y_rate_test_probs),4)}',\n",
    "      sep='\\n'\n",
    "      )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of the engineered features\n",
    "\n",
    "Let's have a look at how many new features we have created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(base_pipeline[:-1].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pipeline_state[:-1].get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have a good amount of new features, in the next step you should prune the feature space to only include the most useful ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id='types'></a>\n",
    "## Other types of feature engineering\n",
    "\n",
    "Feature engineering is a way to highlight key information based on your own domain knowledge, which helps the model focus on the most important information. This is by far not an exhaustive list of the types of feature engineering that exist. Other examples include:\n",
    "* **Date and time features**: Creating features from the dates available, e.g. holidays, time of the day or day of the week.  \n",
    "* **Grouping sparse classes**: If you have a feature with an individual low sample count, you might group various values together under some other category. For example: We could group our `state` column (which will generate at 49 new columns through the onehot encoder) into *northern* vs. *southern*, and *east-coast* vs. *west-coast* vs. *mid-west/central* states.\n",
    "* **Group from threshold**: A new grouped variable for other variables, e.g. `obese`, and `normal`, `underweight` based on the `BMI`.\n",
    "* **Indicator from threshold**: An indicator variable (0 or 1) based on a threshold on a column, e.g. `retired` based on `age`. \n",
    "* **Statistical features**: For time-series or otherwise co-dependent data, it is useful to look at statistical features such as the variance or skewness of a data distribution. In financial applications, volatility (the variance) of a feature can be a good predictor for future performance (e.g., volatility in the last 30 days can indicate a trend shift in stock value).   \n",
    "\n",
    "### Other modalities\n",
    "We have so far only dealt with tabular data. In practice, you may want to also include other modalities such as text (e.g., patient history or doctor's notes) and images (e.g., MRI scans, X-Rays or CT scans)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='conclusion'></a>\n",
    "## Conclusion\n",
    "\n",
    "In this notebook, you have learned \n",
    "- The essentials of feature engineering and its impact on model accuracy and interpretability\n",
    "- You have implemented core feature engineering techniques such as indices/interactions, binning, text extraction, and combining outside information to generate new features.\n",
    "- You also have insight into other techniques, including those used with different types of data\n",
    "- You can explain that new features do not always increase model performance or interpretability, so it is good practice to add new features while assessing both of these things."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
