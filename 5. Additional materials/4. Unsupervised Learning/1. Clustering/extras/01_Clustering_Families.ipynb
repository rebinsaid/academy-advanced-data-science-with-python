{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../images/gdd-logo.png width=300px align=right>\n",
    "\n",
    "# Unsupervised Learning\n",
    "\n",
    "**Supervised learning** algorithms are machine learning algorithms that learn with the help of external feedback: the algorithm makes a prediction, compares its prediction to a provided ground truth, and \"learns\" by adjusting its internal parameters. \n",
    "\n",
    "In contrast, the techniques described in this lecture do not rely on some external notion of what is or is not correct; this class of learning techniques is referred to as **unsupervised learning**. \n",
    "\n",
    "For unsupervised learning, we can roughly differentiate between two categories: \n",
    "   - Clustering\n",
    "   - Dimensionality reduction\n",
    "\n",
    "In this notebook, we will take a look at the different families of clustering algorithms: partional, density-based and hierarchical clustering.\n",
    "\n",
    "# Clustering families\n",
    "\n",
    "- [Partitional clustering]()\n",
    "- [Density-based clustering]()\n",
    "- [Hierarchical clustering]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitional clustering\n",
    "\n",
    "### K-means\n",
    "K-means is a widely used clustering algorithm that assigns each point in the dataset to a cluster. \n",
    "\n",
    "1. A number (_k_) of _centroids_ are initialised. These are the centers of our clusters. Usually, these centroids are data points in the data set. \n",
    "2. Each point in the dataset gets assigned to one out of _k_ clusters based on the minimal Euclidean distance between the data point and each centroid. \n",
    "3. The centroid of each cluster is recalculated to the average of the points in that cluster. \n",
    "4. Repeat 2-3 until points no longer get reassigned.\n",
    "\n",
    "\n",
    "<img src=\"../images/kmeans.gif\" alt=\"K-Means Illustration\" height=600 width=600>\n",
    "\n",
    "The downside of k-means is that it requires you to define in advance how many clusters there are expected to be in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blobs, blobs_labels = make_blobs(n_samples=300, \n",
    "                                 centers=4,\n",
    "                                 cluster_std=0.60, \n",
    "                                 random_state=0)\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(blobs)\n",
    "\n",
    "blobs_kmeans = kmeans.predict(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(blobs[:, 0], blobs[:, 1], \n",
    "            c=blobs_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Models\n",
    "\n",
    "K-means is a partional clustering technique that uses a distance-based model. However, a major drawback of k-means is that these cluster models must be circular: k-means has no way of accounting for differently shaped clusters, such as oblong or elliptical. \n",
    "\n",
    "Let's examine some data like that: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/clustering_gmm.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array([data['Weight'], data['Height']]).transpose()\n",
    "data.shape\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(data[:, 0], data[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are 4 clearly visible clusters, k-means is unable to correctly identify these. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "kmeans.fit(data)\n",
    "\n",
    "data_kmeans = kmeans.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1], \n",
    "            c=data_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where **Gaussian Mixture Models** come in. Instead of a distanced-based model, GMM uses a distribution-based model. Gaussian Mixture Models are probabilistic models and use a soft clustering approach for distributing the points in different clusters. \n",
    "\n",
    "In the simplest case, this means GMMs can be used for finding clusters in the same manner as k-means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=4, init_params='kmeans')\n",
    "gmm.fit(blobs)\n",
    "\n",
    "blobs_gmm = gmm.predict(blobs)\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1],\n",
    "            c=blobs_gmm, s=50, cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, as GMM contains a probabilistic model under the hood, it is also possible to find probabilistic cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilities = gmm.predict_proba(blobs)\n",
    "probabilities[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualise this uncertainty by making the size of each point proportional to the certainty of its prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 50 * probabilities.max(1) ** 2  # square emphasizes differences\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], c=blobs_gmm, cmap='viridis', s=size);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means can be seen as a version of Gaussian Mixture Models that hard assigns the clusters. However, GMMs are - as they are distribution based - more applicable to non-circular data shapes. \n",
    "\n",
    "The algorithm for Gaussian Mixture Models is as follows: \n",
    "1. A number (k) of centroids are initialised. These are the centers of our clusters. \n",
    "2. For each point in the dataset, find the probability of membership in each cluster. \n",
    "3. For each cluster, update its location, normalisation, and shape based on all the data points weighted by their probability of membership to the cluster. \n",
    "4. Repeat 3-4 until points no longer get reassigned. \n",
    "\n",
    "![](../images/gmm.gif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm = GaussianMixture(n_components=4)\n",
    "gmm.fit(data)\n",
    "\n",
    "data_gmm = gmm.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data[:, 0], data[:, 1],\n",
    "            c=data_gmm, s=50, cmap='viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(KMeans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Density-based clustering\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means only performs well when the what we want to take into account is the distance between the points and the centroid, _not_ when we're interested in the distances between the data points themselves. This is where density-based methods come in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "\n",
    "moons, moons_labels = make_moons(200, noise=.05, random_state=0)\n",
    "plt.scatter(moons[:, 0], moons[:, 1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2)\n",
    "kmeans.fit(moons)\n",
    "\n",
    "moons_kmeans = kmeans.predict(moons)\n",
    "\n",
    "plt.scatter(moons[:, 0], moons[:, 1], \n",
    "            c=moons_kmeans,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DBSCAN\n",
    "\n",
    "DBSCAN stands for *Density-Based Spatial Clustering of Applications with Noise*. Essentially, it looks for areas of high density and assigns clusters to them, whereas points in less dense regions are not even included in the clusters and labeled as anomalies. It has two key settings:\n",
    "\n",
    "- *eps*: maximum distance between two points to consider them as neighbors. If this distance is too large we might end up with all the points in one huge cluster, however, if it is too small we might not even form a single cluster.\n",
    "- *min_points*: minimum number of points to form a cluster. If we set a low value for this parameters we might end up with a lot of really small clusters. However, a large value can stop the algorithm from creating any clusters at all.\n",
    "\n",
    "\n",
    "<img src=\"../images/DBSCAN_search.gif\" alt=\"DBSCAN Illustration\" height=600 width=600>\n",
    "\n",
    "DBSCAN looks at how many neighbors (closer than *eps*) each point has, considering neighbors all the points closer than a certain distance (*eps*). If more than *min_points* are neighbors, then a cluster is created, and this cluster is expanded with all the neighbors of the neighbors. Intuitively, it is important to have the input data scaled.\n",
    "\n",
    "Again, scikit-learn has a neat implementation of this algorithm, which we can implement with relatively little effort: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "dbscan = DBSCAN(eps=.2, min_samples=5)\n",
    "moons_dbscan = dbscan.fit_predict(moons)\n",
    "\n",
    "plt.scatter(moons[:, 0], moons[:, 1], \n",
    "            c=moons_dbscan,\n",
    "            s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.5, min_samples=10)\n",
    "\n",
    "blobs_dbscan = dbscan.fit_predict(blobs)\n",
    "\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], c=blobs_dbscan, s=50, cmap='viridis');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs_dbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DBSCAN in this case can function as some form of outlier detection - after all, the points not assigned to one of the clusters. \n",
    "\n",
    " A downside of DBSCAN, however, is that it does not have a `.predict` method, even though it does have a `.fit_predict()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan.predict(moons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Density-based Hierarchical Clustering \n",
    "\n",
    "Our data is, unfortunately, not always as neat as the blobs or moons dataset. Imagine the following dataset, a combination of moons, blobs and random data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data.\n",
    "moons, _ = make_moons(n_samples=50, noise=0.05)\n",
    "blobs, _ = make_blobs(n_samples=50, centers=[(-0.75,2.25), (1.0, 2.0)], cluster_std=0.25)\n",
    "random = np.random.uniform(-2, 2, (20, 2))\n",
    "data = np.vstack([moons, blobs, random])\n",
    "\n",
    "# Plot data. \n",
    "plot_kwds = {'alpha' : 0.5, 's' : 20, 'linewidths':0}\n",
    "plt.scatter(data[:, 0], data[:, 1], color='b', **plot_kwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HBSCAN\n",
    "\n",
    "HDBSCAN, the abbreviation for Hierarchical Density-Based Spatial Clustering of Applications with Noise, is an extension of DBSCAN and and appropriate choice for this type of data. DBSCAN is performed over varying epsilon values and the result is integrated, which allows HBSCAN to find clusters of varying densities - unlike DBSCAN. \n",
    "\n",
    "HDBSCAN is currently not available in scikit-learn, so we need to import it from a separate package `hdbscan`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan = HDBSCAN(min_cluster_size=5, gen_min_span_tree=True)\n",
    "hdbscan.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "hdbscan.minimum_spanning_tree_.plot(edge_cmap='viridis', \n",
    "                                      edge_alpha=0.6, node_size=80, edge_linewidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This minimum spanning tree show the mutual reachability distance. \n",
    "\n",
    "$$d_{\\text { mreach }-k}(a, b)=\\max \\left\\{\\operatorname{core}_{k}(a), \\operatorname{core}_{k}(b), d(a, b)\\right\\}$$\n",
    "\n",
    "This distance is a bit fancy. The $d(a,b)$ is just normal distance but $\\operatorname{core}_{k}(a)$ is the distance the point would need to be eligable to be in a cluster. This means that outlier points immediately get a high distance. The algorithm therefore has a dendogram tree but the weights of the arcs are built by using information about the density of the points. This is what the dendogram would look like;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "hdbscan.single_linkage_tree_.plot(cmap='viridis', colorbar=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now here's a fun game to play. Given this tree we can use a notion of a **minimum cluster size** to select clusters. If at a split we see that the resulting cluster does not meet the requirement we will throw away the new cluster and let the parent keep it. If we do that then we are able to draw a new tree with new properties; \n",
    "\n",
    "- the thickness of the line denotes the number of points\n",
    "- the length of the line denotes the $\\lambda = 1/\\text{distance}$ value of the points \n",
    "- each split is a split that is allowed by the minimum cluster rule \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "hdbscan.condensed_tree_.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm now selects the clusters by optimising for the area of ink. Note that when a parent is selected to be a cluster that a child cannot be selected to be a cluster. \n",
    "\n",
    "This selection is done here; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "hdbscan.condensed_tree_.plot(select_clusters=True, selection_palette=sns.color_palette());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Any point not in a selected cluster is considered to be a noisy point. We can see the result of this below (note corresponding colors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "palette = sns.color_palette()\n",
    "\n",
    "cluster_colors = [sns.desaturate(palette[col], sat) \n",
    "                  if col >= 0 else (0.5, 0.5, 0.5) for col, sat in \n",
    "                  zip(hdbscan.labels_, hdbscan.probabilities_)]\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=cluster_colors, **plot_kwds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A main downside of this clustering approach is that the algorithm does not offer `.predict()` method, much like DBSCAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "HDBSCAN is a hierarchical, density-based clustering algorithm. There are, however, also pure hierarchical clustering algorithms. These algorithms aim to build a hierarchy of clusters and typically follow one of two approaches: \n",
    "- Agglomerative: a bottom-up approach where each observation starts in its own cluster and pairs of clusters are merged as one moves up the hierarchy\n",
    "- Divisive: a top-down approach where all observations start in one cluster and splits are performed recursively as one moves down the hierarchy. \n",
    "\n",
    "![Example of agglomerative hierarchical clustering](../images/hierarchical.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show hierarchical clustering in practice, we're going to take a look at clients at a wholesale distributor based on their annual spending on diverse product categories, like milk, frozen, fresh, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/customers.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "data_scaled = normalize(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dendrogram\n",
    "\n",
    "A dendrogram can be used to visualise the history of groupings and figure out the optimal number of clusters. \n",
    "\n",
    "First we create our dendogram using `scipy`. \n",
    "\n",
    "The linkage criteria refers to how the distance between clusters is calculated. A couple of options are: \n",
    "- Single linkage: $L(r, s) = min(D(x_{ri}, x_{sj}))$. The distance between two clusters is the shortest distance between two points in each cluster. \n",
    "- Complete linkage: $L(r, s) = max(D(x_{ri}, x_{sj}))$ The distance between two clusters is the longest distance between two points in each cluster.\n",
    "- Average linkage: $L(r, s) = \\frac{1}{n_r n_s} \\sum^{n_r} \\sum^{n_s} D(x_{ri}, x_{sj})$ The distance between clusters is the average distance between each point in one cluster to every point in other cluster.\n",
    "- Ward linkage: The distance between clusters is the sum of squared differences within all clusters. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "plt.figure(figsize=(10, 7))  \n",
    "\n",
    "link = linkage(data_scaled, method='ward')\n",
    "dend = dendrogram(link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you have your dendrogram, the optimal number of cluster is found in the following way: \n",
    "1. Determine the largest vertical distance that does not intersect any of the other clusters. \n",
    "2. Draw a horizontal line through. \n",
    "3. The optimal number of clusters is equal to the number of vertical lines going through the horizontal line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 7))  \n",
    "\n",
    "link = linkage(data_scaled, method='ward')\n",
    "dend = dendrogram(link)\n",
    "\n",
    "plt.axhline(y=6, color='r', linestyle='--')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the largest vertical distance is between approximately 5 and 12. The horizontal line drawn crosses 2 vertical lines, so the optimal number of clusters for this dataset is 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=2, \n",
    "                                  affinity='euclidean', \n",
    "                                  linkage='ward')  \n",
    "cluster.fit_predict(data_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise our clusters on two variables: milk and grocery. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(data_scaled, columns=df.columns)\n",
    "plt.scatter(df_scaled['Milk'], \n",
    "            df_scaled['Grocery'], \n",
    "            c=cluster.labels_);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "- The three families of clustering algorithms are: partional, density-based and hierarchical. \n",
    "- Partitional algorithms include k-means and Gaussian Mixture Models, where k-means is hard assigned version of GMMs and GMMs are able to provide probabilities.\n",
    "- Density-based approaches include DBSCAN and HDBSCAN\n",
    "- Hierarchical clustering methods are either agglomerative or divisive. These may correspond to meaningful taxonomies and a dendrogram can help find the most meaningful number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
