{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/gdd-logo.png width=300px align=right>\n",
    "\n",
    "# Unsupervised Learning\n",
    "\n",
    "Until now we have considered machine learning algorithms that learn with the help of external feedback: the algorithm makes a prediction, compares its prediction with a provided ground truth, and \"learns\" by adjusting its internal parameters. This class of learning techniques is referred to as **supervised learning**.\n",
    "\n",
    "In contrast, the techniques described in this lecture do not rely on some external notion of what is or is not correct; this class of learning techniques is referred to as **unsupervised learning**.\n",
    "\n",
    "For unsupervised learning, we can roughly differentiate between two categories: \n",
    "   - Clustering\n",
    "   - Dimensionality reduction\n",
    "   \n",
    "In this notebook, we will take a look at techniques to determine whether your data can be clustered and a popular clustering algorithm called k-means.  \n",
    "\n",
    "# Clustering\n",
    "\n",
    "- [Introduction to clustering](#intro)\n",
    "- [K-Means clustering](#kmeans)\n",
    "- [Determining the number of clusters](#nr-clusters)\n",
    "- [Scaling](#scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "## Introduction to clustering\n",
    "\n",
    "Clustering is the technique of **grouping together** points that in some form or another 'belong' together. When you have a clustering problem, you are given an *unlabeled* data set and hace an algorithm automatically group the data into coherent subsets or into coherent clusters for you. \n",
    "\n",
    "<mark>**Question:** Can you think of any examples of unsupervised ML problems?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "  <summary><span style=\"color:blue\">Show examples</span></summary>\n",
    "  \n",
    "Examples include market segmentation, social network analysis, organising computer clusters/data centers better, or understanding galaxy formation. \n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The type of algorithm to choose for your clustering heavily depends on the type of data you have. Let's start out with creating a dataset with four distinct clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "blobs, blobs_labels = make_blobs(n_samples=300, \n",
    "                                 centers=4,\n",
    "                                 cluster_std=0.6, \n",
    "                                 random_state=0)\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see the four distinct clusters here, because we are able to visualise our data in 2D. However, if our data is more than two-dimensional, it can be difficult to visualise and therefore assess the clusterability.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='kmeans'></a>\n",
    "## K-Means clustering\n",
    "\n",
    "K-means is a widely used clustering algorithm that assigns each point in the dataset to a cluster. \n",
    "\n",
    "1. A number (_k_) of **centroids** are initialised. These are the centers of our clusters. Usually, these centroids are data points in the data set. \n",
    "2. Each point in the dataset gets assigned to one out of _k_ clusters based on the **minimal Euclidean distance** between the data point and each centroid. \n",
    "3. The centroid of each cluster is **recalculated** to the average of the points in that cluster. \n",
    "4. Repeat 2-3 until points no longer get reassigned.\n",
    "\n",
    "\n",
    "<img src=\"images/kmeans.gif\" alt=\"K-Means Illustration\" height=600 width=600>\n",
    "\n",
    "<mark>**Question:** Can you think of any potential downsides of Kmeans?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "  \n",
    "Onedownside of k-means is that it requires you to define **in advance** how many clusters there are expected to be in the data.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out in python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "help(KMeans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Experiment with different values of k!\n",
    "kmeans = KMeans(n_clusters=4, n_init=1)\n",
    "kmeans.fit(blobs)\n",
    "\n",
    "blobs_kmeans = kmeans.predict(blobs)\n",
    "blobs_kmeans[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = kmeans.cluster_centers_\n",
    "centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(blobs[:, 0], blobs[:, 1], \n",
    "            c=blobs_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means is also pretty sensitive to the initial initialisation. To ensure you have not become stuck in a local minimum, you can run K-Means multiple times and choose the centroid for which the **cost function** (sum of squared distances of samples to their closest cluster center) is the lowest. In sklearn, you can access this value with the `.inertia_` attribute of your estimator. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exercise</mark>\n",
    "\n",
    "Run the code below and experiment with different values for `k` (e.g. what happens when k=300) and the amount of initialisations (`n_init`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=4, n_init=1)\n",
    "kmeans.fit(blobs)\n",
    "\n",
    "blobs_kmeans = kmeans.predict(blobs)\n",
    "\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], \n",
    "            c=blobs_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);\n",
    "\n",
    "print(f\"The intertia score for this clustering attempting is {kmeans.inertia_}\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='nr-clusters'></a>\n",
    "## Determining the number of clusters\n",
    "Inertia will tend to zero as the number of centers increases to the amount of data points.\n",
    "\n",
    "You can use this property to infer the optimal $k$ value: you should choose a number of clusters so that adding another cluster would not give a much better inertia value.\n",
    "\n",
    "The **Elbow method** is one of the most popular methods to determine this optimal number of clusters for the data at hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs, blobs_labels = make_blobs(n_samples=300, \n",
    "                                 centers=4,\n",
    "                                 cluster_std=0.60, \n",
    "                                 random_state=0)\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the Elbow method, you plot the inertia for the number of clusters you consider. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1, 10)\n",
    "score = []\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(blobs)\n",
    "    score.append(kmeans.inertia_)\n",
    "    \n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, score, 'bx-')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the plot looks like an arm with a clear elbow at k = 4. The choice of number of clusters, based on this plot, would be **four**.\n",
    "\n",
    "Unfortunately, you do not always have such clearly clustered data. Let's create a bit more ambiguous data by altering the cluster standard deviation in the blobs example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs, blobs_labels = make_blobs(n_samples=300, \n",
    "                                 centers=4,\n",
    "                                 cluster_std=1.20, \n",
    "                                 random_state=0)\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's again plot the inertia for various numbers of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(1, 10)\n",
    "score = []\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(blobs)\n",
    "    score.append(kmeans.inertia_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, score, 'bx-')\n",
    "plt.title('The Elbow Method')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Inertia');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elbow is not as sharp. Although we know (as we created our data) the best number of clusters is four, based on this plot the choice is ambiguous. Five, for instance, could also be a decent choice. \n",
    "\n",
    "In such an ambiguous case, you can also use the **Silhouette method**. This metric measures how similar a point is to its own cluster compared to other clusters. The range of the Silhouette value is between -1 and +1 and the higher it is, the better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "help(silhouette_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = range(2, 10)\n",
    "score = []\n",
    "for k in K: \n",
    "    kmeans = KMeans(n_clusters=k, n_init=10)\n",
    "    kmeans.fit(blobs)\n",
    "    labels = kmeans.labels_\n",
    "    sil_score = silhouette_score(blobs, labels, metric='euclidean')\n",
    "    score.append(sil_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(K, score, 'bx-')\n",
    "plt.title('The Sihouette Score')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Silhouette Score');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Silhouette score reaches its global maximum at the optimal k. This means that the highest Silhouette score value corresponds with the best choice of number of clusters. In a plot, this appears as a peak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, val in zip(K, score):\n",
    "    print(f'No. clusters {k}: {val:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be noted that the Elbow method and the Silhouette score are not alternatives to each other for finding the optimal K. Rather, they are tools to be used together for a more confident decision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='scaling'></a>\n",
    "## Scaling\n",
    "\n",
    "**Distance-based algorithms** like k-means (but also k-nearest neighbors for supervised learning, or even support vector machines) are sensitive to the **scale** of the variables. \n",
    "\n",
    "Imagine data that has an age and an income variable. A typical range for age is 25-60, while the range of income can vary between \\\\$25,000 and \\\\$150,000. In measuring the distance to a cluster centroid, every variable is taken into account equally. A change of 25 in terms of years (25 years old or 50 years old) is treated similarly as a change of 25 in income (\\\\$25,000 or \\\\$25,025). This does not seem right and influences the clustering results. \n",
    "\n",
    "Let's recreate the blobs dataset, but make a small adjustment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs, blobs_labels = make_blobs(n_samples=300, \n",
    "                                 centers=4,\n",
    "                                 cluster_std=0.60, \n",
    "                                 random_state=0)\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blobs[:, 1] = blobs[:, 1] * 100\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], s=50);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values of the second feature (y-axis) have been multiplied by a factor of 100. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(blobs[:, 0]), np.max(blobs[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(blobs[:, 1]), np.max(blobs[:, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranges of the features are no longer similar. Let's see what this does to for the clustering algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=4, n_init=10)\n",
    "kmeans.fit(blobs)\n",
    "\n",
    "blobs_kmeans = kmeans.predict(blobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(blobs[:, 0], blobs[:, 1], \n",
    "            c=blobs_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers = kmeans.cluster_centers_\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering results have shifted! Let's fix this by scaling the data before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(blobs)\n",
    "blobs_transformed = scaler.transform(blobs) \n",
    "\n",
    "kmeans = KMeans(n_clusters=4, n_init=10)\n",
    "kmeans.fit(blobs_transformed)\n",
    "\n",
    "blobs_kmeans = kmeans.predict(blobs_transformed)\n",
    "\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], \n",
    "            c=blobs_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers_scaled = kmeans.cluster_centers_\n",
    "centers = scaler.inverse_transform(centers_scaled)\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we repeat the process but using Scikit-Learn Pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', KMeans(n_clusters=4, n_init=10))\n",
    "])\n",
    "\n",
    "pipe.fit(blobs)\n",
    "blobs_kmeans = pipe.predict(blobs)\n",
    "\n",
    "plt.scatter(blobs[:, 0], blobs[:, 1], \n",
    "            c=blobs_kmeans, s=50, cmap='viridis')\n",
    "\n",
    "centers_scaled = pipe['model'].cluster_centers_\n",
    "centers = pipe['scaler'].inverse_transform(centers_scaled)\n",
    "\n",
    "plt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "**Clustering** is the technique of grouping together points that in some form or another 'belong' together.\n",
    "\n",
    "A popular approach to clustering is using the **k-means algorithm**. Each point in the dataset is assigned to a cluster through an iterative method based on cluster centroids. The number of cluster centroids needs to be determined upfront. \n",
    "\n",
    "Two helpful methods to help determine the appropriate number of cluster centroids are: \n",
    "* **The Elbow Method**: the inertia is plotted against k. The point where the decrease in inertia slows down rapidly (the \"elbow\") is a good number of cluster points. \n",
    "* **Silhouette score**: a score between -1 and +1. The highest Silhouette score corresponds to the best number of clusters for this particular problem. \n",
    "\n",
    "It is important to be mindful of **scaling** when working with distance-based methods, like k-means clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
