{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualisation with PCA\n",
    "\n",
    "![](../images/countries.png)\n",
    "\n",
    "This assignment is a continuation of the k-means assignment notebook. The goal is to cluster the coutries using some socio-economic and health factors that determine the overall development of the country.\n",
    "\n",
    "In the previous assignment, you immediately applied the k-means algorithm to this problem. In this assignment, we will see how PCA can help us visualise our results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(pd.read_csv('../data/country-data.csv'))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['country'], axis=1)\n",
    "y = data['country']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data was preprocessed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to visualise our clustering results, we will first visualise the data. Without any dimensionality reduction techniques, we would have to choose which features to display on the x- and y-axis. However, with PCA, we can choose to simply reduce our data to two components. \n",
    "\n",
    "\n",
    "### Exercise  1\n",
    "Implement PCA on the data for visualisation purposes. Don't forget to scale your data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualisation purposes, we're going to use a package called _Altair_. This provides is with some nice functionality with the tooltip to allow us to explore the data a little better. In this piece of code, we assume the output of your PCA transformation on the data was called `pca_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import altair as alt\n",
    "\n",
    "# Combine PCA results with original dataframe\n",
    "pca_df = pd.DataFrame({'pc1': pca_data[:, 0], 'pc2': pca_data[:, 1]})\n",
    "df = data.merge(pca_df, left_index=True, right_index=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chart.\n",
    "alt.Chart(df).mark_circle().encode(\n",
    "    x = 'pc1', \n",
    "    y = 'pc2', \n",
    "    tooltip=data.columns.tolist(), \n",
    "    color='country'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This visualises your data on the principal components.\n",
    "\n",
    "What we are now going to do is recreate the clustering from the previous k-means assignment. This will give us labels for each of the countries. \n",
    "\n",
    "### Exercise 2\n",
    "Recreate your k-means model from the previous assignment here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's recreate the chart again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine PCA results with original dataframe\n",
    "pca_df = pd.DataFrame({'pc1': pca_data[:, 0], 'pc2': pca_data[:, 1], 'labels': best_model.labels_})\n",
    "df = data.merge(pca_df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we recreate the dataframe, which now not only contains the original data and principal components, but also the labels. \n",
    "\n",
    "Next up, we recreate the chart. The color is now no longer determined by the column name `country`, but by the column name `label`. The `:N` is added to encode that the data is nominal - a discrete, unordered category. You can remove it to see how that changes your visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chart.\n",
    "alt.Chart(df).mark_circle().encode(\n",
    "    x = 'pc1', \n",
    "    y = 'pc2', \n",
    "    tooltip=data.columns.tolist(), \n",
    "    color='labels:N'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Inspect the chart! Does the clustering look good to you? \n",
    "\n",
    "In the previous exercise, there were originally 5 clusters with very few data points. Can you find the data points that originally belonged to the small clusters in the chart? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering with PCA \n",
    "\n",
    "PCA is not only a helpful tool for visualising your clusters. \n",
    "\n",
    "A downside of the K-means algorithm is the \"curse of dimensionality\". In high-dimensionality spaces, Euclidean distances tend to become inflated. In such cases, running a dimensionality reduction algorithm such as principal component analysis prior to k-means clustering can alleviate this problem and speed up computations. \n",
    "\n",
    "![](../images/wine.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_wine\n",
    "\n",
    "wines = load_wine()\n",
    "wine_df = pd.DataFrame(wines.data, columns=wines.feature_names)\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our wine dataset has 13 distinct features - hardly what we would call 'high-dimensional', but good enough for our purposes. For clustering with k-means, this dataset might benefit from a reduction in dimensionality. \n",
    "\n",
    "### Exercise 4\n",
    "Scale your data and perform PCA to visualise the current dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Perform PCA with the number of components equal to the number of features to choose the right number of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Use the Elbow method to determine the right number of clusters for this dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercie 7\n",
    "Find the best model with K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "Plot the labels on the principal components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9\n",
    "Determine the following for the model you just created.\n",
    "- Inertia\n",
    "- Silhouette score\n",
    "- Calinkski-harabasz score [[documentation]](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.calinski_harabasz_score.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inertia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calinkski-harabasz score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus Exercise \n",
    "\n",
    "Create a k-means model without PCA. Compare the inertia, silhouette score and calinski-harabasz score. Are the results what you expect? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
