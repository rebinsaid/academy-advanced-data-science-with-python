{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=../images/gdd-logo.png width=300px align=right>\n",
    "\n",
    "# Dimensionality Reduction\n",
    "\n",
    "Another form of unsupervised learning, besides clustering, is what we call _dimensionality reduction_. The dimensionality of data can be reduced through feature selection (e.g. missing value ratio, low variance or high correlation filter) or based on a supervised learning method such as Lasso with `SelectFromModel` in sklearn. \n",
    "\n",
    "However, there are also _unsupervised_ approaches to dimensionality reduction. Dimensionality reduction, and therefore PCA, has two applications: \n",
    "* Data compression\n",
    "* Visualisation \n",
    "\n",
    "**Data compression** can be especially useful in the case that you have many, many features as it allows you to compress e.g. 1000 features into simply 100 features. This obviously cannot happen without some form of data loss, but the magic of dimensionality reduction is that it tries to limit the information loss. For example, say you have two features: height in cm and height in inches where in both cases the values have been rounded off. These are highly redundant as they are highly correlated and therefore quite easy to reduce to one single feature. \n",
    "\n",
    "**Visualisation** is also often a goal of dimensionality reduction. Visual inspection of your data can be hugely beneficial, but anything bigger than 3D data becomes problematic to visualise. In such case, we might reduce our dimensionality to two or three compontents in order to visualise our data.\n",
    "\n",
    "In this notebook, we will take a look into different dimensionality reduction techniques that allow us to visualize the data.\n",
    "\n",
    "# Dimensionality reduction for visualisation\n",
    "\n",
    "- [PCA]()\n",
    "- [t-SNE]()\n",
    "- [UMAP]()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import fetch_openml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "\n",
    "In order to demonstrate dimensionality reduction techniques, we require some high-dimensional data. In this case, images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y = fetch_openml(\"mnist_784\", version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us load in the MNIST dataset and examine their shapes. The data consists of images of 28x28 pixels, where each of the 784 pixels in total is a feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning approaches, especially dimensionality reduction for visualisation, do not require a test set. So we do not need to perform a train/test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X / 255.0\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some preprocessing: normalizing the X values to a 0 - 1 range. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 20))\n",
    "\n",
    "examples = 5\n",
    "for i in range(10): \n",
    "    indices = np.where(y == i)[0]\n",
    "    for j in range(examples):\n",
    "        img = X[indices[j]].reshape((28, 28))\n",
    "        plt.subplot(10, examples, (i * examples) + j + 1)\n",
    "        plt.imshow(img, cmap=plt.get_cmap('gray'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA for Visualization\n",
    "\n",
    "Principal Component Analysis can be used to reduce the number of dimensions. PCA is a **projection** based unsupervised learning technique, which means this technique deals with projecting every data point which is in high dimension, onto a subspace suitable lower-dimensional space in a way which approximately preserves the distances between the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "pca = PCA()\n",
    "pca_data = pca.fit_transform(X)\n",
    "\n",
    "time_pca = time.time() - time_start\n",
    "print(f'PCA done in {time_pca} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = {}\n",
    "times['PCA'] = time_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to maintain all or nearly all of the information contained in all the pixels, we would need approximately 200 components. However, that would be still be far too many dimensions to visualize. Let's see how much we can do with just two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Explained variance with 3 components: {np.sum(pca.explained_variance_ratio_[:3])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's not necessarily a lot - let's see what it looks like in 2D though. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({\n",
    "    'label': y,\n",
    "    'pc1': pca_data[:, 0],\n",
    "    'pc2': pca_data[:, 1],\n",
    "    'pc3': pca_data[:, 2],\n",
    "})\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_2d(df, x_axis, y_axis):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    sns.scatterplot(\n",
    "    x = df[x_axis],\n",
    "    y = df[y_axis],\n",
    "    hue = 'label',\n",
    "    data = df,\n",
    "    palette=sns.color_palette(\"hls\", 10),\n",
    "    legend=\"full\"\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df, 'pc1', 'pc2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot we can see that two components definitely hold some information, especially for specific digits, but not clearly enough to set all of them apart. \n",
    "\n",
    "Let's see if we can improve this with a 3D visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Explained variance with 3 components: {np.sum(pca.explained_variance_ratio_[:3])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We maintain quite a bit more information with three components than with two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(15, 20))\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(\n",
    "    xs = df['pc1'],\n",
    "    ys = df['pc2'],\n",
    "    zs = df['pc3'],\n",
    "    c = df['label'],\n",
    "    cmap = 'tab10'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('pc1')\n",
    "ax.set_ylabel('pc2')\n",
    "ax.set_zlabel('pc3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we can create a 3D plot with matplotlib, it is not interactive and does not really allow us to fully explore the data. Instead, we will use plotly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_3d(df, x_axis, y_axis, z_axis): \n",
    "    fig = px.scatter_3d(\n",
    "        df.sort_values('label'),\n",
    "        x = x_axis, \n",
    "        y = y_axis, \n",
    "        z = z_axis,\n",
    "        color = 'label',\n",
    "    )\n",
    "\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df, 'pc1', 'pc2', 'pc3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# t-SNE \n",
    "\n",
    "Whereas PCA is a **linear** dimensionality reduction technique, t-distributed Stochastic Neighbor Embedding (t-SNE) is a **nonlinear** dimensionality reduction technique. This means this algorithm, unlike PCA, allows us to separate data that cannot be separated by any straight line - making it more appropriate for certain datasets. \n",
    "\n",
    "\n",
    "The t-SNE algorithm models the probability distribution of neighbors around each point. Here, the term neighbors refers to the set of points which are closest to each point. In the original, high-dimensional space this is modeled as a Gaussian distribution. In the 2-dimensional output space this is modeled as a t-distribution. The goal of the procedure is to find a mapping onto the 2-dimensional space that minimizes the differences between these two distributions over all points. The fatter tails of a t-distribution compared to a Gaussian help to spread the points more evenly in the 2-dimensional space.\n",
    "\n",
    "t-SNE was introduced in 2008 by van der Maaten and Hinton [[link to paper]](https://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_data = tsne.fit_transform(X)\n",
    "\n",
    "time_tsne = time.time() - time_start\n",
    "print(f't-SNE done in {time.time() - time_start} seconds!')\n",
    "\n",
    "times['tsne'] = time_tsne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The computation takes some time. t-SNE is quite heavy on system resources, especially compared to PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .assign(tsne1_2d = tsne_data[:, 0])\n",
    "    .assign(tsne2_2d = tsne_data[:, 1])\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df, 'tsne1_2d', 'tsne2_2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like the digits are very clearly clustered in their own subgroups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df, 'pc1', 'pc2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D Visualization\n",
    "Just like with PCA, we can also use t-SNE for a 3D visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd = np.random.permutation(df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000\n",
    "data = X[rnd[:size]]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the computation would take rather a long time for many features and many samples, we will use only 10,000 samples to run the algorithm on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=3, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_data_3d = tsne.fit_transform(data)\n",
    "\n",
    "print(f't-SNE done in {time.time() - time_start} seconds!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the number of samples, even while increasing the number of components, leads to a faster result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_subset = (\n",
    "    df\n",
    "    .iloc[rnd[:size]]\n",
    "    .assign(tsne1_3d = tsne_data_3d[:, 0])\n",
    "    .assign(tsne2_3d = tsne_data_3d[:, 1])\n",
    "    .assign(tsne3_3d = tsne_data_3d[:, 2])\n",
    ")\n",
    "\n",
    "df_subset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_3d(df_subset, 'tsne1_3d', 'tsne2_3d', 'tsne3_3d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have seen, t-SNE can be rather slow - a lot slower than PCA - on the high dimensional data that we have. \n",
    "\n",
    "The scikit-learn documentation recommends the following:\n",
    "> It is highly recommended to use another dimensionality reduction method (e.g. PCA for dense data or TruncatedSVD for sparse data) to reduce the number of dimensions to a reasonable amount (e.g. 50) if the number of features is very high. This will suppress some noise and speed up the computation of pairwise distances between samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exercise</mark>\n",
    "\n",
    "The t-SNE algorithms has multiple parameters that can be set. One of the main parameters controlling the fitting is _perplexity_. The perplexity is related to the number of nearest neighbors considered when matching the original and fitted distributions for each point. Larger datasets usually require a larger value for perplexity, while smaller datasets suffice with focusing on the closest other points. A value between 5 and 50 is recommended, and different values can result in significantly different results. \n",
    "\n",
    "**Exercise:** Experiment with various values for _perplexity_ and observe how it influences your final result. For computational purposes, consider a dataset that is smaller than the original, e.g. 10.000 data samples. Feel free to set this value to whatever suits you, but keep in mind that a good value for perplexity is related to the size of the dataset. \n",
    "\n",
    "Also feel free to experiment with other parameters, such as _learning rate_ or _number of iterations_ (related to the optimisation process). See [scikit-learn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html) for more information.\n",
    "\n",
    "Keep in mind that t-SNE is not deterministic; it does not necessarily produce similar output on successive runs with the same parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(TSNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 10000\n",
    "data = X[rnd[:size]]\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_data = tsne.fit_transform(data)\n",
    "\n",
    "time_tsne = time.time() - time_start\n",
    "print(f't-SNE done in {time.time() - time_start} seconds!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_df = (\n",
    "    df\n",
    "    .assign(tsne1_2d = tsne_data[:, 0])\n",
    "    .assign(tsne2_2d = tsne_data[:, 1])\n",
    ")\n",
    "\n",
    "exercise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(exercise_df, 'tsne1_2d', 'tsne2_2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE with PCA\n",
    "\n",
    "So let's try t-SNE with PCA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Explained variance with 50 components: {np.sum(pca.explained_variance_ratio_[:50])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data_50 = pca_data[:, :50]\n",
    "pca_data_50.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensure we have the right data. The shapes look good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "\n",
    "tsne = TSNE(n_components=2, verbose=1, perplexity=40, n_iter=300)\n",
    "tsne_data = tsne.fit_transform(pca_data_50)\n",
    "\n",
    "time_tsne_pca = time.time() - time_start\n",
    "print(f't-SNE done in {time_tsne_pca} seconds!')\n",
    "\n",
    "times['tsne-PCA'] = time_tsne_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .assign(tsne1_pca = tsne_data[:, 0])\n",
    "    .assign(tsne2_pca = tsne_data[:, 1])\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the values between t-sne and t-sne with PCA differ. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df, 'tsne1_pca', 'tsne2_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_2d(df, 'tsne1_2d', 'tsne2_2d')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As t-SNE is an iterative approach, it cannot be applied on another dataset, unlike PCA. PCA computes a global covariance matrix and uses that to reduce the data, that matrix can be applied to a new set of data. Although the implementation in scikit-learn has a `.fit_transform()` method, it does not have a `.transform()` method to use for new data. \n",
    "\n",
    "Therefore, t-SNE is mostly used to visualise and understand high-dimensional data, while PCA is more often used for dimensionality reduction for clustering and/or supervised learning algorithms. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UMAP\n",
    "\n",
    "Uniform Manifold Approximation and Projection (UMAP) is a dimensionality reduction technique first published in 2018. [[link to paper]](https://arxiv.org/pdf/1802.03426.pdf) UMAP at its core is very similar to t-SNE, although it aims to alleviate some of the challenges with t-SNE. \n",
    "\n",
    "The mathematics behind UMAP is quite advanced and relies on the following three assumptions: \n",
    "1. The data is uniformly distributed on a Riemannian manifold;\n",
    "2. The Riemannian metric is locally constant (or can be approximated as such);\n",
    "3. The manifold is locally connected.\n",
    "\n",
    "UMAP essentially builds a representation of a weighted graph, where the edge weights represent the likelihood that two points are connected. This likelihood that two points are connected is determined in the following way: each point extends a radius outward, the size of which is determined by the point's distance to its _nth_ nearest neighbor. Points whose radii overlap are considered connected, and the likelihood of connection decreases as the radius grows. UMAP ensures that the local structure is perserved in balance with the global structure by stipulating that each point must be connected to at least its closest neighbor. \n",
    "\n",
    "This constructed graph is then projected to a lower dimensionality. The layout of the low-dimensional version of the graph is optimised in a process similar to that of t-SNE, but remarkably faster. \n",
    "\n",
    "A deeper dive into UMAP theory: [[link]](https://pair-code.github.io/understanding-umap/supplement.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "help(umap.UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "umap_transformer = umap.UMAP(n_components=3)\n",
    "\n",
    "umap_data = umap_transformer.fit_transform(X)\n",
    "\n",
    "time_umap = time.time() - time_start\n",
    "print(f'UMAP done in {time_umap} seconds!')\n",
    "times['umap'] = time_umap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although it seems to take longer than PCA, it UMAP is a lot quicker than t-SNE! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    df\n",
    "    .assign(umap1 = umap_data[:, 0])\n",
    "    .assign(umap2 = umap_data[:, 1])\n",
    "    .assign(umap3 = umap_data[:, 2])\n",
    ")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run UMAP for three components, and use the first two to create a 2D visualisation - just like with PCA. This removes the need to  rerun UMAP to create a 3D visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP 2D\n",
    "plot_2d(df, 'umap1', 'umap2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP 3D\n",
    "plot_3d(df, 'umap1', 'umap2', 'umap3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whereas t-SNE has `perplexity` as the most important parameter to set, UMAP also has a few tunable parameters that determine the end result. \n",
    "\n",
    "* `n_neighbors`: the number of approximate nearest neighbors to construct the high-dimensional graph\n",
    "* `min_dist`: the minimum distance between points in low-dimensional space. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exercise</mark>\n",
    "Experiment with various values for `n_neighbors` and `min_dist` and see how this influences your resulting visualisation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(umap.UMAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "time_start = time.time()\n",
    "umap_transformer = umap.UMAP(n_components=3)\n",
    "\n",
    "umap_data = umap_transformer.fit_transform(X)\n",
    "\n",
    "time_umap = time.time() - time_start\n",
    "print(f'UMAP done in {time_umap} seconds!')\n",
    "times['umap'] = time_umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercise_df = (\n",
    "    df\n",
    "    .assign(umap1 = umap_data[:, 0])\n",
    "    .assign(umap2 = umap_data[:, 1])\n",
    "    .assign(umap3 = umap_data[:, 2])\n",
    ")\n",
    "\n",
    "exercise_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP 2D\n",
    "plot_2d(exercise_df, 'umap1', 'umap2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP 3D\n",
    "plot_3d(exercise_df, 'umap1', 'umap2', 'umap3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike t-SNE, UMAP can be applied to transform new data. This makes it not only useful for generating visualisations, but also gives us the option to use it in machine learning tasks. It is, however, quite a bit slower than PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen three different techniques for dimensionality reduction for visualisation.\n",
    "\n",
    "**Principal Component Analysis** is a linear projection-based transformation technique that compresses a dataset onto a lower-dimensional feature subspace, while maintaining as much relevant information as possible. It ais to find the directions of maximum variance in the high-dimensional data, and projects it onto a new subspace with equal or fewer dimensions than the original. While it can be used for visualisation and exploratory data analysis or to reduce required storage space and improve computational efficiency, it is in practice also used to improve the predictive performance for both supervised methods and clustering. \n",
    "\n",
    "**t-Distributed Stochastic Neighbor Embedding** is a non-linear manifold learning based transformation technique. It is not able to transform new data, and not widely used to improve predictive performance for machine learning models. However, it is an incredibly powerful - though slow - technique that is especially useful for exploratory data analysis and visualisation. With higher dimensional data, it is recommended to first apply another dimensionality reduction technique such as PCA to reduce the number of dimensions to no more than 50 before t-SNE is applied.\n",
    "\n",
    "**Uniform Manifold Approximation and Projection** is another non-linear manifold learning based transformation technique that offers a number of advantages over t-SNE, including increased speed and better preservation of the data's global structure. It is able to transform new data and can therefore be used both for visualisation as well as a dimensionality reduction technique to improve predictive performance, although PCA is more commonly used for the latter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for transformer, time in times.items():\n",
    "    print(f'{transformer.replace(\"-\", \" & \")} took {time} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "plot_2d(df, 'pc1', 'pc2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# t-SNE\n",
    "plot_2d(df, 'tsne1_2d', 'tsne2_2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE with PCA\n",
    "plot_2d(df, 'tsne1_pca', 'tsne2_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UMAP\n",
    "plot_2d(df, 'umap1', 'umap2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
