{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=images/gdd-logo.png width=300px align=right>\n",
    "\n",
    "# Unsupervised Learning\n",
    "\n",
    "**Supervised learning** algorithms are machine learning algorithms that learn with the help of external feedback: the algorithm makes a prediction, compares its prediction with a provided ground truth, and \"learns\" by adjusting its internal parameters. \n",
    "\n",
    "In contrast, the techniques described in this lecture do not rely on some external notion of what is or is not correct; this class of learning techniques is referred to as **unsupervised learning**. \n",
    "\n",
    "In this notebook, we will take a look at one of the most popular dimensionality reduction techniques: principal component analysis. \n",
    "\n",
    "# PCA\n",
    "- [Introduction to PCA](#intuition)\n",
    "- [PCA for visualisation](#vis)\n",
    "- [PCA for classification](#clas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intuition'></a>\n",
    "## Introduction to PCA\n",
    "### PCA: Motivation\n",
    "PCA is a technique used for dimensionality reduction, which can have a number of practical benefits:\n",
    "- **Data Compression**: In high-dimensional spaces, storing and processing fewer dimensions saves computational resources.\n",
    "- **Visualization**: When reducing to 2 or 3 dimensions, it’s easier to plot and analyze patterns in the data.\n",
    "- **Noise Reduction**: PCA can help filter out noise by compressing components with low variance (which often represent irrelevant or random fluctuations).\n",
    "\n",
    "\n",
    "### PCA: Intuition\n",
    "\n",
    "The idea behind Principal Component Analysis (PCA) is rooted in the fact that, although our data may exist in a high-dimensional space (e.g., hundreds or thousands of features), often the important patterns or structures in the data can be captured in a lower-dimensional space without losing much information.\n",
    "\n",
    "Imagine you’re looking at a cloud of points in 3D space (like a scatterplot of stars in the sky). While this cloud seems to span three dimensions, it might actually lie mostly along a flat plane within the 3D space. If that's true, we could describe the entire cloud using just two dimensions (the x and y coordinates of the plane) instead of all three. PCA helps us identify this lower-dimensional structure by:\n",
    "\n",
    "- Finding the direction in which the data varies the most (called the first principal component).\n",
    "- Finding additional directions (orthogonal to the first) that capture the remaining variability*.\n",
    "\n",
    "For data in *n*-dimensional space, there will be at max *n* principal components.\n",
    "\n",
    "<img src=\"images/pca.png\" alt=\"PCA Illustration\" height=600 width=600>\n",
    "\n",
    "### Why does this work?\n",
    "1. Redundancy in Features:\n",
    "    - Real-world datasets often have features that are highly correlated. For example, \"height in inches\" and \"height in centimeters\" are essentially the same information. PCA merges such redundant information into a single component.\n",
    "    Focus on Variation:\n",
    "\n",
    "2. PCA identifies the directions where the data changes the most. These are the directions that carry the most information about the dataset. \n",
    "    - By ignoring directions with minimal variation (which often represent noise or irrelevant details), PCA simplifies the data without losing its core structure.\n",
    "  \n",
    "### 2D Example\n",
    "\n",
    "Let’s take a 2D example to visualize the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(1)\n",
    "X = np.dot(rng.rand(2, 2), rng.randn(2, 200)).T\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Principal Components**:\n",
    "- The first principal component (PC1) will be the line that passes through the middle of the scatter and points in the direction of maximum spread of the points.\n",
    "- The second principal component (PC2) is orthogonal (perpendicular) to the first and represents the next largest direction of variation.\n",
    "\n",
    "By projecting the data onto PC1 and PC2, you essentially rotate the axes to align with the directions of the most important variations, simplifying how the data is represented."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works using **scikit-learn**! \n",
    "\n",
    "`PCA` is just like any other scikit-learn transformer. However, as it is an **unsupervised** method, it only takes X and **no y**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the estimator has been fitted to the data, some attributes are set. \n",
    "\n",
    "You can then extract the principal axes in feature space, representing the directions of maximum variance in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, you can see the amount of variance explained by each of the selected components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, the percentage of variance explained by each of the selected components can also be extracted. As our number of principal axes is equal to the original number of axes, this sums up to 1.0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise these values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',\n",
    "                    linewidth=2,\n",
    "                    shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    # Scale the vector based on the explained variance\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    \n",
    "    # Draw vector in both directions\n",
    "    draw_vector(pca.mean_ - v, pca.mean_ + v)\n",
    "\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "  <summary><span style=\"color:blue\">What is that 3 doing there?</span></summary>\n",
    "  \n",
    "The number 3 in this line:\n",
    "    \n",
    "`v = vector * 3 * np.sqrt(length)`\n",
    "    \n",
    "is a scaling factor used to make the principal component vectors visually longer so they stand out clearly when plotted over the data. It has no mathematical significance in terms of PCA itself; it's purely for visualization purposes.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now that you have seen how to compute principal components, let's have a go at using it for dimensionaly reduction.\n",
    "\n",
    "#### <mark>Exercise: Compress to one component</mark>\n",
    "\n",
    "1. Perform PCA with only one component on the feature matrix *X*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/pca-one-component.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Check the shape of the data before and after, has it worked?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## add your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compare the output of the following cells to the PCA with 2 components: what is different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also visualise this as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = pca.inverse_transform(X_pca)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.scatter(X_new[:, 0], X_new[:, 1], color='blue', alpha=0.7)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_info = pca.explained_variance_ratio_.sum()*100\n",
    "n_components = pca.components_.shape[0]\n",
    "print(f'In total, you managed to preserve {retained_info:.2f}% of information with {n_components} component(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='vis'></a>\n",
    "## PCA for visualisation\n",
    "\n",
    "Below, we have a dataset with measurements of abnormalities (nodules) that may be breast cancer (malignant) or simply benign. For each nodule, there are 30 different types of measurements. Taking these measurements is a time-consuming, labour-intensive task that the radiologist is tasked with, which limits the number of patients they can review in a single day. Therefore, our aim is not only to create a model that can accurately predict whether a nodule is malignant or benign, but also to see if we can limit the number of measurements the radiologist has to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/cancer.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with some early preprocessing: split the data in X and y and remove unnecessary columns like 'Unnamed' and 'id'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['Unnamed: 32'], axis='columns').dropna()\n",
    "y = df['diagnosis']\n",
    "X = df.drop(['id','diagnosis'], axis = 'columns')\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "\n",
    "When you apply PCA on any regular dataset, scaling is **hugely** important. \n",
    "\n",
    "<mark>**Question:** Why do you think this is so important?</mark>\n",
    "<details>\n",
    "    \n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "  \n",
    "In PCA, we are interested in the components that maximize the variance. If one component (e.g. height) varies less than another (e.g. weight) because of their respective scales (meters vs. kilos), PCA might determine that the direction of maximal variance more closely corresponds with the ‘weight’ axis, if those features are not scaled. As a change in height of one meter can be considered much more important than the change in weight of one kilogram, this is clearly incorrect. This is why we scale the data with the `StandardScaler` before we apply PCA. \n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's scale the data using a standard scaler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What would you now expect the mean and standard deviation to be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check mean\n",
    "X_scaled.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.std()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we're interested in visualising our data first, we choose either **two** or **three** components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data seems to be properly scaled, which means you can apply PCA! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca_transformer = PCA(n_components=2)\n",
    "pca_data = pca_transformer.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pca = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "df_pca.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformer.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retained_info = pca_transformer.explained_variance_ratio_.sum()*100\n",
    "n_components = pca_transformer.components_.shape[0]\n",
    "print(f'In total, you managed to preserve {retained_info:.2f}% of information with {n_components} component(s)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise our data on these components.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise(df_pca, targets, colors, labels):\n",
    "    plt.figure()\n",
    "    plt.title('Principal Component Analysis')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "\n",
    "    for i in range(len(targets)): \n",
    "        indices = labels == targets[i]\n",
    "        plt.scatter(\n",
    "            df_pca.loc[indices, 'PC1'],\n",
    "            df_pca.loc[indices, 'PC2'],\n",
    "            c = colors[i], s = 20)\n",
    "\n",
    "    plt.legend(targets);\n",
    "    \n",
    "targets = ['B', 'M']\n",
    "colors = ['g', 'r']\n",
    "visualise(df_pca, targets, colors, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks quite nice! The dataset seems pretty linearly separable based on these two components. \n",
    "\n",
    "How would it look if you hadn't scaled the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unscaled version\n",
    "pca_transformer = PCA(n_components=2)\n",
    "pca_data = pca_transformer.fit_transform(X)\n",
    "\n",
    "df_pca_unscaled = pd.DataFrame(pca_data, columns=['PC1', 'PC2'])\n",
    "visualise(df_pca_unscaled, targets, colors, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much less nice! \n",
    "\n",
    "### <mark>Exercise: Visualise the Palmer Penguins dataset with PCA.</mark>\n",
    "\n",
    "Let's load in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penguins = pd.read_csv('data/penguins.csv')\n",
    "penguins.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA cannot be performed on rows with missing values and does not perform well on categorical data. Hence we shall process our dataset accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "penguins_processed = penguins.dropna()\n",
    "feature_columns = ['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm', 'body_mass_g']\n",
    "\n",
    "X_penguins = penguins_processed.loc[:, feature_columns]\n",
    "y_penguins = penguins_processed.loc[:, 'species'].reset_index(drop=True)\n",
    "\n",
    "print(f'The shape of feature matrix X is: {X_penguins.shape}')\n",
    "print(f'The shape of target vector y is: {y_penguins.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,\n",
    "1. Scale the dataset,\n",
    "2. Perform PCA, \n",
    "3. Visualise the PCA transformation (using the *visualise* function from earlier) , and\n",
    "4. Calculate how much information was retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "    \n",
    "  <summary><span style=\"color:blue\">Show hint</span></summary>\n",
    "  \n",
    "When visualising the PCA transformation, make sure:\n",
    "    \n",
    "- That you use the right input data in every transformation step;\n",
    "- That the `targets` and `colors` variables contain appropriate values for this penguin dataset (e.g. the targets are not \"B\" and \"M\" anymore).\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex-PCA.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA and categorical values\n",
    "\n",
    "When you preprocessed the dataset for PCA, you only took a few columns into consideration. _Species_ was dropped as this is the target variable, but _island_ and _sex_ were also dropped. \n",
    "\n",
    "<mark>**Question:** What's different about these and why might you not want to include them in the PCA?</mark>\n",
    "\n",
    "<details>\n",
    "    \n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "  \n",
    "PCA is designed for _continuous_ variables. It aims to minimize the variance (the squared devations). However, the concept of squared deviations breaks down when you have binary variables (the output of one-hot encoding your categorical variables). This means that PCA can be used (in the sense that you get an output) but that output will be less _meaningful_ than you'd want it to be. \n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clas'></a>\n",
    "## PCA for classification\n",
    "\n",
    "There is a large variety of algorithms to choose from for classification purposes, each of which has their own advantages and disadvantages. Some of these algorithms, including k-nearest neighbors algorithm, are not very suitable for high-dimensional data. In the case of k-nearest neighbors, this is because the distance measure loses accuracy with high-dimensional data: there is little difference between the nearest and farthest neighbors. \n",
    "\n",
    "Feature selection focuses on **discarding irrelevant features**, which k-nearest neighbor also is not very robust against as it assigns the same importance to all features, and automatically leads to a reduction in dimensionality. But to relief the problem of high dimensionality, dimensionality reduction techniques can be applied to transform the data before we train our classifier. \n",
    "\n",
    "Let's see that in action on the breast cancer dataset! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to create a train-test split and a baseline pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "base = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "base.fit(X_train, y_train)\n",
    "y_pred = base.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, you want to extend this pipeline with PCA. Remember that scaling must be performed _before_ PCA is used to transform the data! However, how do you choose the right number of components?\n",
    "\n",
    "First of all, there are 30 features to start with, so your number of components must be less than 30. What happens if you apply PCA with no number of components specified? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data. \n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Perform PCA. \n",
    "pca_transformer = PCA()\n",
    "pca_data = pca_transformer.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca_transformer.components_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA without the number of components specified gives you back the same number of components as number of features you put in: 30. This also means that, with these 30 components, you should be able to perfectly model the data. You can prove this by looking at the explained variance ratio. This should sum to 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(pca_transformer.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what determines the number of components you choose if we want to reduce the dimensionality? You want the lowest amount of components that most accurately models the data. Let's visualise this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot(np.cumsum(pca_transformer.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_transformer.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Exercise: Include PCA in the pipeline</mark>\n",
    "Extend the pipeline to include PCA. Plug in various values based on the graph. What gives you the best results? Can you explain that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', KNeighborsClassifier())\n",
    "])\n",
    "\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "PCA is a powerful and fast _unsupervised_ approach for reducing the number of dimensions in your dataset by finding the principal components in the dataset. It is important to scale the data before applying PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
