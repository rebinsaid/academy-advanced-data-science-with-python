{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/pandas_pipelines/panda-paint.jpeg' width='200px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "# Modern Pipelines in pandas\n",
    "\n",
    "This notebook considers `pandas` in practice and how we can adopt great practice when working with data. \n",
    "\n",
    "Let's pretend that we've read in a timeseries and that this is the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_ts_df():\n",
    "    dates = [str(_) for _ in pd.date_range(\"2018-01-01\", \"2019-01-01\")]\n",
    "    values = [np.nan if np.random.random() < 0.05 else _ for _ in np.random.normal(0, 1, 366)]\n",
    "    return pd.DataFrame({\"date\": dates, \"value\": values})\n",
    "\n",
    "date_df = make_ts_df()\n",
    "date_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start analysing the data, let's imagine we want to do the following:\n",
    "\n",
    "- Get rid of the redundant hours.\n",
    "- Clean the `nan` values.\n",
    "- Remove outliers. \n",
    "\n",
    "One way of doing it could be like so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    date_df\n",
    "    .assign(date=lambda df: pd.to_datetime(df['date']).dt.normalize())\n",
    "    .dropna()\n",
    "    .loc[lambda df: df['value'] > -2.0]\n",
    "    .loc[lambda df: df['value'] < 2.0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the way we've been doing it so far, but we can do better.\n",
    "\n",
    "If you were to just look at the code above it could be a bit hard to understand what is going on.\n",
    "\n",
    "Also, if we were to get a new date dataframe, we'd have to start all over again. \n",
    "\n",
    "Whilst this is not a big issue when we are only doing 3 processing steps, as the amount of processing increases it could become time consuming.\n",
    "\n",
    "## Pipeline abstraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(dataf):\n",
    "    \"\"\"Removes the hours from dates\"\"\"\n",
    "    return (dataf\n",
    "            .assign(date=lambda d: pd.to_datetime(d['date']).dt.normalize()))\n",
    "\n",
    "def remove_nan_rows(dataf):\n",
    "    \"\"\"Removes rows with missing values\"\"\"\n",
    "    return (dataf.dropna())\n",
    "\n",
    "def fill_nan(dataf):\n",
    "    \"\"\"Replaces NaN values with 0\"\"\"\n",
    "    return (dataf.fillna(0))\n",
    "\n",
    "def remove_outliers(dataf):\n",
    "    \"\"\"Removes values less than -2 and greater than 2\"\"\"\n",
    "    return (dataf\n",
    "            .loc[lambda d: d['value'] > -2.0]\n",
    "            .loc[lambda d: d['value'] < 2.0])\n",
    "\n",
    "prep_df = (date_df\n",
    "           .pipe(parse_dates)\n",
    "           .pipe(remove_nan_rows)\n",
    "           .pipe(remove_outliers))\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_df = (\n",
    "    date_df\n",
    "   .pipe(parse_dates)\n",
    "   .pipe(remove_nan_rows)\n",
    "   .pipe(remove_outliers)\n",
    ")\n",
    "prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.pipe()` method allows us to pass a function that accepts a dataframe as it's first argument. This is a very nice flow. \n",
    "\n",
    "- We can easily use this pipeline (or parts of this pipeline) for different datasets.\n",
    "\n",
    "<img src='images/lego.png' width='400px'  style=\"padding: 15px\">\n",
    "\n",
    "- If there is ever a bug this pipeline will make it easier for us to figure out where it is. Since every step is merely a function, we'll know eactly where the process is breaking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We can give the function a descriptive name and on a pipeline level this allows us to see \"what\" is happening \"when\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g. You may not have seen how the parse_dates function works yet\n",
    "help(parse_dates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveats \n",
    "\n",
    "We should be careful when we are writing `.pipe`-lines. The function going into a `.pipe()` might not be ***stateless***. Here's an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df = make_ts_df() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(dataf):\n",
    "    dataf.columns = [\"a\", \"b\"]\n",
    "    return dataf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df.pipe(rename_columns).columns, date_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In such a situation it is best to include a `.copy()` command - or better - use a stateless method like `.rename()`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's recreate the random data\n",
    "date_df = make_ts_df()\n",
    "\n",
    "def rename_columns(dataf):\n",
    "    return dataf.rename(columns = {'date':'a','value':'b'}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_df.pipe(rename_columns).columns, date_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with this. We want our functions to be stateless, otherwise we might accidentally change the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline abstraction on higher Levels\n",
    "\n",
    "To fully appreciate what the pandas pipelines can do, let us rewrite one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(dataf, min_value=None, max_value=None):\n",
    "    \"\"\"Removes outliers less than min_value and greater than max_value\"\"\"\n",
    "    \n",
    "    if not (min_value and max_value):\n",
    "        raise ValueError('Hey silly, you need to state a min and max!')\n",
    "    \n",
    "    return (dataf\n",
    "            .loc[lambda d: d['value'] > min_value]\n",
    "            .loc[lambda d: d['value'] < max_value])\n",
    "\n",
    "(\n",
    "    date_df\n",
    "    .pipe(parse_dates)\n",
    "    .pipe(remove_nan_rows)\n",
    "    .pipe(remove_outliers, min_value=-2, max_value=2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.pipe()` can accept keyword arguments. This allows you to change, say, threshold values on a high level. No need to change the original function, you can change things from a higher level. This is great because it will encourage you to write functions that are general. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <mark>Exercise</mark>\n",
    "\n",
    "Rewrite the following as a pandas pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sanfran = pd.read_csv('data/san_fran_crime_sample.csv')\n",
    "sanfran.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sanfran\n",
    "    .rename(columns=str.lower)\n",
    "    .rename(columns={'dates': 'date'})\n",
    "    .assign(date = lambda df: pd.to_datetime(df['date']).dt.normalize())\n",
    "    .set_index('date')\n",
    "    .sort_index()\n",
    "    .loc['2004':'2014']\n",
    "    .resample('ME')[['category']].count()\n",
    "    .assign(category_rolling = lambda df: df['category'].rolling(10).mean())\n",
    "    .plot(figsize=(9,5), title='Crime Count in San Fransisco')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/pandas_pipelines/pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <mark>Bonus Exercise: Add a decorator<mark>\n",
    "\n",
    "Familiar with decorators? Add a decorator to log:\n",
    "    \n",
    "- the shape of the dataframe before and after (see decorator `log_shape`)\n",
    "- the time it takes to run the function (create a decorator called `log_time`)\n",
    "    \n",
    "We can add a little more power here and add some logging functionality with **a decorator**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: See below for example with decorators on the dataframe `date_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import wraps\n",
    "\n",
    "\n",
    "def log_shape(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        result = func(*args, **kwargs)\n",
    "        shape_before = args[0].shape\n",
    "        shape_after = result.shape\n",
    "        print(f\"{func.__name__} => before shape:{shape_before} after shape:{shape_after}\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@log_shape\n",
    "def parse_dates(dataf):\n",
    "    return (dataf\n",
    "            .assign(date=lambda d: pd.to_datetime(d.date)))\n",
    "\n",
    "@log_shape\n",
    "def remove_nan_rows(dataf):\n",
    "    return (dataf.dropna())\n",
    "\n",
    "@log_shape\n",
    "def remove_outliers(dataf, min_val=-2.0, max_val=2.0):\n",
    "    return (dataf\n",
    "            .loc[lambda d: d['value'] > min_val]\n",
    "            .loc[lambda d: d['value'] < max_val])\n",
    "\n",
    "prep_df = (date_df\n",
    "           .pipe(parse_dates)\n",
    "           .pipe(remove_nan_rows)\n",
    "           .pipe(remove_outliers, min_val=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/pandas_pipelines/pipeline-decorator.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the benefit of having a standard decorator that can log pandas steps: \n",
    "\n",
    "1. When writing code, this might help you in discovering what is happening. If you see rows dissapear while they shouldn't this log might give you a proxy. \n",
    "2. When this pandas code goes to production you will have some logging for free in airflow. If something goes wrong there you may also be able to debug more easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion \n",
    "\n",
    "> **\"Pipelines are the only correct way to write pandas.\"**\n",
    "\n",
    "This is a bold statement, but some of people very strongly about this. \n",
    "\n",
    "Even if you take this statement with a grain of salt, it is important to write your code in such a way that your notebooks remains clear - if it takes a lot of effort to understand the code of your colleagues, then your team will be slower than you want it to be. \n",
    "\n",
    "A notebook is a great scratchpad, but that is no excuse to write unclear code!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
