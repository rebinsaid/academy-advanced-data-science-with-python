{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worldwide-bouquet",
   "metadata": {},
   "source": [
    "### <img src=images/gdd-logo.png width=200px align=right>\n",
    "# Cost Sensitive Learning\n",
    "\n",
    "In the previous notebook, you explored how over- and under- sampling techniques can address issues with modeling imbalanced data and learned to utilize appropriate evaluation metrics.\n",
    "\n",
    "Taking a step back, we realize that the goal is not to only account for class imbalance, but to build a classifier that is robust and representative of the real-world, and more importantly, drives business value!\n",
    "\n",
    "ü§î Assuming the minority class is the negative class, if the cost of false negatives are far lower compared to false positives, would you even bother with resampling?\n",
    "\n",
    "In this notebook, we‚Äôre going to explore how to build robust classifiers that really focus on boosting business value. We‚Äôll kick things off by looking at how each classification (and any misclassifications) impacts your business‚Äôs bottom line. We‚Äôll break this down using something we call a ‚Äúcost matrix.‚Äù Let‚Äôs dive in! üí∞\n",
    "\n",
    "\n",
    "### Outline \n",
    "- [The cost matrix](#intro)\n",
    "- [Cost-sensitive learning in ```sklearn```](#sklearn)\n",
    "- [Tuning the decision threshold](#tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a880d0-1eed-4708-bb51-6959203c31cf",
   "metadata": {},
   "source": [
    "<a id = 'intro'></a>\n",
    "\n",
    "## Cost matrix\n",
    "\n",
    "The essence of cost-sensitive decision-making is that it can be optimal to act as if one class is true even when some other class is more probable. For example, it can be rational not to approve a large credit card transaction even if the transaction is most likely legitimate. \n",
    "\n",
    "When working on a task where the cost of misclassification is not equal, you can use a ***cost matrix*** to specify the cost of misclassification.\n",
    "\n",
    "### Example: Fraud Detection\n",
    "Let's take an example of a banking application, in particular, credit card transaction fraud detection. \n",
    "\n",
    "In this case, the cost of labelling a fraud as a non-fraud is much higher than labelling a non-fraud as a fraud. This is because missing a fraudulent transaction (false negative) involves a loss directly related to the amount of the transaction, but also on further fraudulent uses of the credit/debit card. At the same time, the blocking of transactions that are legitimate (false positive) causes inconvenience to customers, generates useless investigation costs, and also impacts the company reputation. \n",
    "\n",
    "In this case, the cost matrix might look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0451f384-4f60-41d2-833c-b075c0d2d50f",
   "metadata": {},
   "source": [
    "| | Predicted: Fraud | Predicted: Non-Fraud |\n",
    "| --- | --- | --- |\n",
    "| **Actual: Fraud** | 0 | 5 |\n",
    "| **Actual: Non-Fraud** | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ce08a7-52f5-43a6-a25a-0e72849edd9e",
   "metadata": {},
   "source": [
    "####  <mark>Exercise: Define a cost matrix</mark>\n",
    "\n",
    "Choose **one** of the following applications and try to define a cost matrix. You will then discuss with your peers your motivations for choosing certain costs.\n",
    "\n",
    "1. You are a data scientist at a manufacturing company producing automobile parts and are tasked with building a model to predict whether a part is defective (positive) or not (negative) based on optical inspection. False negatives might lead to death on the highway while false positive might lead to good parts being discarded. What cost matrix would you define?\n",
    "2. You are a data scientist working in a bank. You are tasked with building a model to predict whether a customer will default on their loan given their financial information . False negatives might lead to missed payments while false positives might lead to lost opportunity costs. What cost matrix would you define?\n",
    "3. You are a data scientist at a hospital and are tasked with building a model to predict whether a patient has a tumor (positive) or not (negative) based on a biopsy. False negatives might lead to death while false positives might lead to unnecessary surgery. What cost matrix would you define?\n",
    "\n",
    "\n",
    "Add your answer in the cell below making sure to specify the application you are working on and the class labels you are using. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acquired-claim",
   "metadata": {},
   "source": [
    "*Double-click or press Enter to open cell*\n",
    "\n",
    "| | Predicted: 0| Predicted: 1|\n",
    "| --- | --- | --- |\n",
    "| **Actual: 0** | 0 | ? |\n",
    "| **Actual: 1** | ? | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-savannah",
   "metadata": {},
   "source": [
    "<a id = 'sklearn'></a>\n",
    "As you may have noticed, defining a cost matrix is hard and often requires the assistance of a domain expert. In practice, a simple heuristic that is often used to define cost matrices is to assign costs based on the inverse class distribution. This is achieved by setting the ```class_weight``` parameter in the model to ```balanced```. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9634ea03",
   "metadata": {},
   "source": [
    "<a id = 'sklearn'></a>\n",
    "\n",
    "## Cost-sensitive learning in ```sklearn```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28eb2097",
   "metadata": {},
   "source": [
    "In this example, we are going to use the [Statlog](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) dataset to predict whether a customer will default on their loan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b1930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "sklearn.set_config(transform_output=\"pandas\")\n",
    "\n",
    "credit_df = fetch_openml(data_id=31, as_frame=True, parser=\"pandas\")\n",
    "X, y = credit_df.data, credit_df.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd21dcc",
   "metadata": {},
   "source": [
    "###  <mark>**Exercise**</mark>\n",
    "\n",
    "1. Explore the dataset and identify the class distribution of the target variable.\n",
    "2. Encode the positive class (good credit) as 1 and the negative class (bad credit) as 0.\n",
    "3. Build, fit, and evaluate a pipeline instance consisting of appropriate preprocessing steps and an ensemble classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c315c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answers here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85959e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/credit-lending-pipeline.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d453e7",
   "metadata": {},
   "source": [
    "### Adding a cost matrix to the pipeline\n",
    "\n",
    "Let's now add a cost matrix to score our model based on the cost of misclassification.\n",
    "\n",
    "| | Predicted: Good credit | Predicted: Bad credit |\n",
    "| --- | --- | --- |\n",
    "| **Actual: Good credit** | 0 | -1 |\n",
    "| **Actual: Bad credit** | -5 | 0 |\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "<details>\n",
    "  <summary>üí° Why do we multiply the costs by -1?</summary>\n",
    "    Scikit-learn model selection tools expect that we follow a convention\n",
    "    that \"higher\" means \"better\", and that the weights represent gains, minimizing the cost is equivalent to maximizing the gain.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd127e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, make_scorer, precision_score, recall_score, accuracy_score\n",
    "\n",
    "\n",
    "def monetary_gain_score(y, y_pred):\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    gain_matrix = np.array(\n",
    "        [\n",
    "            [0, -1],  # -1 gain for false positives\n",
    "            [-5, 0],  # -5 gain for false negatives\n",
    "        ]\n",
    "    )\n",
    "    return np.sum(cm * gain_matrix)\n",
    "\n",
    "\n",
    "scores = {\n",
    "    \"accuracy\": make_scorer(accuracy_score),\n",
    "    \"precision\": make_scorer(precision_score),\n",
    "    \"recall\": make_scorer(recall_score),\n",
    "    \"monetary_gains\": make_scorer(monetary_gain_score)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18232911",
   "metadata": {},
   "source": [
    "Let's now print out the monetary cost of the model's (mis)classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4ddeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {scores['accuracy'](model, X_test, y_test)}\")\n",
    "print(f\"Precision: {scores['precision'](model, X_test, y_test)}\")\n",
    "print(f\"Recall: {scores['recall'](model, X_test, y_test)}\")\n",
    "print(f\"Business cost metric: {scores['monetary_gains'](model, X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d542c3c",
   "metadata": {},
   "source": [
    "We can also investigate the precision-recall curve to better understand the model's sensitivity to the decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7c8cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "\n",
    "def plot_precision_recall_curve(est, ax, name, decision_threshold):\n",
    "    PrecisionRecallDisplay.from_estimator(\n",
    "        est,\n",
    "        X_test,\n",
    "        y_test,\n",
    "        ax=ax,\n",
    "        name=name,\n",
    "    )\n",
    "    ax.plot(\n",
    "        scores[\"recall\"](est, X_test, y_test),\n",
    "        scores[\"precision\"](est, X_test, y_test),\n",
    "        marker=\"o\",\n",
    "        markersize=10,\n",
    "        label=f\"Decision Threshold: {decision_threshold:.2f}\",\n",
    "    )\n",
    "    ax.set_title(\"Precision-Recall curve\")\n",
    "    ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plot_precision_recall_curve(model, ax, \"GBM\", 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed04d081",
   "metadata": {},
   "source": [
    "<a id = 'tuning'></a>\n",
    "\n",
    "## Tuning the decision threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d03783d6",
   "metadata": {},
   "source": [
    "To find the optimal decision threshold, we need to compute the expected cost (or gain) for each possible threshold value.\n",
    "Rather than computing the costs manually, we can use the ```TunedThresholdClassifierCV``` class to automatically find the optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0921d064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TunedThresholdClassifierCV\n",
    "\n",
    "tuned_model = TunedThresholdClassifierCV(\n",
    "    estimator=model,\n",
    "    scoring=scores[\"monetary_gains\"],\n",
    "    store_cv_results=True,  # necessary to inspect all results\n",
    ")\n",
    "\n",
    "tuned_model.fit(X_train, y_train)\n",
    "print(f\"{tuned_model.best_threshold_=:0.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25ffc55",
   "metadata": {},
   "source": [
    "Let's now visualize the cost curve and find the optimal threshold, and further, evaluate the model's performance using the optimal threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e2e26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_objective_score_curve(tuned_model, ax):\n",
    "    ax.plot(\n",
    "        tuned_model.cv_results_[\"thresholds\"],\n",
    "        tuned_model.cv_results_[\"scores\"],\n",
    "        color=\"tab:orange\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        tuned_model.best_threshold_,\n",
    "        tuned_model.best_score_,\n",
    "        \"o\",\n",
    "        markersize=10,\n",
    "        color=\"tab:orange\",\n",
    "        label=\"Optimal cut-off point for the business metric\",\n",
    "    )\n",
    "    ax.legend()\n",
    "    ax.set_xlabel(\"Decision threshold (probability)\")\n",
    "    ax.set_ylabel(\"Monetary gains\")\n",
    "    ax.set_title(\"Business metric as a function of the decision threshold\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 6))\n",
    "plot_precision_recall_curve(tuned_model, axs[0], \"GBM\", tuned_model.best_threshold_)\n",
    "plot_objective_score_curve(tuned_model, axs[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c0826b",
   "metadata": {},
   "source": [
    "Have we improved the model's performance by tuning the decision threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57a92b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Accuracy: {scores['accuracy'](tuned_model, X_test, y_test)}\")\n",
    "print(f\"Precision: {scores['precision'](tuned_model, X_test, y_test)}\")\n",
    "print(f\"Recall: {scores['recall'](tuned_model, X_test, y_test)}\")\n",
    "print(f\"Business cost metric: {scores['monetary_gains'](tuned_model, X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4632b0e4",
   "metadata": {},
   "source": [
    "####  <mark>Food for thought ü§î</mark>\n",
    "\n",
    "1. Why does the precision recall curve not change for the tuned threshold classifier?\n",
    "2. The tuned threshold classifier uses cross-validation to find the optimal threshold. What if we want to tune a pre-trained model?\n",
    "   1. Look at the [documentation](https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.TunedThresholdClassifierCV.html) for the ```TunedThresholdClassifierCV``` class and see what function arguments you need to change to use a pre-trained model.\n",
    "   2. Would you use the same dataset to train and tune the threshold? Why or why not?\n",
    "3. Repeat the above steps for a cost-matrix where the cost of false positives is twice the cost of false negatives. How does the optimal threshold change?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
