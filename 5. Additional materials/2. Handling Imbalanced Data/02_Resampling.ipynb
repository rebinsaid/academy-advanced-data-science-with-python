{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "worldwide-bouquet",
   "metadata": {},
   "source": [
    "### <img src=images/gdd-logo.png width=200px align=right>\n",
    "# Resampling\n",
    "\n",
    "In the last notebook, you saw how imbalanced data can cause problems when training a machine learning model. You also chose a metric that is more appropriate than accuracy. In this notebook, you will see how you can improve your predictions using resampling.\n",
    "\n",
    "### Outline \n",
    "- [Can you collect more data?](#more)\n",
    "- [Resampling](#resample)\n",
    "    - [Undersampling](#under)\n",
    "    - [Oversampling](#over)\n",
    "    - [Combining under- and oversampling](#combine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29af74c-ec7e-4020-bc40-950768ffbc71",
   "metadata": {},
   "source": [
    "Let's load in the data again, prepare the feature matrix and target vector and perform a train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22051ea7-dc19-47f8-b5d7-fbfd6117f1cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from data import create_Xy\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87dba859-b63b-4565-81b6-b4bb662ecfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "stroke = pd.read_csv('data/full_data.csv').rename(str.lower, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "twenty-guyana",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_columns = ['gender', 'ever_married', 'work_type', 'residence_type', 'smoking_status']\n",
    "target = 'stroke'\n",
    "\n",
    "X, y = create_Xy(stroke, \n",
    "                 target=target, \n",
    "                 categorical_columns = categorical_columns)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-acrylic",
   "metadata": {},
   "source": [
    "<a id = 'more'></a>\n",
    "\n",
    "## Can you collect more data?\n",
    "\n",
    "There are generally two main causes for an imbalance in your dataset:\n",
    "\n",
    "1. The imbalance could be due to **collection bias** (e.g., only collecting data that favors one class) or errors made during collection (e.g., misclassifications). In this case it would be worth investigating whether more data can be collected and/or sampling methods can be improved.\n",
    "\n",
    "2. The second cause of imbalance might be that it is simply a **property of the problem domain**. For example, relatively few people suffer from a stroke when looking at the entire population. In such a case, it can be hard to collect more data from the minority group without introducing more bias.\n",
    "\n",
    "<mark>**Question:** Why do you think collecting more data only for the minority class could be a problem?</mark>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "acquired-claim",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ancient-phrase",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "\n",
    "E.g. you want to get more data of stroke patients, so you approach different hospitals. But then, all healthy participants would come from the same hospital, leading to bias.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-savannah",
   "metadata": {},
   "source": [
    "So what can you do when collecting more data is not an option?\n",
    "\n",
    "<a id = 'resample'></a>\n",
    "\n",
    "## Resampling\n",
    "\n",
    "One solution is to try transform the dataset in order to balance the class distribution. This can be done by selectively deleting examples from the majority class (**undersampling**) or duplicating\n",
    "or synthesizing new examples in the minority class (**oversampling**).\n",
    "\n",
    "Balancing the class distributions helps the model to become more sensitive to the minority class. However, it is important to note that resampling is not a silver bullet. Let's now discover some of these methods and discuss their pros and cons.\n",
    "\n",
    "<img src=images/resampling.png width=600px>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thousand-league",
   "metadata": {},
   "source": [
    "<a id = 'under'></a>\n",
    "\n",
    "### Undersampling\n",
    "\n",
    "When undersampling, you're reducing the number of samples from the majority class. There are several different methods of doing this, but the most naive way is by random undersampling, whereby you select random data points from the majority class that will be deleted.\n",
    "\n",
    "Let's compress our data using Principal Component Analysis (PCA) so that we can visualize it in 2-dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-completion",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def plot_majority_minority_class(X, y, title=''):\n",
    "    for label in np.unique(y):\n",
    "        plt.scatter(X[y==label, 0], X[y==label, 1], label=label)\n",
    "    plt.legend()\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# reduce number of features to two with PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_trans = pca.fit_transform(X)\n",
    "\n",
    "plot_majority_minority_class(X_train_trans, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "different-engineering",
   "metadata": {},
   "source": [
    "You can see that a lot of points from the majority class are very similar. \n",
    "\n",
    "Therefore, it may be the case that not all of those points are needed to learn a model that can distinguish between the two classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-origin",
   "metadata": {},
   "source": [
    "<mark>**Question:** Should the train-test split be done before or after resampling? Why?</mark>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "seeing-parcel",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b38a1e-5edf-487c-931e-ce643ba533a3",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "\n",
    "Resampling should be applied to the train set.\n",
    "\n",
    "The purpose of having a test set is so that we can evaluate how the model would perform \"in the wild\" on unseen data. It should therefore be reflective of real-world data.\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "municipal-configuration",
   "metadata": {},
   "source": [
    "### Imbalanced-learn\n",
    "\n",
    "<img src=images/imblearn.png width=300px>\n",
    "\n",
    "A lot of resamplers can be imported from [imbalanced-learn](https://imbalanced-learn.org/stable/). This is a library that provides tools when dealing with classification of imbalanced classes, and relies on scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-angola",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from collections import Counter\n",
    "\n",
    "rand_under = RandomUnderSampler(sampling_strategy=1.0)\n",
    "X_train_res, y_train_res = rand_under.fit_resample(X_train, y_train)\n",
    "\n",
    "print(f\"Original dataset shape: {Counter(y_train)}\\nResampled dataset shape: {Counter(y_train_res)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "settled-queen",
   "metadata": {},
   "source": [
    "<mark>**Question:** Try out different values for the *sampling_strategy* parameter, what does it do?</mark>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "collect-above",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-omaha",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "  <summary><span style=\"color:blue\">Show answer</span></summary>\n",
    "\n",
    "Check out the documentation for the [`RandomUnderSampler`](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html#imblearn.under_sampling.RandomUnderSampler)!\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-carroll",
   "metadata": {},
   "source": [
    "Let's visualise the results after random undersampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "owned-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_train_res_pca = pca.fit_transform(X_train_res)\n",
    "\n",
    "plot_majority_minority_class(X_train_res_pca, y_train_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-sender",
   "metadata": {},
   "source": [
    "<mark>**Question:** How does this PCA visualisation compare to the one produced prior to undersampling?</mark>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "compact-environment",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solid-mobile",
   "metadata": {},
   "source": [
    "Of course you now want to know if undersampling helped to increase the performance of our model (which had a not so dazzling F2 score of 0.0 when we started)!\n",
    "\n",
    "**Note:** To use the RandomUnderSampler from imbalanced-learn, we have to use the `imblearn.pipeline.Pipeline` class, which extends the `sklearn.pipeline.Pipeline` class with support for sampler steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oriental-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline # different from the sklearn Pipeline!\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "model = SVC()\n",
    "\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "\n",
    "pipeline = Pipeline(steps=[('under', under),\n",
    "                          ('model', model)])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "scores = cross_val_score(pipeline, X_train, y_train, scoring=ftwo_scorer, cv=cv, n_jobs=-1)\n",
    "\n",
    "print(f\"Mean score: {(np.mean(scores)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355da87f-91bf-4a70-ae44-7c74119e8e49",
   "metadata": {},
   "source": [
    "That's already an increase in performance!\n",
    "\n",
    "### Other undersampling methods\n",
    "\n",
    "Although random undersampling is simple and often effective, there is a limitation. Data points are randomly removed without considering how important or useful they are to determine the decision boundary between classes. This means that you can easily delete useful information. \n",
    "\n",
    "Other undersampling algorithms exist that try to identify redundant examples for deletion or useful examples for non-deletion.\n",
    "\n",
    "<img src=images/tomek-links.png width=600px>\n",
    "\n",
    "###  <mark>**Exercise: Try other undersampling methods**</mark>\n",
    "\n",
    "Choose **one** of the following undersamplers to investigate:\n",
    "\n",
    "1. Near-Miss-3\n",
    "2. Tomek Links\n",
    "3. One-Sided Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97e4b6e-f9b6-4550-a2a8-5be1ef1cbb9f",
   "metadata": {},
   "source": [
    "1. Import and instantiate your chose method using [imbalanced-learn](https://imbalanced-learn.org/stable/references/under_sampling.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384f9583-e366-4b95-88c6-025080cd8804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import TomekLinks, NearMiss, OneSidedSelection\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf2a76f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex1-1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d631c9-fed0-4a09-8d00-c733e3e1617d",
   "metadata": {},
   "source": [
    "2. Transform the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc01054-8f7c-4954-8874-274aaa844055",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9496820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex1-2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d08ef7-64a6-4058-9ab1-e261a526e9e8",
   "metadata": {},
   "source": [
    "3. Visualize the results after transforming the data using the `plot_majority_minority_class` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf17d95-9220-42d8-926f-7db3fedfead1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex1-3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63c722ab-edaa-4126-a1f5-0f9066db004d",
   "metadata": {},
   "source": [
    "4. Investigate its performance by making a pipeline with the undersampler and a SVC, and then using cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e9f9fa-115f-4177-bf25-04f5bb5a2007",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ecb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex1-4.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab5f572-d3e0-4247-8b82-4aa2b5b442cf",
   "metadata": {},
   "source": [
    "#### Bonus questions\n",
    "\n",
    "Investigate what the undersampler does a little bit more. Look at the documentation and/or google around!\n",
    "\n",
    "1. Does your method select examples to keep, to delete or a combination of both?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "hollywood-accordance",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dominican-synthetic",
   "metadata": {},
   "source": [
    "2. Describe in one or two sentence(s) how it undersamples."
   ]
  },
  {
   "cell_type": "raw",
   "id": "least-ghost",
   "metadata": {},
   "source": [
    "Answer: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc5954f",
   "metadata": {},
   "source": [
    "3. You may have found that some of the undersamplers still gave a F-score of 0. Any idea why this could be the case? And how you could solve it?\n",
    "\n",
    "*Hint: Check what the model predicts using `.fit()` and `.predict()`. Is it predicting both classes?*"
   ]
  },
  {
   "cell_type": "raw",
   "id": "11ead06a",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-toner",
   "metadata": {},
   "source": [
    "<a id = 'over'></a>\n",
    "\n",
    "### Oversampling\n",
    "\n",
    "When oversampling, you're adding copies or synthetic examples of datapoints from the minority class. This helps to balance the class distribution, thus increasing the weights of those datapoints. \n",
    "\n",
    "### <mark>**Exercise: Random Oversampling**</mark>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-edgar",
   "metadata": {},
   "source": [
    "1. Import and instantiate the [RandomOverSampler](https://imbalanced-learn.org/stable/references/over_sampling.html) transformer from imbalanced-learn. Then transform the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offensive-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce8ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex2-1.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-design",
   "metadata": {},
   "source": [
    "2. Visualize the results.\n",
    "    <br>a) Use PCA and the `plot_majority_minority_class` function.\n",
    "    <br>b) How does this PCA visualisation compare to the undersampling one?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "descending-barcelona",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cba02cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex2-2.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-marking",
   "metadata": {},
   "source": [
    "3. Assess the performance of the oversampling pipeline.\n",
    "    <br>a) Create a pipeline containing the RandomOversampler and a SVC. Use cross-validation to see the performance.\n",
    "    <br>b) How does the performance compare to the Undersampler you implemented previously?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-testament",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ccba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex2-3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-limit",
   "metadata": {},
   "source": [
    "### Creating synthetic examples of the minority class\n",
    "\n",
    "Although random oversampling can balance the class distribution, it does not provide any additional information to the model. An alternative is to *synthesize* new examples from the minority class.\n",
    "\n",
    "There are lots of different approaches to synthesizing new data, including **SMOTE**, **KMeans SMOTE** and **ADASYN**. You can read more about these methods [here](https://imbalanced-learn.org/stable/over_sampling.html#smote-adasyn). \n",
    "\n",
    "<img src=images/smote.png width=600px>\n",
    "\n",
    "Let's take a look what happens to the performance when you use SMOTE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d77c9a-eebd-4d0f-b569-fbbf944eec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "model = SVC()\n",
    "over = SMOTE()\n",
    "\n",
    "pipeline = Pipeline(steps=[('over', over),\n",
    "                          ('model', model)])\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "scores_smote = cross_val_score(pipeline, X_train, y_train, scoring=ftwo_scorer, cv=cv, n_jobs=-1)\n",
    "\n",
    "print(f'Mean score (SMOTE): {(np.mean(scores_smote)):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-scanner",
   "metadata": {},
   "source": [
    "<a id = 'combine'></a>\n",
    "\n",
    "### <mark>Exercise: Combining methods</mark>\n",
    "\n",
    "You can also combine methods (e.g., oversampling with undersampling) to improve performance.\n",
    "\n",
    "An example of first oversampling and then undersampling is provided below.\n",
    "\n",
    "1. Experiment with different values for the `sampling_strategy`.\n",
    "2. Evaluate how the model generalises on the **test** data. Use the `ftwo_scorer` function to get the test score of the model.\n",
    "3. **Bonus:** Add Tomek Links to the pipeline. Where would it make most sense and does it improve performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-reminder",
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "over = RandomOverSampler(sampling_strategy=0.3)\n",
    "under = RandomUnderSampler(sampling_strategy=0.5)\n",
    "\n",
    "pipeline_comb = Pipeline(steps=[('over', over),\n",
    "                           ('under', under),\n",
    "                          ('model', model)])\n",
    "\n",
    "scores_comb = cross_val_score(pipeline_comb, X_train, y_train, scoring=ftwo_scorer, cv=cv, n_jobs=-1)\n",
    "\n",
    "print(f'Mean score: {(np.mean(scores_comb)):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c905cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/ex3.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03f131f",
   "metadata": {},
   "source": [
    "# A note on resampling using class weights\n",
    "\n",
    "In sklearn, we have the option of using the `class_weight` parameter in the model. This is a way of ensuring balance in the class distribution without resampling the data.\n",
    "In effect, this parameter mimics resampling by assigning a weight to each class. The weight is inversely proportional to the class frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6396d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import make_scorer, fbeta_score\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "model = SVC(class_weight='balanced')\n",
    "\n",
    "cv = StratifiedKFold(n_splits=10)\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)\n",
    "\n",
    "scores = cross_val_score(model, X_train, y_train, scoring=ftwo_scorer, cv=cv, n_jobs=-1)\n",
    "\n",
    "print(f\"Mean score: {(np.mean(scores)):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-heart",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook covered how to improve predictions by using several resamplers from imbalanced-learn. This included undersampling and oversampling, and a combination of both. Besides random over/undersampling, there are several other algorithms available that resample in a less naive way. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
