{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mlflow\n",
    "\n",
    "<img src='./images/xd-logo.png' width='300px' align='right' style=\"padding: 15px\">\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "\n",
    "- Recognize the components of MLflow\n",
    "- Use MLflow to track experiments\n",
    "- Use MLflow to log metrics, parameters, and artifacts\n",
    "- Use MLflow to log models\n",
    "- Use the MLflow UI to visualize experiments\n",
    "- Use the MLflow API to query experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow is an open source platform for managing the end-to-end machine learning lifecycle. It consists of the following components.\n",
    "\n",
    "<img src='./images/learn-core-components.png' width='400px' align='center' style=\"padding: 15px\">\n",
    ">\n",
    "\n",
    "Source: [MLflow](https://mlflow.org/docs/latest/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tracking Server\n",
    "\n",
    "The core component of MLflow is the tracking server which logs and stores the parameters, metrics, and artifacts of machine learning experiments. In our demo, we will run a local tracking server. In a production environment, you would access a managed tracking server.\n",
    "\n",
    "<img src='./images/tracking-setup-local-server.png' width='400px' align='center' style=\"padding: 15px\">\n",
    "\n",
    "Source: [MLflow](https://mlflow.org/docs/latest/index.html)\n",
    "\n",
    "To start the tracking server, run the following command in your terminal.\n",
    "\n",
    "```bash\n",
    "mlflow server --host 0.0.0.0 --port 8080\n",
    "```\n",
    "\n",
    "Congratulations! You have started the MLflow tracking server. Now, let's start tracking some experiments. üìà\n",
    "\n",
    "‚ö†Ô∏è Remember to keep the tracking server running in the background while you run the code in this notebook.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have started the tracking server, let's start tracking some experiments. üìà\n",
    "We will use the MLflow API to log metrics, parameters, and artifacts.\n",
    "First, let's train a logistic regression model on the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "# Load the Iris dataset\n",
    "X, y = datasets.load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to log our experiment using the MLflow API. \n",
    "First, we set the tracking URI to the local tracking server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "# we set the tracking server uri to the remote server\n",
    "mlflow.set_tracking_uri(uri=\"http://0.0.0.0:8080\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLflow uses the notion of experiments and runs to organize the machine learning lifecycle. An experiment is a collection of runs. A run is a single execution of a machine learning workflow. We will create an experiment called \"MLflow Tracking Demo\" and log the parameters, metrics, and artifacts of the run.\n",
    "\n",
    "<img src='./images/tracking-basics.png' width='600px' align='center' style=\"padding: 15px\">\n",
    "\n",
    "Source: [MLflow](https://mlflow.org/docs/latest/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "from mlflow.models import infer_signature\n",
    "\n",
    "\n",
    "# Create a new MLflow Experiment\n",
    "mlflow.set_experiment(\"MLflow Tracking Demo\")\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run() as run:\n",
    "\n",
    "    # Define the model hyperparameters\n",
    "    params = {\n",
    "        \"solver\": \"lbfgs\",\n",
    "        \"max_iter\": 10000,\n",
    "        \"random_state\": 42,\n",
    "    }\n",
    "    # Log the model hyperparameters\n",
    "    mlflow.log_params(params)\n",
    "\n",
    "    # Train the model\n",
    "    lr = LogisticRegression(**params)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    # Infer the model signature\n",
    "    signature = infer_signature(X_train, lr.predict(X_train))\n",
    "\n",
    "    # Obtain predictions\n",
    "    y_pred = lr.predict(X_test)\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Log the loss metric\n",
    "    mlflow.log_metric(\"accuracy\", accuracy)\n",
    "\n",
    "    # Set a tag that we can use to remind ourselves what this run was for\n",
    "    mlflow.set_tag(\"Training Info\", \"Basic LR model for iris data\")\n",
    "\n",
    "    # Log the model\n",
    "    model_info = mlflow.sklearn.log_model(\n",
    "        sk_model=lr,\n",
    "        artifact_path=\"iris_model\",\n",
    "        signature=signature,\n",
    "        input_example=X_train,\n",
    "        registered_model_name=\"baseline_clf\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üßë‚Äçüè´ Exercise\n",
    "\n",
    "1. Modify the code above to track the following metrics:\n",
    "    - Accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 Score\n",
    "  \n",
    ":bulb: Look up the [API documentation](https://mlflow.org/docs/latest/python_api/mlflow.html#mlflow.log_metrics) to log multiple metrics in a single call."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logging model Artifacts\n",
    "\n",
    "In addition to logging metrics and parameters, we can also log artifacts - any files, such as images, plots, or models, that are relevant to the experiment. \n",
    "\n",
    ":bulb: You generally log artifacts after the run has been client using the MLflow client API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlflow.client import MlflowClient\n",
    "\n",
    "# Create an instance of the MLflow client\n",
    "client = MlflowClient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now log a confusion matrix as an artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Computing the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "\n",
    "# Creating a figure object and axes for the confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Plotting the confusion matrix using the created axes\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
    "disp.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "\n",
    "# Setting the title of the plot\n",
    "ax.set_title('Confusion Matrix')\n",
    "\n",
    "# Now 'fig' can be used with MLFlow's log_figure function\n",
    "client.log_figure(run.info.run_id, figure=fig, artifact_file=\"confusion_matrix.png\")\n",
    "\n",
    "# Showing the plot here for demonstration\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßë‚Äçüè´ Review your experiment in the UI\n",
    "\n",
    "To review the model and its details, follow these step-by-step instructions:\n",
    "\n",
    "+ **Step 1: Go to the Tracking server**\n",
    "\n",
    "    - Open the Mlflow tab.\n",
    "\n",
    "\n",
    "+ **Step 2: Locate Your Experiment:**\n",
    "\n",
    "    - Find the experiment name you specified in your MLflow run.\n",
    "\n",
    "+ **Step 3: Review Run Details:**\n",
    "\n",
    "  - Click on the experiment name to view the runs within that experiment.\n",
    "  - Locate the specific run you want to review.\n",
    "\n",
    "+ **Step 4: Reviewing Artifacts and Metrics:**\n",
    "\n",
    "  - Click on the run to see detailed information.\n",
    "  - Navigate to the \"Artifacts\" tab to view logged artifacts.\n",
    "  - Navigate to the \"Metrics\" tab to view logged metrics.\n",
    "\n",
    "+ **Step 5: Viewing Confusion Matrix Image:**\n",
    "\n",
    "  - If you logged the confusion matrix as an artifact, you can find it in the \"Artifacts\" tab.\n",
    "  - You may find a file named \"confusion_matrix.png\" (or the specified artifact file name).\n",
    "  - Download or view the confusion matrix image.\n",
    "\n",
    "+ **Step 6: View models in the UI:**\n",
    "  - You can find details about the logged model under the **Models** tab.\n",
    "  - Look for the model name you specified in your MLflow run (e.g., \"baseline_clf\").\n",
    "\n",
    "+ **Explore Additional Options:**\n",
    "\n",
    "  - You can explore other tabs and options in the MLflow UI to gather more insights, such as \"Parameters,\" \"Tags,\" and \"Source.\"\n",
    "\n",
    "These instructions will guide you through reviewing and exploring the tracked models using the MLflow UI, providing valuable insights into the experiment results and registered models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auto Logging\n",
    "\n",
    "Auto logging is a powerful feature that allows you to log metrics, parameters, and models without the need for explicit log statements but just a single ```mlflow.autolog()``` call at the top of your ML code. \n",
    "\n",
    "This feature is available for popular ML libraries like scikit-learn, TensorFlow, and PyTorch. See the [documentation](https://mlflow.org/docs/latest/tracking/autolog.html#supported-libraries) for the list of supported libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the sklearn auto-logging feature\n",
    "\n",
    "# Enable auto-logging\n",
    "mlflow.sklearn.autolog()\n",
    "\n",
    "# Train the model\n",
    "lr = LogisticRegression(**params)\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, you learned how to use MLflow to track experiments, log metrics, parameters, and artifacts, and log models. \n",
    "You also learned how to use the MLflow UI to visualize experiments and query experiments using the MLflow API. \n",
    "\n",
    "You are now prepared to contribute to more informed and reproducible machine learning workflow using MLflow. üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
