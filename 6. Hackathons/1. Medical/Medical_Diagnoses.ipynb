{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "steady-auction",
   "metadata": {},
   "source": [
    "# Hacakthon: Medical Diagnoses\n",
    "\n",
    "You are working for a hospital and you want to be able to detect a certain disease using data taken from medical images.\n",
    "\n",
    "You have data describing an image of a medical sample and knowledge of whether the patient the sample as taken from was diagnosed with the disease or not.\n",
    "\n",
    "You want to be able to capture every patient who need to be assessed further so as to rule out any complications related to the disease.\n",
    "\n",
    "\n",
    "<img src=\"images/doctor.png\" style=\"display: block;margin-left: auto;margin-right: auto;height: 300px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efficient-patient",
   "metadata": {},
   "source": [
    "## About the data\n",
    "\n",
    "This dataset comes from Scikit-Learn as one of the many datasets to explore and on which perform machine learning. [The Breast cancer wisconsin (diagnostic) dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-wisconsin-diagnostic-dataset).\n",
    "\n",
    "The features in the dataset were computed from a digitized image of a breast tissue sample; they describe characteristics of the cell nuclei present in the image.\n",
    "\n",
    "\n",
    "|Column|Description|Type|\n",
    "|:---|:---|:---|\n",
    "|id|ID number|float|\n",
    "|diagnosis|The diagnosis of breast tissues (M = malignant, B = benign)|float|\n",
    "|radius_mean|mean of distances from center to points on the perimeter|float|\n",
    "|texture_mean|standard deviation of gray-scale values|float|\n",
    "|perimeter_mean|mean size of the perimeter of the tumor|float|\n",
    "|area_mean|mean size of the tumor|float|\n",
    "|smoothness_mean|mean of local variation in radius lengths|float|\n",
    "|compactness_mean|mean of perimeter^2 / area - 1.0|float|\n",
    "|concavity_mean|mean of severity of concave portions of the contour|float|\n",
    "|concave points_mean|mean for number of concave portions of the contour|float|\n",
    "|fractal_dimension_mean|mean for \"coastline approximation\" - 1|float|\n",
    "|radius_se|standard error for the mean of distances from center to points on the perimeter|float|\n",
    "|texture_se|standard error for standard deviation of gray-scale values|float|\n",
    "|perimeter_se|standard error for the perimeter of the tumor|float|\n",
    "|area_se|standard error for the size of the tumor|float|\n",
    "|smoothness_se|standard error for local variation in radius lengths|float|\n",
    "|compactness_se|standard error for perimeter^2 / area - 1.0|float|\n",
    "|concavity_se|standard error for severity of concave portions of the contour|float|\n",
    "|concave points_se|standard error for number of concave portions of the contour|float|\n",
    "|fractal_dimension_se|standard error for \"coastline approximation\" - 1|float|\n",
    "|radius_worst|\"worst\" or largest mean value for mean of distances from center to points on the perimeter|float|\n",
    "|texture_worst|\"worst\" or largest mean value for standard deviation of gray-scale values|float|\n",
    "|perimeter_worst|\"worst\" or largest mean value for mean of perimeter|float|\n",
    "|area_worst|\"worst\" or largest mean value for mean of area|float|\n",
    "|smoothness_worst|\"worst\" or largest mean value for local variation in radius lengths|float|\n",
    "|compactness_worst|\"worst\" or largest mean value for perimeter^2 / area - 1.0|float|\n",
    "|concavity_worst|\"worst\" or largest mean value for severity of concave portions of the contour|float|\n",
    "|concave points_worst|\"worst\" or largest mean value for number of concave portions of the contour|float|\n",
    "|fractal_dimension_worst|\"worst\" or largest mean value for \"coastline approximation\" - 1|float|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-scope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "medical = pd.read_csv('data/medical.csv')\n",
    "medical.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "widespread-question",
   "metadata": {},
   "source": [
    "### Exercise: \n",
    "\n",
    "Perform some preliminary analysis on the dataset.\n",
    "\n",
    "1. How many patients is there data for?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-salon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "southeast-jacob",
   "metadata": {},
   "source": [
    "2. How could you show the datatype of each column? Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-timeline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "affiliated-constraint",
   "metadata": {},
   "source": [
    "3. How many predictive features are there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-reasoning",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "southwest-thirty",
   "metadata": {},
   "source": [
    "4. Run a pair plot on the features you think are the most correlated, using hue as the diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-perfume",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "average-multimedia",
   "metadata": {},
   "source": [
    "**Answers** Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loving-pillow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/data-exploration-medical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-strip",
   "metadata": {},
   "source": [
    "# Prepare `X` and `y`\n",
    "\n",
    "Split the data into `X` and `y` where `X` is the feature matrix and `y` is the target (`diagnosis`)\n",
    "\n",
    "Exclude `id` from the feature matrix due to it being a unique identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "olive-context",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-jersey",
   "metadata": {},
   "source": [
    "Check the shape of `X` and `y`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thirty-sculpture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "associate-greece",
   "metadata": {},
   "source": [
    "**Answers** Prepare X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "awful-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/prepare-x-y-medical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-caribbean",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "Perform the train test split on the data to create `X_train`, `X_test`, `y_train`, `y_test`\n",
    "\n",
    "Use a `random_state` to ensure the split is the same each time it is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-primary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "judicial-ethernet",
   "metadata": {},
   "source": [
    "Check the shape of `X_train`, `X_test`, `y_train` and `y_test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-wilson",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "discrete-customs",
   "metadata": {},
   "source": [
    "**Answers** Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accessory-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/train-test-medical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entertaining-warning",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "There are no categorical values or missing values to deal with. However since we are building a `Logistic Regression` we will want to `scale` the data so that the coefficients can be compared.\n",
    "\n",
    "Choose from the below and import it in from `sklearn.preprocessing`\n",
    "\n",
    "- `StandardScaler`\n",
    "- `RobustScaler`\n",
    "- `MinMaxScaler`\n",
    "\n",
    "Instantiate your scaler (eg. `scaler = RobustScaler()`) and try it out by performing:\n",
    "\n",
    "```python\n",
    "pd.DataFrame(scaler.fit_transform(X_train), columns=features)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beginning-venture",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "difficult-vanilla",
   "metadata": {},
   "source": [
    "**Answers** Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-thesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/preprocessing-medical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sound-christian",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "\n",
    "Now that we have a scaler chosen, we're ready to build a pipeline.\n",
    "\n",
    "- Import `Pipeline` from `sklrean.pipeline` and `LogisticRegression` from `sklearn.linear_model`.\n",
    "- Instantiate the model with no parameters\n",
    "- Instantiate the pipeline with the scaler and model as the 2 steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-mechanism",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "experimental-turkey",
   "metadata": {},
   "source": [
    "Fit the pipeline to `X_train` and `y_train`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-factory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-supervision",
   "metadata": {},
   "source": [
    "**Answers** Building the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-train",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/build-model-medical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intellectual-crown",
   "metadata": {},
   "source": [
    "## Performance metrics\n",
    "\n",
    "Find the accuracy score on both the train and the test. Is your model generalising well? Is your model overfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-answer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "boolean-funds",
   "metadata": {},
   "source": [
    "The high accuracy score on both train and test demonstrates a model that generalises well. Since the drop from train to test is minor, this also suggests that the model is not overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-philippines",
   "metadata": {},
   "source": [
    "**Answers** Performance metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unusual-hostel",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %load answers/performance-metrics-medical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "least-fifteen",
   "metadata": {},
   "source": [
    "## Feature Selection\n",
    "\n",
    "Look at the coefficients of each feature from the model. Which are the most important features to determine the diagnosis?\n",
    "\n",
    "- Create a dataframe using\n",
    "    - `pd.DataFrame(pipeline_lr['model'].coef_[0], index=features, columns=['Importances'])`\n",
    "    - Use assign to update the column `Importances` to be absolute values\n",
    "    - Sort the data by `Importances`\n",
    "    - Save the dataframe to a variable called `coefs_plotter`\n",
    "- Plot the data using `coefs_plotter.plot(y='Importances', kind='barh')`\n",
    "    \n",
    "\n",
    "\n",
    "**Bonus:** Add a column `color_negative` that is `darkorange` when the original `Importances` column was negative and `royalblue` when not. Add the parameter `color=coefs_plotter['color_negative']` to color the bars accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-montana",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cardiovascular-yesterday",
   "metadata": {},
   "source": [
    "**Answers**: Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-check",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/feature-importances-medical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-reduction",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Import `GridSearchCV()` from `sklearn.model_selection` and use it to search across the different scalers that you can use.  \n",
    "\n",
    "- Check out the actual names of the model parameters using `pipeline.get_params()`\n",
    "- Create a parameter dictionary for your grid\n",
    "- Make sure to instanitate the grid (eg. `grid = GridSearch(pipeline, parameters)`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-equipment",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "local-chance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "registered-unemployment",
   "metadata": {},
   "source": [
    "Fit your grid to `(X_train, y_train)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-commander",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-approach",
   "metadata": {},
   "source": [
    "What was the best scaler according to the `GridSearchCV` and what was the average accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-woman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bearing-provincial",
   "metadata": {},
   "source": [
    "Use `grid.score(X, y)` to find the accuracy score on your train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "funny-performer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "beautiful-cancer",
   "metadata": {},
   "source": [
    "Make a dataframe from the `cv_results_` from your `grid` and `sort_values()` by `rank_test_score` to see the best models and their parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exciting-seeker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "economic-spell",
   "metadata": {},
   "source": [
    "**Answers** Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "split-complement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/hyperparameter-tuning-medical.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blank-bahamas",
   "metadata": {},
   "source": [
    "## <mark>Bonus: Custom Scikit-Learn</mark>\n",
    "\n",
    "This dataset has quite a lot of features/columns. Accordingly, we may want to perform some feature selection, i.e. drop some of the features.\n",
    "\n",
    "This may be for visualisation purposes or to aid model performance. For example, we saw in the feature importances chart that some features were more important to the model than others. Additionally, we saw previously that some features were (unsuprisngly) correlated with one another, e.g. `perimeter_mean` and `area_mean`, which can impede model performance.\n",
    "\n",
    "We could choose to manually drop some. However, it could be more efficient to automate this.\n",
    "\n",
    "### Part A\n",
    "\n",
    "Create a customer Transformer that is able to drop all columns containing a particular string, e.g. all feature names containing the str `perimeter_`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parliamentary-saint",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DropColumnsContaining(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        #TODO\n",
    "        pass\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        #TODO\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-evaluation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/custom-transformer-medical-A.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mechanical-religious",
   "metadata": {},
   "source": [
    "### Part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "directed-principle",
   "metadata": {},
   "source": [
    "Add your custom transformer to your Pipeline and experiment to see if dropping certain columns can aid performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorrect-cincinnati",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "streaming-installation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load answers/custom-transformer-medical-B.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integral-think",
   "metadata": {},
   "source": [
    "<img src='images/gdd-logo.png' align=right width=300px>\n",
    "\n",
    "# Conclusion\n",
    "\n",
    "You have now gone through a model build from start to finish following these steps:\n",
    "\n",
    "- Data Exploration\n",
    "- Split into X and y\n",
    "- Using train_test_split\n",
    "- Preprocessing using scalers\n",
    "- Building the Model\n",
    "- Performance Metrics\n",
    "- Hyperparameter Tuning\n",
    "\n",
    "This is a great place to be in to start your first Machine Learning problems!\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "There are more advanced techniques that you might also want to implement. Your next topic of learning may be one of the following\n",
    "\n",
    "- Further metrics and how to interpret them\n",
    "- Engineering and selecting features\n",
    "- Building your own sci-kit learn estimator\n",
    "- Model interpretation\n",
    "\n",
    "All of these are visited in our Advanced Data Science with Python course!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
